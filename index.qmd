---
title: "Modelos Lineares I: Exercícios resolvidos em R e SAS"
subtitle: "Baseados nos capítulos do livro: Linear Models in Statistics (Rencher e Schaalje, 2008)"
format:
  html:
    output-dir: docs
    theme: cosmo
    page-layout: article
    toc: true
    code-link: true
    number-sections: true
    grid:
      sidebar-width: 250px
      body-width: 800px
      margin-width: 250px
      gutter-width: 1.5em
editor: visual
---

Este material faz parte da disciplina "**Modelos Lineares I**" (2025), ministrada pelo Prof. Dr. César Gonçalves de Lima na ESALQ/USP, como parte do Programa de Pós-Graduação em Estatística e Experimentação Agronômica. O objetivo deste material é disponibilizar os scripts e exercícios utilizados nas aulas, originalmente desenvolvidos em SAS, convertidos para a linguagem de programação R.

O material abrange desde conceitos básicos de álgebra matricial aplicada à estatística multivariada, até modelos de regressão simples e múltipla, culminando com modelos de análise de variância, tanto balanceados quanto desbalanceados.

# Capítulos 1 a 3 - Covariância, Correlação e Distância de Mahalanobis

Exemplo de cálculo de matriz de Covariâncias e de Correlações amostrais, além da distância de Mahalanobis.

Fonte dos dados: *Weights of Cork Boring (in Centigrams) in Four Directions for 28 trees. Applied Multivariate Statistics with SAS Software. Khattree & Naik(2003) - p. 11.*

## Vetores e Matriz

::: panel-tabset
### R

Para criar **vetores** no R, definimos um nome para cada vetor (`y1`, `y2`, `y3` e `y4`), em seguida, utilizamos o operador `<-` e a função `c()` para atribuir valores a cada um dos objetos (vetores). Dentro de `c()`, declaramos seus valores, separados por vírgula.

```{r}
y1 <- c(72,60,56,41,32,30,39,42,37,33,32,63,54,47,91,56,79,81,78,46,39,32,60,35,39,50,43,48)

y2 <- c(66,53,57,29,32,35,39,43,40,29,30,45,46,51,79,68,65,80,55,38,35,30,50,37,36,34,37,54)

y3 <- c(76,66,64,36,35,34,31,31,31,27,34,74,60,52,100,47,70,68,67,37,34,30,67,48,39,37,39,57)

y4 <- c(77,63,58,38,36,26,27,25,25,36,28,63,52,43,75,50,61,58,60,38,37,32,54,39,31,40,50,43)
```

Com a função `cbind()` juntamos os vetores `y1`, `y2`, `y3` e `y4`, salvando-os em um objeto chamado `Y`. Utilizando a função `colnames()`, alteramos os nomes dos vetores (`"North"`, `"East"`, `"South"`, `"West"`).

```{r}
Y <- cbind(y1, y2, y3, y4)
colnames(Y) <- c("North", "East", "South", "West")
Y
```

Com a função `class()`, constatamos que o objeto `Y` é uma **matriz** (`matrix`).

```{r}
class(Y)
```

### SAS

```{r}
#| eval: false

proc iml;

y1 = {72,60,56,41,32,30,39,42,37,33,32,63,54,47, 91,56,79,81,78,46, 39,32,60,35,39,50,43,48};

y2 = {66,53,57,29,32,35,39,43,40,29,30,45,46,51, 79,68,65,80,55,38, 35,30,50,37,36,34,37,54};

y3 = {76,66,64,36,35,34,31,31,31,27,34,74,60,52,100,47,70,68,67,37, 34,30,67,48,39,37,39,57};

y4 = {77,63,58,38,36,26,27,25,25,36,28,63,52,43, 75,50,61,58,60,38, 37,32,54,39,31,40,50,43};
```

```{r}
#| eval: false

Y = y1||y2||y3||y4;
create Cork var {North East South West};
append from Y;
Close Cork;
```
:::

## Matriz de Variâncias e Covariâncias

Para obtermos a matriz de variâncias e covariâncias, precisamos dos seguintes objetos:

-   `n`: número de observações $n$;

-   `In`: matriz identidade $\mathbf{I}$;

-   `jn`: vetor coluna de 1's $\mathbf{j}$;

-   `Jnn`: matriz de 1's $\mathbf{J}$.

::: panel-tabset
### R

As funções `nrow()` e `ncol()` nos retornam o número de **linhas** e **colunas** de um objeto, respectivamente.

```{r}
n <- nrow(Y)
p <- ncol(Y)

n; p
```

No caso da matriz `Y`, apresenta dimensão $28 \times 4$.

A função `diag()` cria uma **matriz identidade**. Basta inserir dentro da função a dimensão da matriz.

```{r}
In <- diag(n)
In
```

$$
\mathbf{I}_{(28)} = 
  \begin{bmatrix}
   1 & 0 & 0 & \dots & 0 \\
   0 & 1 & 0 & \dots & 0 \\
   0 & 0 & 1 &\dots & 0 \\
   \vdots & \vdots & \vdots & \ddots & \vdots \\
   0 & 0 & 0 & \dots & 1
   \end{bmatrix}
$$

Com a função `matrix()`, podemos criar qualquer tipo de matriz, vetor ou escalar. Para isso, utilizamos três argumentos dentro da função:

-   `data =`: os elementos que compõem a matriz;

-   `nrow =`: número de linhas da matriz;

-   `ncol =`: número de colunas da matriz.

```{r}
jn <- matrix(data = 1, nrow = n, ncol = 1)
jn

Jnn <- matrix(data = 1, nrow = n, ncol = n)
Jnn
```

Nos casos anteriores, criamos o vetor coluna de 1's `jn` de dimensão $28 \times 1$ e a matriz de 1's `Jnn` de dimensão $28 \times 28$.

$$
j_n = \mathbf{j}_{(28 \times 1)} = [1,1,...,1]'
$$

$$
Jnn = \mathbf{J}_{(28 \times 28)} = 
  \begin{bmatrix}
   1 & 1 & 1 & \dots & 1 \\
   1 & 1 & 1 & \dots & 1 \\
   1 & 1 & 1 &\dots & 1 \\
   \vdots & \vdots & \vdots & \ddots & \vdots \\
   1 & 1 & 1 & \dots & 1
   \end{bmatrix}
$$

Além disso, note que a matriz $\mathbf{J}$ (`Jnn`) pode ser obtida por $\mathbf{J} = \mathbf{j} \space \mathbf{j}'$, ou seja, a multiplicação do vetor coluna $\mathbf{j}$ (`jn`) pela sua transposta.

No R, utilizamos a função `t()` para realizar a transposição de uma matriz ou vetor.

```{r}
Jnn <- jn %*% t(jn)
Jnn
```

$$
\mathbf{J} = \mathbf{j} \space \mathbf{j}' = 
\begin{bmatrix} 
1 \\ 1 \\ \vdots \\1 
\end{bmatrix}
\begin{bmatrix} 
1 & 1 & \dots & 1 
\end{bmatrix}
= 
  \begin{bmatrix}
   1 & 1 & \dots & 1 \\
   1 & 1 & \dots & 1 \\
   \vdots & \vdots & \ddots & \vdots \\
   1 & 1 & \dots & 1
   \end{bmatrix}
$$

A seguir, calcularemos a matriz de variâncias e covariâncias amostrais, dada pela equação:

$$\mathbf{\Sigma} = \frac{1}{n-1} \mathbf{Y}'(\mathbf{I} - \frac{1}{n}J) \mathbf{Y}$$

```{r}
Sigma <- (1 / (n - 1)) * t(Y) %*% (In - (1/n) * Jnn) %*% Y
Sigma
```

A função `t()` realiza a transposição de uma matriz ou vetor. Já o operador `%*%` realiza a multiplicação entre duas matrizes ou vetores conformes.

No R, temos a função `cov()` que realiza o cálculo da matriz de variâncias e covariâncias. Para isso, basta declarar dentro da função a matriz desejada.

```{r}
cov(Y)
```

$$
\mathbf{\Sigma} = 
  \begin{bmatrix}
   \sigma^2_1 & \sigma_{12} & \sigma_{13} & \dots & \sigma_{1p} \\
   \sigma_{21} & \sigma^2_2 & \sigma_{23} & \dots & \sigma_{2p} \\
   \sigma_{31} & \sigma_{32} & \sigma^2_3 &\dots & \sigma_{p3} \\
   \vdots & \vdots & \vdots & \ddots & \vdots \\
   \sigma_{p1} & \sigma_{p2} & \sigma_{p3} & \dots & \sigma^2_p
   \end{bmatrix}
$$

Os elementos da diagonal principal são as variâncias e os demais elementos, as covariâncias. Note que a matriz de variâncias e covariâncias é simétrica.

## Matriz de Correlações {#sec-corr}

Para calcular a matriz de correlação ($\mathbf{\rho}_{ij}$), utilizamos a seguinte expressão:

$$\mathbf{\rho}_{ij} = \mathbf{D}_{\sigma}^{-1} \mathbf{\Sigma}  \mathbf{D}_{\sigma}^{-1}$$

em que $\mathbf{D}_{\sigma}$ é uma matriz diagonal com a raiz quadrada das variâncias, ou seja, a raiz quadrada da diagonal da matriz de variâncias e covariâncias ($\mathbf{\Sigma}$).

```{r}
D <- sqrt(diag(Sigma))
D

corr <- solve(diag(D)) %*% Sigma %*% solve(diag(D))
corr
```

A função `sqrt()` realiza a operação raiz quadrada. Já a `solve()`, calcula a inversa de uma matriz.

No R, temos a função `cor()` que realiza o cálculo da matriz de correlação. Novamente, basta declarar a matriz dentro da função.

```{r}
cor(Y)
```

$$
\mathbf{\rho} = 
  \begin{bmatrix}
   1 & \rho_{12} & \rho_{13} & \dots & \rho_{1p} \\
   \rho_{21} & 1 & \rho_{23} & \dots & \rho_{2p} \\
   \rho_{31} & \rho_{32} & 1 &\dots & \rho_{p3} \\
   \vdots & \vdots & \vdots & \ddots & \vdots \\
   \rho_{p1} & \rho_{p2} & \rho_{p3} & \dots & 1
   \end{bmatrix}
$$

Por fim, a partir da matriz de correlação ($\mathbf{\rho}_{ij}$), podemos retornar para a matriz de variâncias e covariâncias ($\mathbf{\Sigma}$) a partir da seguinte equação:

$$
\mathbf{\Sigma} = \mathbf{D}_{\sigma} \mathbf{\rho}_{ij} \mathbf{D}_{\sigma}
$$

```{r}
Verifica <- diag(D) %*% corr %*% diag(D)
Verifica
```

### SAS

```{r}
#| eval: false

p = ncol(Y);
n = nrow(Y);
In = I(n);
jn = j(n,1,1);
Jnn = J(n,n,1);
Sigma = (1/(n-1))*t(Y)*(In-(1/n)*Jnn)*Y;
D = sqrt(diag(Sigma));
corr = inv(D)*Sigma*inv(D);
Verifica = D*corr*D;
title 'Matriz de variâncias e covariâncias amostrais utilizando proc iml';
print ,,Sigma[format=8.4],, 'Matriz de correlações:' ,, corr[format=8.5],, Verifica[format=8.4];
```
:::

## Distância de Mahalanobis (Distância padronizada)

:::::: panel-tabset
### R

Primeiramente, calcularemos o vetor de médias ($\mathbf{\mu}$) da matriz $\mathbf{Y}$.

$$\mathbf{\mu} = \frac{1}n \mathbf{j}' \mathbf{Y}$$

```{r}
mi <- (1/n) * t(jn) %*% Y
mi
```

Com o vetor de médias ($\mathbf{\mu}$), calcularemos a distância de Mahalanobis ($DM$), dada pela seguinte expressão:

$$
DM = (\mathbf{y} - \mathbf{\mu})' \mathbf{\Sigma} (\mathbf{y} - \mathbf{\mu})
$$

```{r}
DM2 <- rep(0, n)
for (i in 1:n) {
  yi <- Y[i,]
  DM <- as.numeric((yi - mi) %*% solve(Sigma) %*% t(yi - mi))
  DM2[i] <- DM
}
DM2
```

No R, podemos utilizar a função `mahalanobis()` para calcular a distância de Mahalanobis. Como argumentos, temos:

-   `x =`: matriz utilizada para o cálculo;

-   `center =`: o vetor de médias ($\mu$);

-   `cov =`: a matriz de variâncias e covariâncias ($\Sigma$).

```{r}
mahalanobis(x = Y, center = mi, cov = Sigma)
```

Ordenando os valores do vetor da distância de Mahalanobis, temos:

```{r}
rank <- rank(DM2)
data.frame(Y, DM2, rank)
```

A observação 13 apresenta a menor distância, enquanto a 16, a maior distância de Mahalanobis.

### SAS

```{r}
#| eval: false

mi = (1/n)*t(jn)*y;
print 'Vetor de médias:' mi[format=5.2],,;
```

```{r}
#| eval: false

DM2 = j(n,1,0);
i=1;
do while (i<=n);
yi= Y[i,];
DM = (yi-mi)*inv(Sigma)*t(yi-mi);
DM2[i] = DM;
i=i+1;
end;

rank = rank(DM2);
print
```

```{r}
#| eval: false

'-----------------------------------------------------------------',
'Distância de Mahalanobis de cada ponto (y) ao vetor de médias(mi)',
'-----------------------------------------------------------------';
print ,,Y ' ' DM2[format=8.4] ' ' rank;
quit;

proc corr cov data=cork;
title 'Matriz de variâncias e covariâncias utilizando proc corr';
var north east south west;
run;
```

# Exemplo

Exemplo de cálculo de matriz de Covariâncias e de Correlações amostrais, além da distância de Mahalanobis.

Fonte dos dados: *Weights of Cork Boring (in Centigrams) in Four Directions for 28 trees. Applied Multivariate Statistics with SAS Software. Khattree & Naik(2003) - p. 11.*

## Vetores e Matriz

::: panel-tabset
### R

Para criar **vetores** no R, definimos um nome para cada vetor (`y1`, `y2`, `y3` e `y4`), em seguida, utilizamos o operador `<-` e a função `c()` para atribuir valores a cada um dos objetos (vetores). Dentro de `c()`, declaramos seus valores, separados por vírgula.

```{r}
y1 <- c(72,60,56,41,32,30,39,42,37,33,32,63,54,47,91,56,79,81,78,46,39,32,60,35,39,50,43,48)

y2 <- c(66,53,57,29,32,35,39,43,40,29,30,45,46,51,79,68,65,80,55,38,35,30,50,37,36,34,37,54)

y3 <- c(76,66,64,36,35,34,31,31,31,27,34,74,60,52,100,47,70,68,67,37,34,30,67,48,39,37,39,57)

y4 <- c(77,63,58,38,36,26,27,25,25,36,28,63,52,43,75,50,61,58,60,38,37,32,54,39,31,40,50,43)
```

Com a função `cbind()` juntamos os vetores `y1`, `y2`, `y3` e `y4`, salvando-os em um objeto chamado `Y`. Utilizando a função `colnames()`, alteramos os nomes dos vetores (`"North"`, `"East"`, `"South"`, `"West"`).

```{r}
Y <- cbind(y1, y2, y3, y4)
colnames(Y) <- c("North", "East", "South", "West")
Y
```

Com a função `class()`, constatamos que o objeto `Y` é uma **matriz** (`matrix`).

```{r}
class(Y)
```

### SAS

```{r}
#| eval: false

proc iml;

y1 = {72,60,56,41,32,30,39,42,37,33,32,63,54,47, 91,56,79,81,78,46, 39,32,60,35,39,50,43,48};

y2 = {66,53,57,29,32,35,39,43,40,29,30,45,46,51, 79,68,65,80,55,38, 35,30,50,37,36,34,37,54};

y3 = {76,66,64,36,35,34,31,31,31,27,34,74,60,52,100,47,70,68,67,37, 34,30,67,48,39,37,39,57};

y4 = {77,63,58,38,36,26,27,25,25,36,28,63,52,43, 75,50,61,58,60,38, 37,32,54,39,31,40,50,43};
```

```{r}
#| eval: false

Y = y1||y2||y3||y4;
create Cork var {North East South West};
append from Y;
Close Cork;
```
:::

## Matriz de Variâncias e Covariâncias

Para obtermos a matriz de variâncias e covariâncias, precisamos dos seguintes objetos:

-   `n`: número de observações $n$;

-   `In`: matriz identidade $\mathbf{I}$;

-   `jn`: vetor coluna de 1's $\mathbf{j}$;

-   `Jnn`: matriz de 1's $\mathbf{J}$.

::: panel-tabset
### R

As funções `nrow()` e `ncol()` nos retornam o número de **linhas** e **colunas** de um objeto, respectivamente.

```{r}
n <- nrow(Y)
p <- ncol(Y)

n; p
```

No caso da matriz `Y`, apresenta dimensão $28 \times 4$.

A função `diag()` cria uma **matriz identidade**. Basta inserir dentro da função a dimensão da matriz.

```{r}
In <- diag(n)
In
```

$$
\mathbf{I}_{(28)} = 
  \begin{bmatrix}
   1 & 0 & 0 & \dots & 0 \\
   0 & 1 & 0 & \dots & 0 \\
   0 & 0 & 1 &\dots & 0 \\
   \vdots & \vdots & \vdots & \ddots & \vdots \\
   0 & 0 & 0 & \dots & 1
   \end{bmatrix}
$$

Com a função `matrix()`, podemos criar qualquer tipo de matriz, vetor ou escalar. Para isso, utilizamos três argumentos dentro da função:

-   `data =`: os elementos que compõem a matriz;

-   `nrow =`: número de linhas da matriz;

-   `ncol =`: número de colunas da matriz.

```{r}
jn <- matrix(data = 1, nrow = n, ncol = 1)
jn

Jnn <- matrix(data = 1, nrow = n, ncol = n)
Jnn
```

Nos casos anteriores, criamos o vetor coluna de 1's `jn` de dimensão $28 \times 1$ e a matriz de 1's `Jnn` de dimensão $28 \times 28$.

$$
j_n = \mathbf{j}_{(28 \times 1)} = [1,1,...,1]'
$$

$$
Jnn = \mathbf{J}_{(28 \times 28)} = 
  \begin{bmatrix}
   1 & 1 & 1 & \dots & 1 \\
   1 & 1 & 1 & \dots & 1 \\
   1 & 1 & 1 &\dots & 1 \\
   \vdots & \vdots & \vdots & \ddots & \vdots \\
   1 & 1 & 1 & \dots & 1
   \end{bmatrix}
$$

Além disso, note que a matriz $\mathbf{J}$ (`Jnn`) pode ser obtida por $\mathbf{J} = \mathbf{j} \space \mathbf{j}'$, ou seja, a multiplicação do vetor coluna $\mathbf{j}$ (`jn`) pela sua transposta.

No R, utilizamos a função `t()` para realizar a transposição de uma matriz ou vetor.

```{r}
Jnn <- jn %*% t(jn)
Jnn
```

$$
\mathbf{J} = \mathbf{j} \space \mathbf{j}' = 
\begin{bmatrix} 
1 \\ 1 \\ \vdots \\1 
\end{bmatrix}
\begin{bmatrix} 
1 & 1 & \dots & 1 
\end{bmatrix}
= 
  \begin{bmatrix}
   1 & 1 & \dots & 1 \\
   1 & 1 & \dots & 1 \\
   \vdots & \vdots & \ddots & \vdots \\
   1 & 1 & \dots & 1
   \end{bmatrix}
$$

A seguir, calcularemos a matriz de variâncias e covariâncias amostrais, dada pela equação:

$$\mathbf{\Sigma} = \frac{1}{n-1} \mathbf{Y}'(\mathbf{I} - \frac{1}{n}J) \mathbf{Y}$$

```{r}
Sigma <- (1 / (n - 1)) * t(Y) %*% (In - (1/n) * Jnn) %*% Y
Sigma
```

A função `t()` realiza a transposição de uma matriz ou vetor. Já o operador `%*%` realiza a multiplicação entre duas matrizes ou vetores conformes.

No R, temos a função `cov()` que realiza o cálculo da matriz de variâncias e covariâncias. Para isso, basta declarar dentro da função a matriz desejada.

```{r}
cov(Y)
```

$$
\mathbf{\Sigma} = 
  \begin{bmatrix}
   \sigma^2_1 & \sigma_{12} & \sigma_{13} & \dots & \sigma_{1p} \\
   \sigma_{21} & \sigma^2_2 & \sigma_{23} & \dots & \sigma_{2p} \\
   \sigma_{31} & \sigma_{32} & \sigma^2_3 &\dots & \sigma_{p3} \\
   \vdots & \vdots & \vdots & \ddots & \vdots \\
   \sigma_{p1} & \sigma_{p2} & \sigma_{p3} & \dots & \sigma^2_p
   \end{bmatrix}
$$

Os elementos da diagonal principal são as variâncias e os demais elementos, as covariâncias. Note que a matriz de variâncias e covariâncias é simétrica.

## Matriz de Correlações

Para calcular a matriz de correlação ($\mathbf{\rho}_{ij}$), utilizamos a seguinte expressão:

$$\mathbf{\rho}_{ij} = \mathbf{D}_{\sigma}^{-1} \mathbf{\Sigma}  \mathbf{D}_{\sigma}^{-1}$$

em que $\mathbf{D}_{\sigma}$ é uma matriz diagonal com a raiz quadrada das variâncias, ou seja, a raiz quadrada da diagonal da matriz de variâncias e covariâncias ($\mathbf{\Sigma}$).

```{r}
D <- sqrt(diag(Sigma))
D

corr <- solve(diag(D)) %*% Sigma %*% solve(diag(D))
corr
```

A função `sqrt()` realiza a operação raiz quadrada. Já a `solve()`, calcula a inversa de uma matriz.

No R, temos a função `cor()` que realiza o cálculo da matriz de correlação. Novamente, basta declarar a matriz dentro da função.

```{r}
cor(Y)
```

$$
\mathbf{\rho} = 
  \begin{bmatrix}
   1 & \rho_{12} & \rho_{13} & \dots & \rho_{1p} \\
   \rho_{21} & 1 & \rho_{23} & \dots & \rho_{2p} \\
   \rho_{31} & \rho_{32} & 1 &\dots & \rho_{p3} \\
   \vdots & \vdots & \vdots & \ddots & \vdots \\
   \rho_{p1} & \rho_{p2} & \rho_{p3} & \dots & 1
   \end{bmatrix}
$$

Por fim, a partir da matriz de correlação ($\mathbf{\rho}_{ij}$), podemos retornar para a matriz de variâncias e covariâncias ($\mathbf{\Sigma}$) a partir da seguinte equação:

$$
\mathbf{\Sigma} = \mathbf{D}_{\sigma} \mathbf{\rho}_{ij} \mathbf{D}_{\sigma}
$$

```{r}
Verifica <- diag(D) %*% corr %*% diag(D)
Verifica
```

### SAS

```{r}
#| eval: false

p = ncol(Y);
n = nrow(Y);
In = I(n);
jn = j(n,1,1);
Jnn = J(n,n,1);
Sigma = (1/(n-1))*t(Y)*(In-(1/n)*Jnn)*Y;
D = sqrt(diag(Sigma));
corr = inv(D)*Sigma*inv(D);
Verifica = D*corr*D;
title 'Matriz de variâncias e covariâncias amostrais utilizando proc iml';
print ,,Sigma[format=8.4],, 'Matriz de correlações:' ,, corr[format=8.5],, Verifica[format=8.4];
```
:::

## Distância de Mahalanobis (Distância padronizada)

::: panel-tabset
### R

Primeiramente, calcularemos o vetor de médias ($\mathbf{\mu}$) da matriz $\mathbf{Y}$.

$$\mathbf{\mu} = \frac{1}n \mathbf{j}' \mathbf{Y}$$

```{r}
mi <- (1/n) * t(jn) %*% Y
mi
```

Com o vetor de médias ($\mathbf{\mu}$), calcularemos a distância de Mahalanobis ($DM$), dada pela seguinte expressão:

$$
DM = (\mathbf{y} - \mathbf{\mu})' \mathbf{\Sigma} (\mathbf{y} - \mathbf{\mu})
$$

```{r}
DM2 <- rep(0, n)
for (i in 1:n) {
  yi <- Y[i,]
  DM <- as.numeric((yi - mi) %*% solve(Sigma) %*% t(yi - mi))
  DM2[i] <- DM
}
DM2
```

No R, podemos utilizar a função `mahalanobis()` para calcular a distância de Mahalanobis. Como argumentos, temos:

-   `x =`: matriz utilizada para o cálculo;

-   `center =`: o vetor de médias ($\mu$);

-   `cov =`: a matriz de variâncias e covariâncias ($\Sigma$).

```{r}
mahalanobis(x = Y, center = mi, cov = Sigma)
```

Ordenando os valores do vetor da distância de Mahalanobis, temos:

```{r}
rank <- rank(DM2)
data.frame(Y, DM2, rank)
```

A observação 13 apresenta a menor distância, enquanto a 16, a maior distância de Mahalanobis.

### SAS

```{r}
#| eval: false

mi = (1/n)*t(jn)*y;
print 'Vetor de médias:' mi[format=5.2],,;
```

```{r}
#| eval: false

DM2 = j(n,1,0);
i=1;
do while (i<=n);
yi= Y[i,];
DM = (yi-mi)*inv(Sigma)*t(yi-mi);
DM2[i] = DM;
i=i+1;
end;

rank = rank(DM2);
print
```

```{r}
#| eval: false

'-----------------------------------------------------------------',
'Distância de Mahalanobis de cada ponto (y) ao vetor de médias(mi)',
'-----------------------------------------------------------------';
print ,,Y ' ' DM2[format=8.4] ' ' rank;
quit;

proc corr cov data=cork;
title 'Matriz de variâncias e covariâncias utilizando proc corr';
var north east south west;
run;
```
:::
::::::

# Capítulo 4 - Distribuição Normal Multivariada

## Gráfico da Normal Bivariada

Construiremos o gráfico da normal bivariada utilizando o pacote `plotly`. Este pacote permite confeccionar gráficos dinâmicos tridimensionais.

```{r}
#| eval: false

install.packages("plotly")
```

```{r}
#| message: false
#| warning: false

library(plotly)
```

::: panel-tabset
### R

Para construir o gráfico da normal bivariada, primeiramente, definimos os valores dos vetores das variáveis `y1` e `y2`.

```{r}
y1 <- seq(from = -4, to = 4, by = 0.1)
y1

y2 <- seq(from = -4, to = 4, by = 0.1)
y2
```

A função `seq()` cria uma sequência de valores e possui três argumentos:

-   `from =`: valor em que a sequência começa;

-   `to =`: valor em que a sequência termina;

-   `by =`: de quanto em quanto a sequência é construída.

Em seguida, calculamos a densidade bivariada, dada pela seguinte equação:

$$
\phi = \frac{1}{2\pi \sqrt{1-r^2}} exp\left\{-\frac{y_1^2 - 2r y_1 y_2 + y_2^2}{2(1-r^2)}\right\}
$$

em que $r$ é o **coeficiente de correlação** entre `y1` e `y2`, variando de -1 a 1.

O valor do coeficiente de correlação será salvo no objeto `r`. Já o objeto `pi` armazena o valor de $\pi$ com seis casas decimais.

Para verificar as alterações gráficas, redefina o valor do objeto `r` (valor entre -1 e 1).

```{r}
r <- -0.75  # Modificar este valor para verificar as alterações gráficas
pi
```

Para realizar o cálculo da densidade bivariada no R, criamos uma função (`function()`) que executa a equação definida anteriormente para cada um dos valores de `y1` e `y2`, gerando a matriz final, salva no objeto `z`.

```{r}
z <- outer(y1, y2, function(y1,y2) { 
  phi <- 1/(2 * pi * sqrt(1 - r^2)) * exp(-(y1^2 - 2 * r * y1 * y2 + y2^2) / (2 * (1 - r^2)))
  return(phi)
})
```

Com isso, podemos criar o gráfico de densidade da normal bivariada.

```{r}
plot_ly(x = y1, y = y2, z = z, type = "surface") |> 
  layout(title = paste("Densidade Normal Bivariada (r =", r, ")"))
```

Com a função `plot_ly()`, definimos os objetos que compõem os eixos x, y e z, além do estilo do gráfico (`type = "surface"`). Em seguida, utilizando a função `layout()`, inserimos o título do gráfico.

### SAS

```{r}
#| eval: false

%let r=-0.75; * Fixa o coeficiente de correlação entre y1 e y2;
data Normal;
 pi=3.1416;
 do y1=-4 to 4 by 0.1;
  do y2=-4 to 4 by 0.1; 
  phi=exp(-(y1*y1-2*&r*y1*y2+y2*y2)/2/(1-&r*&r))/2/pi/sqrt(1-&r*&r);
  output;
  end;
 end;
run;

goptions reset=all border;
proc g3d data=Normal;
 title 'Densidade Normal Bivariada (r =' &r ')';
 plot y1*y2=phi / rotate=-20;
run;
```
:::

## Propriedades

A seguir, serão apresentados exemplos para ilustrar as propriedades da distribuição normal multivariada.

### Exemplo 1

Para os exemplos dos Teoremas 1, 2 e 3 considere um vetor aleatório $\mathbf{y} \sim N_3(\mathbf{\mu}, \mathbf{\Sigma})$, em que:

$$
\mathbf{\mu} = 
  \begin{bmatrix}
  3 \\ 1 \\ 2
  \end{bmatrix}
\space e \space
\mathbf{\Sigma} = 
  \begin{bmatrix}
   4 & 0 & 2 \\
   0 & 1 & -1 \\
   2 & -1 & 3 \\
   \end{bmatrix}
$$

::: panel-tabset
#### R

```{r}
mi <- c(3, 1, 2)
mi

Sigma <- matrix(c(4, 0, 2, 0, 1, -1, 2, -1, 3), nrow = 3, ncol = 3)
Sigma
```

#### SAS

```{r}
#| eval: false

options nocenter ps=1000;
proc iml;
reset fuzz;

y = {'y1','y2','y3'};
mi = {3,1,2};
Sigma = {4 0 2, 0 1 -1, 2 -1 3};
```
:::

#### Teorema 1.1

Seja $\mathbf{y} \sim N_p(\mathbf{\mu}, \mathbf{\Sigma})$ e $\mathbf{a}$ um vetor $p \times 1$ de constantes. Então, a **variável aleatória** $z$ é dada por:

$$z = \mathbf{a'}\mathbf{y} \sim N(\mathbf{a' \mu}, \mathbf{a'\Sigma a})$$

Como exemplo, considere:

$$
z = y_1 - 2y_2 + y_3 
\text{ ,   em que } 
\mathbf{a} = 
  \begin{bmatrix}
    1 \\ -2 \\ 1
  \end{bmatrix}
$$

::: panel-tabset
#### R

```{r}
a <- c(1, -2, 1)
a

E_z <- t(a) %*% mi
E_z

var_z <- t(a) %*% Sigma %*% a
var_z
```

Com isso, a variável aleatória $z$ fica:

$$z = \mathbf{a'y} \sim N(\mathbf{a' \mu} = 3, \mathbf{a'\Sigma a} = 19)$$

#### SAS

```{r}
#| eval: false

a = {1, -2, 1};
mi_z = t(a) * mi;
var_z = t(a)*Sigma*a;
print y mi Sigma,,,,, 'item (i)   ',,'z = y1-2y2+y3   ' mi_z var_z;
```
:::

#### Teorema 1.2

Seja $\mathbf{y} \sim N_p(\mathbf{\mu}, \mathbf{\Sigma})$ e $\mathbf{A}$ uma matriz $k \times p$ de constantes e posto $k \le p$. Então, o **vetor aleatório** $\mathbf{z}$ é dado por:

$$\mathbf{z} = \mathbf{A} \mathbf{y} \sim N_k(\mathbf{A \mu}, \mathbf{A\Sigma A'})$$

Agora, considere as seguintes combinações lineares $z_1$ e $z_2$:

$$
\begin{aligned}
z_1 = y_1 - y_2 + y_3 \space \text{ e } \space z_2 = 3y_1 + y_2 - 2y_3 \\
\mathbf{z} = 
\begin{bmatrix}
\mathbf{z_1} \\ \mathbf{z_2}
\end{bmatrix}
\text{ = }
\begin{bmatrix}
1 & -1 & 1 \\
3 & 1 & -2
\end{bmatrix}
\begin{bmatrix}
y_1 \\ y_2 \\ y_3
\end{bmatrix}
\text{ = }
\mathbf{Ay}
\end{aligned}
$$

::: panel-tabset
#### R

```{r}
A <- matrix(c(1, -1, 1, 3, 1, -2), nrow = 2, ncol = 3, byrow = TRUE)
A

E_z1z2 <- A %*% mi
E_z1z2

cov_z1z2 <- A %*% Sigma %*% t(A)
cov_z1z2
```

Pelo teorema, o vetor aleatório fica:

$$
\mathbf{z} = \mathbf{Ay} \sim N_2
\left(
  \mathbf{A \mu} = 
    \begin{bmatrix}
      4 \\ 6
    \end{bmatrix},
  \mathbf{A\Sigma A'} = 
    \begin{bmatrix}
      14 & 4 \\ 
      4 & 29
    \end{bmatrix}
\right)
$$

#### SAS

```{r}
#| eval: false

ZZ = {'z1 = y1-y2+y3','z2 = 3y1+y2-2y3'}; 
A = {1 -1  1, 3  1 -2};
mi_ZZ = A*mi;
Sigma_ZZ = A*Sigma*t(A);
print 'item (ii)',,  ZZ '     ' mi_ZZ '     ' Sigma_ZZ;
```
:::

#### Teorema 2 {#sec-T2}

Se $\mathbf{y} \sim N_p(\mathbf{\mu, \Sigma})$, então **qualquer subvetor** $r \times 1$ de $\mathbf{y}$ tem uma distribuição normal $r$-variada com médias, variâncias e covariâncias iguais às da distribuição normal $p$-variada original.

Como exemplo, considere: $y_1 \sim N(3,4)$, $y_2 \sim N(1,1)$ e $y_3 \sim N(2, 3)$.

::: panel-tabset
#### R

Dessa forma, o vetor $\begin{bmatrix} y_1 \\ y_2\end{bmatrix}$:

```{r}
A12 <- matrix(c(1, 0, 0, 0, 1, 0), nrow = 2, ncol = 3, byrow = TRUE)
A12

mi_12 <- A12 %*% mi
mi_12

Sigma_12 <- A12 %*% Sigma %*% t(A12)
Sigma_12
```

$$
\begin{bmatrix}
  y_1 \\ y_2
\end{bmatrix}
  \sim N_2\left(
    \begin{bmatrix}
      3 \\ 1
    \end{bmatrix}, 
    \begin{bmatrix}
      4 & 0 \\ 
      0 & 1
    \end{bmatrix}
  \right) \\
$$

Enquanto isso, o vetor $\begin{bmatrix} y_1 \\ y_3\end{bmatrix}$:

```{r}
A13 <- matrix(c(1, 0, 0, 0, 0, 1), nrow = 2, ncol = 3, byrow = TRUE)
A13

mi_13 <- A13 %*% mi
mi_13

Sigma_13 <- A13 %*% Sigma %*% t(A13)
Sigma_13
```

$$
\begin{bmatrix}
  y_1 \\ y_3
\end{bmatrix}
  \sim N_2\left(
    \begin{bmatrix}
      3 \\ 2
    \end{bmatrix}, 
    \begin{bmatrix}
      4 & 2 \\ 
      2 & 3
    \end{bmatrix}
  \right)
$$

#### SAS

```{r}
#| eval: false

A12 = {1 0 0, 0 1 0};
mi_12 = A12*mi;
Sigma_12 = A12*Sigma*t(A12);
print mi_12 Sigma_12;

A13 = {1 0 0, 0 0 1};
mi_13 = A13*mi;
Sigma_13 = A13*Sigma*t(A13);
print mi_13 Sigma_13;
```
:::

#### Teorema 3

Se o vetor particionado $\mathbf{v} = \begin{bmatrix} \mathbf{y} \\ \mathbf{x} \end{bmatrix} \sim N_{p+q}(\mathbf{\mu, \Sigma})$ então os subvetores aleatórios $\mathbf{y}$ e $\mathbf{x}$ são **independentes** se $\mathbf{\Sigma}_{xy} = 0$.

::: panel-tabset
#### R

```{r}
a1 <- c(1, 0, 0)
a1

b2 <- c(0, 1, 0)
b2

Sigma

cov_12 <- t(a1) %*% Sigma %*% b2
cov_12
```

#### SAS

```{r}
#| eval: false

a1 = {1 0 0};
b2 = {0 1 0};
cov_12 = a1*Sigma*t(b2);
print cov_12;
```
:::

### Exemplo 2

#### Teorema 4 {#sec-T4}

Se $\mathbf{y}$ e $\mathbf{x}$ têm distribuição conjunta normal multivariada com $\mathbf{\Sigma}_{yx} \ne 0$ então a **distribuição condicional** de $\mathbf{y}$ dado $\mathbf{x}$, $f(\mathbf{y} \mid \mathbf{x})$, é **normal multivariada** com vetor de médias e matriz de covariâncias dados por:

$$
\begin{aligned}
E(\mathbf{y} \mid \mathbf{x}) = \mathbf{\mu}_y + \mathbf{\Sigma}_{yx} \mathbf{\Sigma}_{xx}^{-1}(\mathbf{x} - \mathbf{\mu}_x) \\
cov(\mathbf{y} \mid \mathbf{x}) = \mathbf{\Sigma}_{yy} - \mathbf{\Sigma}_{yx} \mathbf{\Sigma}_{xx}^{-1} \mathbf{\Sigma}_{xy}
\end{aligned}
$$

Para ilustrar o Teorema 4, considere o vetor aleatório $\mathbf{v} \sim N_4(\mathbf{\mu, \Sigma})$ em que:

$$
\begin{aligned}
\mathbf{\mu} = 
\begin{bmatrix}
2 \\ 5 \\ -2 \\ 1
\end{bmatrix}
\text{ e }
\mathbf{\Sigma} = 
\begin{bmatrix}
9 & 0 & 3 & 3 \\
0 & 1 & -1 & 2 \\
3 & -1 & 6 & -3 \\
3 & 2 & -3 & 7 \\
\end{bmatrix}
\end{aligned}
$$

::: panel-tabset
#### R

```{r}
mi <- c(2, 5, -2, 1)
mi

Sigma <- matrix(
  c(9, 0, 3, 3, 0, 1, -1, 2, 3, -1, 6, -3, 3, 2, -3, 7), 
  nrow = 4, byrow = TRUE
)
Sigma
```

#### SAS

```{r}
#| eval: false

options nocenter ps=1000;
proc iml;
reset fuzz;

mi = {2,5,-2,1};
Sigma = {9 0 3 3, 0 1 -1 2, 3 -1 6 -3, 3 2 -3 7};
print v mi Sigma;
```
:::

Se $\mathbf{v} = \begin{bmatrix} y_1, y_2, x_1, x_2 \end{bmatrix} '$ é um vetor particionado dessa forma, então:

$$
\mathbf{\mu_y} = 
\begin{bmatrix} 
2 \\ 5
\end{bmatrix}
, 
\mathbf{\mu_x} =
\begin{bmatrix} 
-2 \\ 1
\end{bmatrix} \\
$$

$$
\mathbf{\Sigma_{yy}} = 
\begin{bmatrix} 
9 & 0 \\ 0 & 1
\end{bmatrix}
,
\mathbf{\Sigma_{xx}}
\begin{bmatrix} 
6 & -3 \\ -3 & 7
\end{bmatrix}
,
\mathbf{\Sigma_{yx}}
\begin{bmatrix} 
3 & 3 \\ -1 & 2
\end{bmatrix}
$$

::: panel-tabset
#### R

```{r}
Ay <- matrix(c(1, 0, 0, 0, 0, 1, 0, 0), nrow = 2, byrow = TRUE)
Ay

mi_y <- Ay %*% mi
mi_y

Sigma_yy <- Ay %*% Sigma %*% t(Ay)
Sigma_yy

Ax <- matrix(c(0, 0, 1, 0, 0, 0, 0, 1), nrow = 2, byrow = TRUE)
Ax

mi_x <- Ax %*% mi
mi_x

Sigma_xx <- Ax %*%Sigma %*% t(Ax)
Sigma_xx

Sigma_yx <- Ay %*% Sigma %*% t(Ax)
Sigma_yx
```

#### SAS

```{r}
#| eval: false

Ay = {1 0 0 0, 0 1 0 0};
mi_y = Ay*mi;
Sigma_yy = Ay*Sigma*t(Ay);

Ax = {0 0 1 0, 0 0 0 1};
mi_x = Ax*mi;
Sigma_xx = Ax*Sigma*t(Ax);

Sigma_yx = Ay*Sigma*t(Ax);
print mi_y Sigma_yy,, mi_x Sigma_xx,, Sigma_yx;
```
:::

Para calcular $cov(\mathbf{y} \mid \mathbf{x}) = \mathbf{\Sigma}_{yy} - \mathbf{\Sigma}_{yx} \mathbf{\Sigma}_{xx}^{-1} \mathbf{\Sigma}_{xy}$ procedemos da seguinte maneira:

::: panel-tabset
#### R

```{r}
cov_ydx <- Sigma_yy - Sigma_yx %*% solve(Sigma_xx) %*% t(Sigma_yx)
cov_ydx
```

$$
cov(\mathbf{y} \mid \mathbf{x}) = 
\begin{bmatrix} 
3,82 & -0,73 \\ -0,73 & 0,42
\end{bmatrix}
$$

#### SAS

```{r}
#| eval: false

cov_ydx = Sigma_yy-Sigma_yx*inv(Sigma_xx)*t(Sigma_yx);

print Sigma_yy[format=6.0],, cov_ydx[format=6.2];
```
:::

Note que a variância de $y_1$ e de $y_2$ são maiores do que as variâncias condicionais:

```{r}
Sigma_yy

cov_ydx
```

$$
\begin{aligned}
var(y_1) = 9 \text{ e } var(y_1 \mid x_1, x_2) = 3,82 \\
var(y_2) = 1 \text{ e } var(y_2 \mid x_1, x_2) = 0,42
\end{aligned}
$$

### Exemplo 3

#### Teorema 4 (Corolário)

Considere:

$$
\mathbf{v} = 
\begin{bmatrix} 
y, & x_1, & \dots, & x_q
\end{bmatrix}
' = 
\begin{bmatrix} 
y \\ \mathbf{x'}
\end{bmatrix} 
\text{ com } 
\mathbf{\mu} = 
\begin{bmatrix} 
\mu_y \\ \mathbf{\mu_x}
\end{bmatrix} 
\text{ e } 
\mathbf{\Sigma} = 
\begin{bmatrix}
\sigma_y^2 & \mathbf{\sigma}_{yx}' \\
\mathbf{\sigma}_{yx} & \mathbf{\Sigma}_{xx}
\end{bmatrix}
\text{, }
$$

então $y \mid \mathbf{x}$ tem distribuição normal univariada com:

$$
\begin{aligned}
E(y \mid \mathbf{x}) = \mu_y + \mathbf{\sigma}_{yx}' \mathbf{\Sigma}_{xx}^{-1} (\mathbf{x} - \mathbf{\mu}_x) \\
var(y \mid \mathbf{x}) = \sigma_{y}^2 - \mathbf{\sigma}_{yx}' \mathbf{\Sigma}_{xx}^{-1} \mathbf{\sigma}_{yx}
\end{aligned}
$$

Como exemplo, seguimos com o vetor $\mathbf{v}$ utilizado no [Teorema 4- @sec-T4].

$$
\mathbf{v} \sim N_4(\mathbf{\mu}, \mathbf{\Sigma})
\text{ , }
\mathbf{\mu} = 
\begin{bmatrix}
2 \\ 5 \\ -2 \\ 1
\end{bmatrix}
\text{ e }
\mathbf{\Sigma} = 
\begin{bmatrix}
9 & 0 & 3 & 3 \\
0 & 1 & -1 & 2 \\
3 & -1 & 6 & -3 \\
3 & 2 & -3 & 7 \\
\end{bmatrix}
$$

::: panel-tabset
#### R

```{r}
mi <- c(2, 5, -2, 1)
mi

Sigma <- matrix(
  c(9, 0, 3, 3, 0, 1, -1, 2, 3, -1, 6, -3, 3, 2, -3, 7), 
  nrow = 4, byrow = TRUE
  )
Sigma
```

#### SAS

```{r}
#| eval: false

options nocenter ps=1000;
proc iml;
reset fuzz;

mi = {2,5,-2,1};
Sigma = {9 0 3 3, 0 1 -1 2, 3 -1 6 -3, 3 2 -3 7};
print v mi Sigma;
```
:::

Se $\mathbf{v} = \begin{bmatrix} y, & x_1, & x_2, & x_3 \end{bmatrix}'$ então:

$$\mu_y = 2 \text{ e } var(y) = 9 \\$$

$$
\mathbf{\mu}_x = 
\begin{bmatrix}
5 \\ -2 \\ 1
\end{bmatrix}
\text{ , }
\mathbf{\Sigma}_{xx} = 
\begin{bmatrix}
1 & -1 & 2 \\
-1 & 6 & -3 \\
2 & -3 & 7
\end{bmatrix}
\text{ e }
\mathbf{\sigma}_{yx} = 
\begin{bmatrix}
0 \\ 3 \\ 3
\end{bmatrix}
$$

::: panel-tabset
#### R

```{r}
Ay <- matrix(c(1, 0, 0, 0), nrow = 1)
Ay

mi_y <- Ay %*% mi
mi_y

Sigma_yy <- Ay %*% Sigma %*% t(Ay)
Sigma_yy

Ax <- matrix(
  c(0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1), 
  nrow = 3, ncol = 4, byrow = TRUE
)
Ax

mi_x <- Ax %*% mi
mi_x

Sigma_xx <- Ax %*% Sigma %*% t(Ax)
Sigma_xx

sigma_yx <- Ay %*% Sigma %*% t(Ax)
sigma_yx
```

#### SAS

```{r}
#| eval: false

Ay = {1 0 0 0};
mi_y = Ay*mi;
Sigma_yy = Ay*Sigma*t(Ay);

Ax = {0 1 0 0, 0 0 1 0, 0 0 0 1};
mi_x = Ax*mi;
Sigma_xx = Ax*Sigma*t(Ax);

Sigma_yx = Ay*Sigma*t(Ax);
print mi_y Sigma_yy,, mi_x Sigma_xx,, Sigma_yx;
```
:::

Pelo Corolário, temos:

$$
var(y \mid x_1, x_2, x_3) = \sigma_{y}^2 - \mathbf{\sigma}_{yx}' \mathbf{\Sigma}_{xx}^{-1} \mathbf{\sigma}_{yx}
$$

::: panel-tabset
#### R

```{r}
cov_ydx <- Sigma_yy - sigma_yx %*% solve(Sigma_xx) %*% t(sigma_yx)
cov_ydx
```

Portanto, $var(y \mid x_1, x_2, x_3) = `r round(cov_ydx, 3)`$.

Note que a variância de $y$ é maior do que a variância condicional:

```{r}
Sigma_yy

cov_ydx
```

$$
\begin{aligned}
var(y) &= `r round(Sigma_yy, 3)` \\
var(y \mid x_1, x_2, x_3) &= `r round(cov_ydx, 3)`
\end{aligned}
$$

#### SAS

```{r}
#| eval: false

cov_ydx = Sigma_yy-Sigma_yx*inv(Sigma_xx)*t(Sigma_yx);

print Sigma_yy[format=6.0],, cov_ydx[format=6.2];
```
:::

## Correlação Parcial

Considere um vetor $\mathbf{v}$, formado por um subconjunto de $y's$ que inclui $y_1$ e $y_2$, sendo denotado por $\mathbf{y}$. O outro subconjunto de $y's$ que inclui $y_3$ e $y_4$ é denotado por $\mathbf{x}$.

$$
\mathbf{v} = 
\begin{bmatrix}
\mathbf{y} \\ \mathbf{x}
\end{bmatrix}
\text{, em que }
\mathbf{y} = 
\begin{bmatrix}
y_1, ..., y_2
\end{bmatrix}
\text{'  e  }
\mathbf{x} = 
\begin{bmatrix}
y_3, ..., y_4
\end{bmatrix}
\text{'}
$$

Vamos comparar o coeficiente de correlação parcial entre $y_1$ e $y_2$ ($\rho_{12}$) com o coeficiente de correlação parcial condicional de $y_1$ e $y_2$ dado $y_3$ e $y_4$ ($\rho_{12.34}$).

Para isso, utilizaremos a seguinte matriz de variâncias e covariâncias:

$$
\mathbf{\Sigma} = 
  \begin{bmatrix}
    9 & 0 & 3 & 3 \\
    0 & 1 & -1 & 2 \\
    3 & -1 & 6 & -3 \\
    3 & 2 & -3 & 7 \\
   \end{bmatrix}
\text{.}
$$

Primeiramente, calcularemos a correlação linear entre as variáveis $y_1$ e $y_2$, ou seja, $\rho_{12}$ (vide @sec-corr).

::: panel-tabset
### R

Com a matriz de variâncias e covariâncias salva no objeto `Sigma`, calculamos a raiz quadrada da diagonal de `Sigma`, ou seja, a raiz quadrada das variâncias (`D`).

```{r}
Sigma <- matrix(
  c(9, 0, 3, 3, 0, 1, -1, 2, 3, -1, 6, -3, 3, 2, -3, 7), 
  nrow = 4, byrow = TRUE
)
Sigma

D <- sqrt(diag(Sigma))
D
```

Com a raiz quadrada das variâncias (`D`), calculamos os coeficientes de correlação por meio da seguinte expressão:

$$\mathbf{\rho} = \mathbf{D}_{\sigma}^{-1} \mathbf{\Sigma} \mathbf{D}_{\sigma}^{-1}$$

```{r}
Ro <- solve(diag(D)) %*% Sigma %*% solve(diag(D))
Ro
```

Assim, verificamos que as variáveis $y_1$ e $y_2$ são não correlacionadas ($\rho_{12} = \rho_{21} = 0$).

### SAS

```{r}
#| eval: false

options nocenter ps=1000;
proc iml;
reset fuzz;

Sigma = {9  0  3  3, 
         0  1 -1  2, 
         3 -1  6 -3, 
         3  2 -3  7};
D = sqrt(diag(Sigma));
Ro = inv(D)*Sigma*inv(D);
print Sigma '     ' Ro[format=6.2];
```
:::

Para calcular o coeficiente de correlação parcial entre $y_1$ e $y_2$ dada a ocorrência de $y_3$ e $y_4$ ($\rho_{12.34}$), particionaremos $\Sigma$ da seguinte maneira:

$$
\mathbf{\Sigma}_{yy} =
  \begin{bmatrix}
    9 & 0 \\
    0 & 1
  \end{bmatrix}
\text{  ,  }
\mathbf{\Sigma}_{xx} =
  \begin{bmatrix}
    6 & -3 \\
    -3 & 7
  \end{bmatrix}
,\mathbf{\Sigma}_{yx} =
  \begin{bmatrix}
    3 & 3 \\
    -1 & 2
  \end{bmatrix}
$$

::: panel-tabset
### R

Para particionar $\mathbf{\Sigma}$, utilizaremos o [Teorema 2 -@sec-T2].

A primeira partição ($\mathbf{\Sigma}_{yy}$) fica:

```{r}
Ay <- matrix(c(1, 0, 0, 0, 0, 1, 0, 0), nrow = 2, byrow = TRUE)
Ay

Sigma_yy <- Ay %*% Sigma %*% t(Ay)
Sigma_yy
```

Note que esta partição corresponde ao $\rho_{12} = 0$:

```{r}
Dyy <- sqrt(diag(Sigma_yy))
Dyy

Ro_yy <- solve(diag(Dyy)) %*% Sigma_yy %*% solve(diag(Dyy))
Ro_yy
```

Para a partição $\mathbf{\Sigma}_{xx}$ temos:

```{r}
Ax <- matrix(c(0, 0, 1, 0, 0, 0, 0, 1), nrow = 2, byrow = TRUE)
Ax

Sigma_xx <- Ax %*% Sigma %*% t(Ax)
Sigma_xx
```

Por fim, a partição $\mathbf{\Sigma}_{yx}$:

```{r}
Sigma_yx <- Ay %*% Sigma %*% t(Ax)
Sigma_yx
```

Usando o [Teorema 4 -@sec-T4], obtemos a matriz de covariâncias condicionais de $y_1$ e $y_2$ dado $y_3$ e $y_4$:

$$cov(y \mid x) = \mathbf{\Sigma}_{yy} - \mathbf{\Sigma}_{yx} \mathbf{\Sigma}_{xx}^{-1} \mathbf{\Sigma}_{xy}$$

```{r}
cov_ydx <- Sigma_yy - Sigma_yx %*% solve(Sigma_xx) %*% t(Sigma_yx)
cov_ydx
```

Note que $\mathbf{\Sigma}$ é uma matriz simétrica, logo $\mathbf{\Sigma}_{yx} = \mathbf{\Sigma}'_{xy} \iff \mathbf{\Sigma}_{xy} = \mathbf{\Sigma}'_{yx}$.

Assim, podemos calcular o coeficiente de correlação parcial $\rho_{12.34}$:

```{r}
D <- sqrt(diag(cov_ydx))
D

Ro_ydx <- solve(diag(D)) %*% cov_ydx %*% solve(diag(D))
Ro_ydx
```

Portanto, conhecendo os valores de $y_3$ e $y_4$, a correlação parcial entre $y_1$ e $y_2$ é negativa ($\rho_{12.34} = -0,571$) e diferente da correlação linear simples ($\rho_{12} = 0$).

```{r}
Ro_yy
Ro_ydx
```

### SAS

```{r}
#| eval: false

Ay = {1 0 0 0, 0 1 0 0};
Sigma_yy = Ay*Sigma*t(Ay);
Dyy = sqrt(diag(Sigma_yy));
Ro_yy = inv(Dyy)*Sigma_yy*inv(Dyy);

Ax = {0 0 1 0, 0 0 0 1};
Sigma_xx = Ax*Sigma*t(Ax);

Sigma_yx = Ay*Sigma*t(Ax);

cov_ydx = Sigma_yy-Sigma_yx*inv(Sigma_xx)*t(Sigma_yx);
D = sqrt(diag(cov_ydx));
Ro_ydx = inv(D)*cov_ydx*inv(D);

print Ro_yy[format=8.2] '    ' Ro_ydx[format=8.3];
```
:::

# Capítulo 6 - Regressão Linear Simples

## Estimação e Teste de Hipóteses

O modelo de regressão linear simples para $n$ observações pode ser escrito como:

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i \space , \quad \text{para } i = 1, \dots,n 
$$

em que:

-   $y$ é a variável resposta;

-   $x_i$ é a variável regressora (única variável preditora de $y$);

-   $\beta_0 \text{ e } \beta_1$ são os parâmetros do modelo;

-   $\epsilon_i$ o termo de erro do modelo.

Como suposições do modelo, temos:

-   $E(\epsilon_i) = 0, \forall \space i = 1,\dots,n$ e $E(y_i) = \beta_0 + \beta_1 x_i$, ou seja, $y_i$ só depende de $x_i$ e que outras variações são aleatórias;

-   $var(\epsilon_i) = \sigma^2, \forall \space i = 1,\dots,n$ e $var(y_i) = \sigma^2$, ou seja, a variância não depende dos valores de $x_i$ (homocedasticidade);

-   $cov(\epsilon_i, \epsilon_j) = 0, \forall \space i \ne j$ e $cov(y_i, y_j) = 0$, portanto, não correlacionados entre si.

O exemplo a seguir trata da seguinte situação: *Estudantes de Estatística alegam que as tarefas de casa não ajudam a prepará-los para o exame final. Os escores do exame (*$y$) e das tarefas ($x$) para os 18 alunos da classe foram:

::: panel-tabset
### R

```{r}
y <- c(95,80,0,0,79,77,72,66,98,90,0,95,35,50,72,55,75,66)

x1 <- c(96,77,0,0,78,64,89,47,90,93,18,86,0,30,59,77,74,67)
```

O objeto `y`, variável resposta, corresponde aos escores na prova e o `x1`, variável regressora, aos escores nas tarefas.

### SAS

```{r}
#| eval: false

options nocenter ls=90 ps=1000;

title 'Exemplo 6.2. Relation between exam score (y) and homework score (x)';
proc iml;
y  = {95,80,0,0,79,77,72,66,98,90, 0,95,35,50,72,55,75,66};
x1 = {96,77,0,0,78,64,89,47,90,93,18,86, 0,30,59,77,74,67};
```
:::

### Análise exploratória

Para visualizar a dispersão dos dados, construiremos um gráfico de pontos (gráfico de dispersão) entre as notas dos alunos na prova (`y`) e nas tarefas de casa (`x1`). Para isso, utilizaremos os recursos do pacote `ggplot2`.

```{r}
#| warning: false

library(ggplot2)
```

```{r}
notas <- data.frame(y, x1)
notas

ggplot(data = notas, aes(x = x1, y = y)) +
  geom_point()
```

Primeiramente, criamos um *data frame* com as notas dos alunos. Em seguida, com a função `ggplot()`, atribuímos a variável nota nas tarefas (`x1`) ao eixo das abscissas e a nota na prova (`y`) ao eixo das ordenadas. Com a `geom_point()`, adicionamos a camada gráfica referente à geometria de pontos, ou seja, a geometria do gráfico de dispersão. Note que as funções `ggplot()` e `geom_point()` são ligadas pelo operador `+`.

### Estimação dos parâmetros $\beta_0$ e $\beta_1$ {#sec-estima-beta}

Para estimar os parâmetros $\beta_0$ e $\beta_1$, utilizaremos o **Método dos Mínimos Quadrados Ordinários (MQO)**. O método consiste em achar os estimadores $\hat\beta_0$ e $\hat\beta_1$ que minimizem a soma de quadrados dos desvios $\sum^n_{i=1} (y_i - \hat{y}_i)^2$.

Primeiramente, criaremos os seguintes objetos:

-   `n`: número de observações $n$;

-   `jn`: vetor coluna de 1's $\mathbf{j}$;

-   `Jnn`: matriz de 1's $\mathbf{J}$;

-   `In`: matriz identidade $\mathbf{I}$.

::: panel-tabset
### R

```{r}
n <- length(y)
n

jn <- matrix(data = 1, nrow = n, ncol = 1)
jn

Jnn <- jn %*% t(jn)
Jnn

In <- diag(n)
In
```

As operações matriciais para obter $\hat\beta_1$ e $\hat\beta_0$ são:

$$
\begin{align}
\hat\beta_1 &= \frac{\left[\mathbf{x}' \left(\mathbf{I} - \frac{1}n \mathbf{J} \right) \mathbf{y}\right]} {\left[\mathbf{x}' \left(\mathbf{I} - \frac{1}n \mathbf{J} \right) \mathbf{x}\right]} \\
\hat\beta_0 &= \frac{1}n \mathbf{j}' (\mathbf{y} - \beta_1 \mathbf{x})
\end{align}
$$

```{r}
Beta1 <- as.numeric(t(x1) %*% (In - (1 / n) * Jnn) %*% y / (t(x1) %*% (In - (1 / n) * Jnn) %*% x1))
Beta1

Beta0 <- as.numeric((1 / n) * t(jn) %*% (y - Beta1 * x1))
Beta0
```

O R possui a função `lm()`, que nos retorna as estimativas dos parâmetros de um modelo linear. Para isso, dentro da função, colocamos os valores de `y` e `x1` ligados pelo operador `~`, que representa uma fórmula.

```{r}
lm(y ~ x1)
```

Com isso, a equação predita fica:

$$\hat{y}_i = 10.7269 + 0.8726 x_i$$

Graficamente, temos:

```{r}
ggplot(data = notas, aes(x = x1, y = y)) +
  geom_point() + 
  geom_abline(intercept = 10.7269, slope = 0.8726, color = "red")
```

Com a função `geom_abline()`, declaramos o $\hat\beta_0 = 10.7269$ no argumento `intercept =` e $\hat\beta_1 = 0.8726$ no argumento `slope =`.

Também podemos utilizar a função `geom_smooth()` para construir a reta de regressão. Essa função calcula, automaticamente, os valores do $\hat\beta_0$ e $\hat\beta_1$ para construir a reta.

```{r}
#| message: false

ggplot(data = notas, aes(x = x1, y = y)) +
  geom_point() + 
  geom_smooth(method = "lm", color = "red", se = FALSE)
```

### SAS

```{r}
#| eval: false

n = nrow(y);
jn = j(n,1,1);
Jnn = j(n,n,1);
In = I(n);
X = jn||x1;
y_barra = (1/n)*Jnn*y;
Tot = y-y_barra;
SQTotal = t(Tot)*(Tot);
* pág.135;
Beta1 = t(x1)*(In-(1/n)*Jnn)*y/(t(x1)*(In-(1/n)*Jnn)*x1);
Beta0 = (1/n)*t(jn)*(y - Beta1*x1);
print 'Estimativas dos parâmetros da reta:' Beta0[format=8.4] Beta1[format=8.4],,,;
```
:::

### Estimação da variância ($\sigma^2$) {#sec-estvar}

Para estimar a variância $\sigma^2$, utilizamos a seguinte expressão:

$$
s^2 = 
\frac{(\mathbf{y} - \mathbf{\hat{y}})' \space (\mathbf{y} - \mathbf{\hat{y}})}{n-2} =  \frac{SQRes}{n-2}
$$

Em que $SQRes$ é a soma de quadrados dos resíduos e $\hat{y}$, os valores estimados de $\mathbf{y}$.

::: panel-tabset
### R

```{r}
y_hat <- Beta0 + Beta1 * x1
y_hat

Res <- y - y_hat
Res

SQRes <- t(Res) %*% Res
SQRes

s2 <- as.numeric(SQRes / (n - 2))
s2
```

Para calcular o resíduo padronizado, dividimos o resíduo pelo desvio padrão da variância estimada ($\sqrt{s^2}$).

```{r}
res_pad <- Res / sqrt(s2)
res_pad
```

```{r}
#| echo: false

tab1 <- data.frame(y, y_hat, Res, res_pad)

kableExtra::kbl(
  x = tab1, digits = 4, align = "c",
  col.names = c("Valores observados(`y`)", "Valores estimados(`y_hat`)",
                "Resíduo(`Res`)", "Resíduo Padronizado(`res_pad`)")
) |> 
  kableExtra::kable_styling(position = "center")
```

Variância dos dados originais e desvio padrão estimado:

```{r}
var_y <- (t(y) %*% (In - (1/n) * Jnn) %*% y) / (n - 1)
var_y

s <- sqrt(s2)
s
```

```{r}
#| echo: false

tab2 <- data.frame(var_y, s2, s)

kableExtra::kbl(
  x = tab2, digits = 4, align = "c",
  col.names = c("Variância dos dados originais (`var_y`)", 
                "Variância de y|x (`s2`)",
                "Desvio padrão de y|x (`s`)")
) |> 
  kableExtra::kable_styling(position = "center")
```

### SAS

```{r}
#| eval: false

k = 1;

y_hat = Beta0 + Beta1*x1;
Reg = y_hat-y_barra;
SQReg = t(Reg)*Reg;

Res = y-y_hat;
SQRes = t(Res)*Res;
s2 = SQRes/(n-k-1);
* s2 = (t(y)*y - t(Beta)*t(X)*y)/(n-k-1);
res_pad = res/sqrt(s2);

*pág. 141;
print 'Valores observados(y) e estimados(y_hat), residuo(res) e residuo padronizado(res_pad):',
      '--------------------------------------------------------------------------------------'; 
print y '   ' y_hat [format=8.4] '   ' res [format=8.4]  '   ' res_pad [format=8.4],,,;
```

```{r}
#| eval: false

var_y = (t(y)*(In - (1/n)*Jnn)*y)/(n-1); * Calcula a variância amostral de y;
s = sqrt(s2);

print 'Variância dos dados originais:' var_y [format=10.4],,
      'Variância de y|x:             ' s2[format=10.4],,
      'Desvio padrão de y|x :        ' s[format=10.4] ,,,;
```
:::

### Teste de Hipóteses e Intervalo de Confiança para $\beta_1$

Realizaremos um teste de hipótese $H_0: \beta_1 = 0$, com a suposição que $y_i \sim N(\beta_0+ \beta_1x_i, \space \sigma^2)$ ou $\epsilon_i \sim N(0, \sigma^2)$.

Das propriedades de $\hat\beta_1$ e $s^2$, a estatística t é dada por:

$$
t = \frac{\hat\beta_1}{\sqrt{\frac{s^2}{\sum^n_{i=1}(x_i-\bar{x})^2}}} =  \frac{\hat\beta_1}{\text{Erro padrão}}
$$

::: panel-tabset
### R

```{r}
x_barra <- t(jn) %*% (x1 / n)
x_barra

var_Beta0 <- s2 * ((1 / n) + (x_barra^2 / (t(x1) %*% (In - (1 / n) * Jnn) %*% x1)))
var_Beta0

stderr_Beta0 <- sqrt(var_Beta0)
stderr_Beta0

var_Beta1 <- s2 / (t(x1) %*% (In - (1 / n) * Jnn) %*% x1)
var_Beta1

stderr_Beta1 <- sqrt(var_Beta1)
stderr_Beta1
```

Como valores da estatística t, temos:

```{r}
t0 <- Beta0 / stderr_Beta0
t0

t1 <- Beta1 / stderr_Beta1
t1
```

Para construir os limites de intervalo de confiança:

$$
\hat\beta \pm t_{\alpha/2, n-2} \sqrt{\frac{s^2}{\sum^n_{i=1}(x_i-\bar{x})^2}}
$$

onde $t_{(\alpha/2, n-2)}$ é o percentil de ordem $100(1-\alpha)\%$ da distribuição $t$ com $n-2$ graus de liberdade e $\alpha$ é o nível de significância do teste.

```{r}
ttab <- qt(0.975, n - 2) # quantil t com probabilidade 0.975 e gl = n-2
ttab

liminf0 <- Beta0 - ttab * stderr_Beta0
liminf0

limsup0 <- Beta0 + ttab * stderr_Beta0
limsup0

liminf1 <- Beta1 - ttab * stderr_Beta1
liminf1

limsup1 <- Beta1 + ttab * stderr_Beta1
limsup1
```

Dado que o $t$ calculado para $\beta_1$ é maior que o limite superior, temos evidências para rejeitar $H_0: \beta_1 = 0$, com nível de significância de 5%.

```{r}
#| echo: false

tab3 <- data.frame(
  param = c("Bo", "B1"),
  estimado = c(Beta0, Beta1),
  var = c(var_Beta0, var_Beta1),
  stderr = c(stderr_Beta0, stderr_Beta1),
  t = c(t0, t1),
  ic = c("[-3.301 , 24.755]", "[0.662 , 1.083]")
)

kableExtra::kbl(
  x = tab3, digits = 4, align = "c",
  col.names = c("Parâmetro", "Estimado", "Variância", "Erro Padrão", "t", "IC 95%")
) |> 
  kableExtra::kable_styling(position = "center")
```

### SAS

```{r}
#| eval: false

x_barra = t(jn)*x1/n;
var_Beta0 = s2*(1/n + x_barra**2/(t(x1)*(In-(1/n)*Jnn)*x1));
stderr_Beta0 = sqrt(var_Beta0);

var_Beta1 = s2/(t(x1)*(In-(1/n)*Jnn)*x1);
stderr_Beta1 = sqrt(var_Beta1);
```

```{r}
#| eval: false

ttab = tinv(0.975,n-2);
liminf0 = Beta0-ttab*stderr_Beta0; limsup0 = Beta0+ttab*stderr_Beta0;
liminf1 = Beta1-ttab*stderr_Beta1; limsup1 = Beta1+ttab*stderr_Beta1;
*pág.146;
print Beta0[format=10.4] var_Beta0[format=10.4] stderr_Beta0[format=10.4]
      '     I.C.(Beta0,95%) = ' liminf0[format=10.4] limsup0[format=10.4] ,,, 
      Beta1[format=10.4] var_Beta1[format=10.4] stderr_Beta1[format=10.4] 
      '     I.C.(Beta1,95%) = ' liminf1[format=10.4] limsup1[format=10.4] ,,,;
```
:::

### Coeficiente de Determinação ($R^2$)

O coeficiente de determinação ($R^2$) indica a proporção da variação em $y$ que é explicada pelo modelo ou que é devida à regressão em $x$. É definido como:

$$R^2 = \frac{SQReg}{SQTotal} = 1 - \frac{SQRes}{SQTotal}$$

em que:

-   $SQReg$ é a soma de quadrados da regressão;

-   $SQRes$ é a soma de quadrados dos resíduos;

-   $SQTotal$ é a soma de quadrados total ($SQTotal = SQReg + SQRes$).

::: panel-tabset
### R

Para calcular $SQReg$:

```{r}
y_barra <- (1 / n) * Jnn %*% y
y_barra

Reg <- y_hat - y_barra
Reg

SQReg <- t(Reg) %*% Reg
SQReg
```

Já $SQTotal$:

```{r}
Tot <- y - y_barra
Tot

SQTotal <- t(Tot) %*% Tot
SQTotal

SQTotal <- SQReg + SQRes
SQTotal
```

Assim, o **coeficiente de determinação** é dado por:

```{r}
R2 <- SQReg / SQTotal
R2
```

O **coeficiente de correlação** ($r$) é dado pela raiz quadrada do coeficiente de determinação ($R^2$).

```{r}
r <- sqrt(R2)
r
```

```{r}
#| echo: false

tab4 <- data.frame(R2, r)

kableExtra::kbl(
  x = tab4, digits = 4, align = "c",
  col.names = c("Coeficiente de determinação (R2)", 
                "Coeficiente de correlação (r)")
) |> 
  kableExtra::kable_styling(position = "center")
```

A estatística t para testar $H_0: \beta_1 = 0$ também pode ser expressa em termos de $r$:

$$
t = \frac{r \space \sqrt{n - 2}}{\sqrt{1-r^2}}
$$

```{r}
tcalc1 <- r * sqrt(n - 2) / (sqrt(1 - r^2))
tcalc1

tcalc2 <- Beta1 / stderr_Beta1
tcalc2
```

Com a estatística t, calculamos o p-valor da seguinte maneira:

```{r}
p_valor <- 2 * (1 - pt(abs(tcalc1), n - 2))
p_valor
```

Onde a função `pt()` retorna a função de distribuição da estatística t.

Dado que o p-valor é menor que $\alpha = 0,05$, rejeitamos $H_0: \beta_1 = 0$.

Por fim, podemos ter uma visão geral dos resultados da análise de regressão com a função `summary()`, a partir do modelo criado com a função `lm()`.

```{r}
modelo <- lm(y ~ x1)

summary(modelo)
```

```{r}
#| layout-ncol: 2

plot(modelo)
```

### SAS

```{r}
#| eval: false

R2 = SQReg/SQTotal;  * Coeficiente de determinação - R2';

corr = sqrt(R2);

print '     SQTotal  =     SQReg   +    SQRes',,
       SQTotal[format=12.4] SQReg[format=12.4] SQRes[format=12.4],,,
      'Coeficiente de determinação (R2): ' R2[format=10.4],,,
      'Coeficiente de correlação (r):    ' corr[format=10.4],,,;

tcalc1 = Beta1/stderr_Beta1; * Para testar H0: Beta1 = 0;
tcalc2 = corr*sqrt(n-2)/(sqrt(1-corr**2));
p_valor = 2*(1-cdf('t',abs(tcalc1),n-2));

print 'H0: Beta1 = 0   ' tcalc1[format=10.4] tcalc2[format=10.4] p_valor[format=10.4];

quit;
```
:::

# Capítulo 7 e 8 - Regressão Linear Múltipla

## Estimação {#sec-rlmest}

Em regressão múltipla, assumimos uma relação linear da variável resposta $y$ com mais de uma variável preditora $x_1, x_2, \dots, x_k$, a fim de predizer os valores de $y$. O modelo pode ser representado da seguinte maneira:

$$
y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots \beta_kx_{ik} + \epsilon_i
\space, \quad i = 1,2, \dots , n \space \text{observações}
$$

Matricialmente, o modelo é dado por:

$$
\begin{bmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{bmatrix}
= 
\begin{bmatrix}
1 & x_{11} & x_{12} & \dots & x_{1k} \\ 
1 & x_{21} & x_{22} & \dots & x_{2k} \\ 
\vdots & \vdots & \vdots & \ddots & \vdots\\ 
1 & x_{n1} & x_{n2} & \dots & x_{nk}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_k
\end{bmatrix}
$$

$$\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$$

-   $\mathbf{y}$: vetor dos valores observados $n \times 1$;

-   $\mathbf{X}$: matriz das variáveis preditoras $n \times (k + 1)$, com $n > (k+1)$ e posto $k + 1$;

-   $\boldsymbol{\beta}$: vetor de parâmetros $(k + 1) \times 1$;

-   $\boldsymbol{\epsilon}$: vetor de erros $n \times 1$.

Como suposições do modelos, temos:

-   $E(\boldsymbol{\epsilon}) = 0$ ou $E(\mathbf{y}) = \mathbf{X} \boldsymbol{\beta}$;

-   $cov(\boldsymbol{\epsilon}) = \sigma^2\mathbf{I}$ ou $cov(\mathbf{y}) = \sigma^2\mathbf{I}$.

Os parâmetros $\beta$ são denominados **coeficientes parciais de regressão**, pois expressam o seu efeito coletivo em $E(\mathbf{y})$ na presença das demais variáveis no modelo. Por exemplo, $\beta_1$ mostra o efeito da variável $x_1$ sobre $E(\mathbf{y})$ na presença das demais variáveis $x$. Caso seja retirado algum $x$ deste modelo, o efeito de $x_1$ pode ser diferente.

Para representar esta diferença, utilizamos um asterisco nos parâmetros beta ($\beta^*$) do **modelo reduzido**.

$$
\begin{align}
y &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon \\
y &= \beta_0^* + \beta^*_1x_1 + \epsilon \\
\end{align}
$$

A primeira equação é referente ao **modelo completo**, cujos valores de $\beta_0$ e $\beta_1$ serão diferentes de $\beta^*_0$ e $\beta^*_1$, respectivos ao **modelo reduzido**, caso retirarmos $x_2$ do modelo completo.

O exemplo a seguir é baseado em Freund e Minton (1979, p. 36-39), cujos dados estão representados a seguir.

```{r}
#| echo: false

data.frame(
  Obs = 1:12,
  y = c(2, 3, 2, 7, 6, 8, 10, 7, 8, 12, 11, 14),
  x1 = c(0, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8),
  x2 = c(2, 6, 7, 5, 9, 8, 7, 10, 11, 9, 15, 13)
) |> 
  kableExtra::kbl(align = "c") |> 
  kableExtra::kable_styling(position = "center")
```

::: panel-tabset
### R

```{r}
y <- c(2, 3, 2, 7, 6, 8, 10, 7, 8, 12, 11, 14)

x1 <- c(0, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8)

x2 <- c(2, 6, 7, 5, 9, 8, 7, 10, 11, 9, 15, 13)
```

### SAS

```{r}
#| eval: false

options nodate nocenter ps=1000;
proc iml;
reset fuzz;
*pág.159;
y = {2,3,2,7,6,8,10,7,8,12,11,14};
x1 = {0,2,2,2,4,4,4,6,6,6,8,8};
x2 = {2,6,7,5,9,8,7,10,11,9,15,13};
```
:::

### Estimação dos parâmetros $\beta$

Para estimar os parâmetros $\beta$, utilizamos o Método dos Mínimos Quadrados Ordinários (MQO), com base em uma amostra de $n$ observações $(y_i, x_{i1},x_{i2},\dots,x_{ik})$ para $i = 1,2, \dots , n$.

Dessa forma, precisamos criar os seguintes objetos:

-   `n`: número de observações $n$;

-   `jn`: vetor coluna de 1's $\mathbf{j}$;

-   `Jnn`: matriz de 1's $\mathbf{J}$;

-   `In`: matriz identidade $\mathbf{I}$.

::: panel-tabset
### R

```{r}
n <- length(y)
n

jn <- matrix(data = 1, nrow = n, ncol = 1)
jn

Jnn <- jn %*% t(jn)
Jnn

In <- diag(n)
In
```

Vale a ressalva de que o vetor de $\beta_0$ corresponde ao vetor coluna de 1's $\mathbf{j}$ (`jn`):

```{r}
x0 <- jn
colnames(x0) <- ("x0")

x0
```

Em seguida, criaremos três objetos, cada qual respectivo às possíveis combinações entre as variáveis preditoras. Calcularemos as equações de predição de $y$ sobre:

-   $x_1$ sozinho;

```{r}
X01 <- cbind(x0, x1)
X01
```

-   $x_2$ sozinho;

```{r}
X02 <- cbind(x0, x2)
X02
```

-   $x_1$ e $x_2$ em conjunto.

```{r}
X012 <- cbind(x0, x1, x2)
X012
```

Para cada caso anterior, unimos os vetores das variáveis preditoras, formando a matriz $\mathbf{X}$ (`X01`, `X02`, `X012`) do modelo matricial $\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$.

Para obter os etimadores dos $\beta 's$ de cada um dos três modelos, utilizamos o MQO que minimiza a soma de quadrados dos desvios ($\sum^n_{i=1} (y_i - \hat{y}_i)^2$), onde

$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_{i1} + \hat{\beta}_2x_{i2} + \dots \hat{\beta}_kx_{ik}
$$

é um estimador de $E(y_i)$.

Assim, o vetor $\hat {\boldsymbol{\beta}} = [\hat \beta_0,\hat \beta_1,\dots,\hat \beta_k]'$ que minimiza a soma de quadrados dos desvios é dado por:

$$
\hat{\boldsymbol{\beta}} = (\mathbf{X'X})^{-1} \mathbf{X}' \mathbf{y}
$$

```{r}
Beta01 <- solve(t(X01) %*% X01) %*% t(X01) %*% y
Beta01
```

```{r}
Beta02 <- solve(t(X02) %*% X02) %*% t(X02) %*% y
Beta02
```

```{r}
Beta012 <- solve(t(X012) %*% X012) %*% t(X012) %*% y
Beta012
```

Dessa forma, obtemos as seguintes equações de predição de $y$:

$$
\begin{align}
\hat{y} &= 1,86 + 1,30 x_1 \\
\hat{y} &= 0,86 + 0,78 x_2\\
\hat{y} &= 5,38 + 3,01 x_1 - 1,29 x_2
\end{align}
$$

Como citado anteriormente, os coeficientes de $x_1$ e $x_2$ diferem de acordo com o modelo proposto.

Como propriedades dos estimadores de MQO $\hat {\boldsymbol{\beta}}$:

-   Se $E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta}$ então $\hat{\boldsymbol{\beta}}$ é um estimador não viesado de $\boldsymbol{\beta}$;

-   Se $cov(\mathbf{y}) = \sigma^2\mathbf{I}$, então $cov(\hat {\boldsymbol{\beta}}) = \sigma^2(\mathbf{X'X})^{-1}$.

### SAS

```{r}
#| eval: false

n = nrow(y);
In = I(n);
Jnn = J(n,n,1);
x0 = j(n,1,1); *cria um vetor nx1 de uns;

x01 = x0||x1;
x02 = x0||x2;
x012 = x0||x1||x2;

* Estima vetor Beta nos três modelos;
Beta01 = inv(t(X01)*X01)*t(X01)*y;
Beta02 = inv(t(X02)*X02)*t(X02)*y;
Beta012 = inv(t(X012)*X012)*t(X012)*y;
print '  Modelo: y = b0 + b1*x1 + e          =>     Beta = ' Beta01[format=10.4],,,
      '  Modelo: y = b0 + b2*x2 + e          =>     Beta = ' Beta02[format=10.4],,,
	  '  Modelo: y = b0 + b1*x1 + b2*x2 + e  =>     Beta = ' Beta012[format=10.4],,,,;
print Beta01[format=10.4] Beta02[format=10.4] Beta012[format=10.4];
```
:::

### Estimação da variância ($\sigma^2$)

Estimaremos a variância $\sigma^2$ do modelo completo por uma média das variâncias das amostras:

$$
s^2 = \frac{\mathbf{y'y} - \hat {\boldsymbol{\beta}'}' \mathbf{X'} \mathbf{y}}{n-k-1} = \frac{\text{SQResíduo}}{n-k-1}
$$

onde $n$ é o tamanho amostral e $k$, o número de variáveis $x$.

:::: panel-tabset
### R

No objeto `k`, definimos o número de variáveis $x$ do modelo completo, ou seja, duas variáveis ($x_1$ e $x_2$). Além disso, anteriormente, já definimos ao objeto `n` o número de observações (12 observações).

```{r}
k <- 2
k

n
```

Dessa forma, com a expressão apresentada anteriormente, obtemos a variância estimada $s^2$.

```{r}
s2 <- as.numeric((t(y) %*% y) - (t(Beta012) %*% t(X012) %*% y)) / (n - k - 1)
s2
```

::: callout-note
A variância estimada $s^2$ também poderia ser calculada da seguinte maneira, como realizado na @sec-estvar para o caso da regressão linear simples:

```{r}
#| eval: false

k <- 2

y_hat <- X012 %*% Beta012
colnames(y_hat) <- ("y_hat")

res <- y - y_hat

SQRes <- t(res) %*% res

s2 <- as.numeric(SQRes / (n - k - 1))
```
:::

Para verificar se $s^2$ é um estimador não viesado de $cov(\hat{\boldsymbol{\beta}})$:

$$\widehat{cov(\hat{\boldsymbol{\beta}})} = s^2(\mathbf{X'X})^{-1}$$

```{r}
cov_Beta <- s2 * solve(t(X012) %*% X012)
cov_Beta
```

Dessa forma, temos:

$$
\begin{align}
s^2 &= 2,829 \\
\widehat{cov} &= 
\begin{bmatrix} 
2.757 & 0,687 & -0,647 \\
0,687 & 0,458 & -0,315 \\
-0,647 & -0,315 & 0,236
\end{bmatrix}
\end{align}
$$

### SAS

```{r}
#| eval: false

*pág.168;
* Ajusta modelo com x1 e x2 + vetor de estimativas da resposta + resíduos do modelo;
X = x0||x1||x2;
Beta = inv(t(X)*X)*t(X)*y;
y_hat = X*Beta;
Nome = {'Beta0','Beta1','Beta2'};
print Nome Beta[format=10.4] '   ' X '   ' y[format=10.] '   ' y_hat[format=10.4],,;

* Calcula estimativa de sigma2 no modelo inicial;
res = y-y_hat;
SQRes = t(res)*res;
s2 = SQRes/(n-k-1);
cov_Beta = s2*inv(t(X)*X);

print s2[format=10.3] cov_Beta[format=10.5];;
```
::::

### Modelo de Regressão na Forma Centrada

O modelo de regressão múltipla pode ser escrito, para cada $y_i$, em termos das variáveis $x$ centradas em suas respectivas médias:

$$
\begin{align}
y_i &= \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots \beta_kx_{ik} + \epsilon_i \\
&= \alpha + \beta_1(x_{i1} - \bar{x}_1) + \beta_2(x_{i2} - \bar{x}_2) + \dots + \beta_k(x_{ik} - \bar{x}_k) + \epsilon_i
\end{align}
$$

em que $\alpha = \beta_0 + \beta_1\bar{x}_1 + \beta_2\bar{x}_2 + \dots + \beta_k\bar{x}_k$, com $\bar{x}_j = \frac{(\sum^n_{i=1} x_{ij})}n$, $j = 1,2,\dots,k$.

Na forma matricial, temos:

$$
\mathbf{y} = 
\begin{bmatrix}
\mathbf{j} & \mathbf{X}_c
\end{bmatrix}
\begin{bmatrix}
\alpha \\ \boldsymbol{\beta_1}
\end{bmatrix}
+
\boldsymbol{\epsilon}
$$

onde $\mathbf{j}$ é o vetor coluna de 1's, $\boldsymbol{\beta_1} = [\boldsymbol{\beta_1,\beta_2,\dots,\beta_k}]'$ e

$$
\mathbf{X}_c = \left(\mathbf{I} - \frac{1}n \mathbf{J}\right) \mathbf{X_1} = 
\begin{bmatrix}
x_{11} - \bar{x}_1 & x_{12} - \bar{x}_2 & \dots & x_{1k} - \bar{x}_k \\
x_{21} - \bar{x}_1 & x_{22} - \bar{x}_2 & \dots & x_{2k} - \bar{x}_k \\
\vdots & \vdots & \ddots & \vdots & \\
x_{n1} - \bar{x}_1 & x_{n2} - \bar{x}_2 & \dots & x_{nk} - \bar{x}_k \\
\end{bmatrix}
$$

em que $\mathbf{X_1} = \begin{bmatrix}\mathbf{x_1} & \mathbf{x_2} & \dots & \mathbf{x_k}\end{bmatrix}$.

Os estimadores de MQO de $\alpha$ e $\boldsymbol{\beta_1}$ são dados por:

$$
\begin{bmatrix}
\hat{\alpha} \\ \hat{\boldsymbol{\beta_1}}
\end{bmatrix}
=
\begin{bmatrix}
\bar{y} \\ (\mathbf{X'_c X_c})^{-1} \mathbf{X'_c} \mathbf{y}
\end{bmatrix}
$$

Na forma centrada, o vetor ajustado $\hat{\mathbf{y}}$ fica:

$$
\hat{\mathbf{y}} = \hat{\alpha}\mathbf{j} + \hat{\beta_1} (\mathbf{x_1} - \bar{\mathbf{x}}_1) + \hat{\beta_2} (\mathbf{x_2} - \bar{\mathbf{x}}_2) + \dots + \hat{\beta_k} (\mathbf{x_k} - \bar{\mathbf{x}}_k)
$$

:::: panel-tabset
### R

Obtendo a matriz $\mathbf{X_1} = \begin{bmatrix}\mathbf{x_1} & \mathbf{x_2} & \dots & \mathbf{x_k}\end{bmatrix}$:

```{r}
x1x2 <- cbind(x1, x2)
x1x2
```

Em seguida, obtendo os valores dos $x$ centrados a partir da expressão $\mathbf{X_c} = \left(\mathbf{I} - \frac{1}n \mathbf{J}\right) \mathbf{X_1}$:

```{r}
x1x2c <- (In - (1 / n) * Jnn) %*% x1x2
x1x2c

Xc <- cbind(x0, x1x2c)
Xc
```

Com a matriz $\mathbf{X_c}$, obtemos as estimativas dos coeficientes de regressão $\beta$ com os $x$ centrados a partir da seguinte expressão:

$$
\hat{\boldsymbol{\beta_1}} = (\mathbf{X'_c X_c})^{-1} \mathbf{X'_c} \mathbf{y}
$$

```{r}
Betac <- solve(t(Xc) %*% Xc) %*% t(Xc) %*% y
Betac
```

Para calcular a estimativa de $\sigma^2$ no modelo centrado, utilizamos a mesma expressão do modelo original:

$$
s^2 = \frac{\mathbf{y'y} - \hat {\boldsymbol{\beta_1}}' \mathbf{X_c'} \mathbf{y}}{n-k-1} = \frac{\text{SQResíduo}}{n-k-1}
$$

```{r}
s2c <- as.numeric((t(y) %*% y) - (t(Betac) %*% t(Xc) %*% y)) / (n - k - 1)
s2c
```

::: callout-note
Mais uma vez, a variância estimada $s^2$ também poderia ser calculada da seguinte maneira, como realizado na @sec-estvar para o caso da regressão linear simples:

```{r}
#| eval: false

y_hatc <- Xc %*% Betac
colnames(y_hatc) <- ("y_hatc")

res_c <- y - y_hatc

SQRes_c <- t(res_c) %*% res_c

s2c <- as.numeric(SQRes / (n - k - 1))
```
:::

Novamente, para verificar se $s^2$ é um estimador não viesado de $cov(\hat{\boldsymbol{\beta}})$:

$$\widehat{cov(\hat{\boldsymbol{\beta}})} = s^2(\mathbf{X'X})^{-1}$$

```{r}
cov_Betac <- s2c * solve(t(X012) %*% X012)
cov_Betac
```

Dessa forma, temos o mesmo resultado do modelo original:

$$
\begin{align}
s^2 &= 2,829 \\
\widehat{cov} &= 
\begin{bmatrix} 
2.757 & 0,687 & -0,647 \\
0,687 & 0,458 & -0,315 \\
-0,647 & -0,315 & 0,236
\end{bmatrix}
\end{align}
$$

### SAS

```{r}
#| eval: false

x1x2 = x1||x2;
x1x2c = (In - (1/n)*Jnn)*x1x2;
Xc = x0||x1x2c; 
BetaC = inv(t(Xc)*Xc)*t(Xc)*y;
y_hatc = Xc*Betac;
XcLXc = t(Xc)*Xc;

*pág. 175;
k = 2;
res = y - y_hat;
SQRes = t(res)*res;
s2c = (1/(n-k-1))*SQRes;
cov_Betac = s2*inv(t(X)*X);
Nome = {'Beta0c','Beta1c','Beta2c'};
print 'Estimativas dos parâmetros com colunas x1 e x2 centradas nas médias';
print Nome Betac[format=10.4] '   ' Xc[format=10.4] '   ' y '   ' y_hatC[format=10.4],;
print s2c[format=10.5] cov_Betac[format=10.5];
```
::::

### Coeficiente de Determinação na Regressão com x-fixos

Para calcular o coeficiente de determinação $R^2$ com x-fixos:

$$
R^2 = \frac{\mathbf{y'}\left[\mathbf{X(X'X)}^{-1}\mathbf{X'} - \frac{1}n \mathbf{J}\right]\mathbf{y}}{\mathbf{y'}[\mathbf{I} - \frac{1}n \mathbf{J}]\mathbf{y}}
=
\frac{\text{SQReg}}{\text{SQTot}}
$$

em que $SQReg$ é a soma de quadrados da regressão com $k$ graus de liberdade, associada somente ao efeito das variáveis regressoras $x$; e $SQTot$, a soma de quadrados total corrigida com $n - 1$ graus de liberdade.

::: panel-tabset
### R

```{r}
SQReg <- t(y) %*% (Xc %*% solve(t(Xc) %*% Xc) %*% t(Xc) - (1/n) * Jnn) %*% y
SQReg

SQTot <- t(y) %*% (In - (1/n) * Jnn) %*% y
SQTot

R2 <- SQReg / SQTot
R2
```

A partir do coeficiente de determinação $R^2$, podemos calcular o coeficiente de determinação ajustado ($R^2 \text{ ajustado}$). Esta medida penaliza a inclusão de regressores ao modelo, visto que a inserção de inúmeras variáveis, mesmo que tenham pouco poder preditivo, aumentam o valor do $R^2$. O modelo proposto é dado por:

$$
R^2_{aj} = \frac{(n - 1) R^2-k}{n - k - 1}
$$

em que $n$ é o número de observações e $k$, o número de variáveis $x$.

```{r}
R2aj <- ((n - 1) * R2 - k) / (n - k - 1)
R2aj
```

Comparativamente, percebe-se que o $R^2_{aj}$ é mais rigoroso em relação ao $R^2$.

$$
\begin{align}
R^2 &= 0,8457 \\
R^2_{aj} &= 0,8114
\end{align}
$$

Todavia, para esta situação, ambos indicam uma boa qualidade no ajuste do modelo.

### SAS

```{r}
#| eval: false

*pág.195;
* Calcula coeficiente de determinação;
SQreg = t(y)*(X*inv(t(X)*X)*t(X)-(1/n)*Jnn)*y;
SQTot = t(y)*(In-(1/n)*Jnn)*y;
R2 = SQReg/SQTot;
R2aj = ((n-1)*R2-k)/(n-k-1);
print R2[format=10.4] R2aj[format=10.4];
```
:::

### Resumo dos resultados

Nota-se que o modelo original e o na forma centrada obtiveram diferentes coeficientes estimados.

```{r}
#| echo: false

data.frame(
  beta = Beta012,
  betac = Betac
) |> 
  kableExtra::kbl(
    align = "c",
    col.names = c("Beta", "Beta Centrado")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Apesar disso, os estimadores de $\sigma^2$ de ambos os modelos são os mesmos, bem como $cov(\hat\beta)$ e os valores estimados $\mathbf{\hat y}$.

```{r}
#| echo: false

data.frame(
  s2 = s2,
  s2c = s2c
) |> 
  kableExtra::kbl(
    align = "c",
    col.names = c("s2", "s2 centrado")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

```{r}
#| echo: false

cat("Cov_Beta:\n")
print(cov_Beta, digits = 4)

cat("Cov_Beta Centrado:\n")
print(cov_Betac, digits = 4)
```

```{r}
#| echo: false

data.frame(
  y_hat = X012 %*% Beta012 |> round(3),
  y_hatc = Xc %*% Betac |> round(3)
) |> 
  kableExtra::kbl(
    align = "c",
    col.names = c("y_hat", "y_hat centrado")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

## Teste de Hipótese - Exemplo 1

Desenvolveremos testes de hipóteses para os parâmetros do vetor $\boldsymbol{\beta} = [\beta_1, \beta_2,\dots, \beta_k]'$ no modelo de regressão múltipla

$$\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$$

Faremos as mesmas suposições do @sec-rlmest, em que $\boldsymbol{\epsilon} \sim N_n(\mathbf{0}, \sigma^2\mathbf{I})$ ou $\mathbf{y} \sim N_n(\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})$, onde $\mathbf{X}$ é uma matriz $n \times(k+1)$, de posto $k+1 < n$, sendo $n$ o número de observações e $k$, o número de variáveis $x$.

Isso garante que as somas de quadrados da regressão e dos resíduos são formas quadráticas independentes, sendo pressuposições importantes para a realização da análise de variância.

Do @sec-rlmest, sabemos que:

-   Modelo de regressão múltipla na forma centrada:

$$
\mathbf{y} = 
\begin{bmatrix}
\mathbf{j} & \mathbf{X}_c
\end{bmatrix}
\begin{bmatrix}
\alpha \\ \boldsymbol{\beta_1}
\end{bmatrix}
+
\boldsymbol{\epsilon}
=
\alpha \mathbf{j} + \mathbf{X}_c \boldsymbol{\beta_1} + \boldsymbol{\epsilon}
$$

em que

$$\mathbf{X}_c = \left(\mathbf{I} - \frac{1}n \mathbf{J}\right) \mathbf{X_1}$$

e $\mathbf{X_1} = \begin{bmatrix}\mathbf{x_1} & \mathbf{x_2} & \dots & \mathbf{x_k}\end{bmatrix}$.

-   Estimadores de MQO de $\alpha$ e $\boldsymbol{\beta_1}$:

$$
\begin{bmatrix}
\hat{\alpha} \\ \hat{\boldsymbol{\beta_1}}
\end{bmatrix}
=
\begin{bmatrix}
\bar{y} \\ (\mathbf{X'_c X_c})^{-1} \mathbf{X'_c} \mathbf{y}
\end{bmatrix}
$$

-   Soma de quadrados da regressão (`SQReg`):

$$
SQReg = \mathbf{y'}\left[\mathbf{X(X'X)}^{-1}\mathbf{X'} - \frac{1}n \mathbf{J}\right]\mathbf{y} \space, \quad \text{ com k graus de liberdade}
$$

-   Soma de quadrados total corrigida (`SQTot`):

$$
SQTot = \mathbf{y'}[\mathbf{I} - \frac{1}n \mathbf{J}]\mathbf{y} \space, \quad \text{ com n-1 graus de liberdade}
$$

-   Soma de quadrados de resíduo (`SQRes`):

$$
SQRes = \mathbf{y'} (\mathbf{I - X(X'X)^{-1}X')y} \space, \quad \text{ com (n-k-1) graus de liberdade}
$$

Para o exemplo, utilizaremos os mesmos dados do @sec-rlmest.

```{r}
#| echo: false

data.frame(
  Obs = 1:12,
  y = c(2, 3, 2, 7, 6, 8, 10, 7, 8, 12, 11, 14),
  x1 = c(0, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8),
  x2 = c(2, 6, 7, 5, 9, 8, 7, 10, 11, 9, 15, 13)
) |> 
  kableExtra::kbl(align = "c") |> 
  kableExtra::kable_styling(position = "center")
```

::: panel-tabset
### R

```{r}
y <- c(2, 3, 2, 7, 6, 8, 10, 7, 8, 12, 11, 14)

x0 <- matrix(data = 1, nrow = length(y), ncol = 1)
colnames(x0) <- ("x0")

x1 <- c(0, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8)

x2 <- c(2, 6, 7, 5, 9, 8, 7, 10, 11, 9, 15, 13)

X <- cbind(x0, x1, x2)
X
```

```{r}
k <- 2    # número de variáveis regressoras x

n <- length(y)

jn <- matrix(data = 1, nrow = n, ncol = 1)

Jnn <- jn %*% t(jn)

In <- diag(n)

X12 <- cbind(x1, x2)
X12

Xc <- (In - (1 / n) * Jnn) %*% X12    # Matriz X12 centrada
Xc
```

### SAS

```{r}
#| eval: false

proc iml;

* Exemplo 8.1 - usando dados do Exemplo 7.2  (Tabela 7.1);
* (Freund & Minton, 1979, pág.36-39);
y = {2,3,2,7,6,8,10,7,8,12,11,14};
X = {1 0 2,1 2 6,1 2 7,1 2 5,1 4 9,1 4 8,1 4 7,1 6 10,1 6 11,1 6 9,1 8 15,1 8 13};
print y '   '  X;
```

```{r}
#| eval: false

n = nrow(y);
k = 2; 	* número de variáveis regressoras x's;
X1 = X[,2:3];
In = I(n);
Jnn = J(n,n,1);
Xc = (In - (1/n)*Jnn)*X1;	* Matriz X1 centrada;
```
:::

### Teste de Regressão Global (Geral)

Como hipóteses do teste de regressão global, assumiremos:

$$
\begin{align}
H_0&: \boldsymbol{\beta_1} = 0 \\
H_a&: \text{Pelo menos um dos } \beta's \text{ não é nulo}
\end{align}
$$

onde $\boldsymbol{\beta_1} = [\beta_1, \dots, \beta_k]'$.

Em outras palavras:

$$
\begin{align}
H_0&: \text{Nenhum dos x's prediz y} \\
H_a&: \text{Pelo menos uma das variáveis x's é importante na predição de y}
\end{align}
$$

O quadro da ANOVA para o teste F de $H_0: \boldsymbol{\beta_1} = 0$ é:

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "FV" = c("Devida a Beta1", "Resíduo", "Total"),
  "gl" = c("k", "n - k - 1", "n - 1"),
  "QM" = c("SQReg / k", "SQRes / (n - k - 1)", "SQTot / (n - 1)"),
  "F" = c("QMReg / QMRes", NA, NA)
) |> 
  kableExtra::kbl(align = "c") |> 
  kableExtra::kable_styling(position = "center")
```

onde $n$ são os números de observações e $k$, o número de variáveis $x$.

::: panel-tabset
### R

A soma de quadrados e os graus de liberdade totais são obtidos da seguinte maneira:

```{r}
SQTot <- t(y) %*% (In - (1 / n) * Jnn) %*% y
SQTot

gl_tot <- n - 1
gl_tot
```

Já a soma de quadrados, quadrado médio e os graus de liberdade da regressão:

```{r}
Beta1 <- solve(t(Xc) %*% Xc) %*% t(Xc) %*% y
Beta1

SQReg <- t(Beta1) %*% t(Xc) %*% y
SQReg

gl_reg <- k
gl_reg

QMReg <- SQReg / gl_reg
QMReg
```

Para os resíduos, temos as seguintes soma de quadrados, quadrado médio e graus de liberdade:

```{r}
SQRes <- SQTot - SQReg
SQRes

gl_res <- n - k - 1
gl_res

QMRes <- SQRes / gl_res
QMRes
```

A estatística F e o p-valor:

```{r}
Fcalc1 <- QMReg / QMRes
Fcalc1

p_valor1 <- 1 - pf(Fcalc1, k, n - k - 1)
p_valor1
```

Dessa forma, o quadro da ANOVA para o teste $H_0: \boldsymbol{\beta_1} = 0$ fica:

```{r}
#| echo: false

options(knitr.kable.NA = '')

ftab1 <- qf(0.95, k, n - k - 1)

data.frame(
  "FV" = c("Regressão", "Resíduo", "Total"),
  "gl" = c(gl_reg, gl_res, gl_tot),
  "SQ" = c(SQReg, SQRes, SQTot) |> round(3),
  "QM" = c(QMReg, QMRes, NA) |> round(3),
  "Fcal" = c(Fcalc1, NA, NA) |> round(3),
  "Ftab" = c(ftab1, NA, NA) |> round(2),
  "p.valor" = c(p_valor1, NA, NA) |> round(6)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "Teste de Regressão Global - H0: Beta1 = 0",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que o $F_{cal} > F_{tab}$, para $F(0.05, 2, 9)$, há evidência para rejeitarmos $H_0: \boldsymbol{\beta_1} = 0$ ao nível de 5% de significância, ou seja, pelo menos uma das variáveis regressoras, $x_1$ ou $x_2$, é importante para predizer $y$. Da mesma maneira, o p-valor corrobora com o resultado, pois é menor que 0,05.

### SAS

```{r}
#| eval: false

* Cálculo das SQ´s para testar hipótese H0: Beta1 = 0;
SQTotal = t(y)*(In - (1/n)*Jnn)*y;	* Calcula SQTotal;
gl_total = n-1;

Beta1 = inv(t(Xc)*Xc)*t(Xc)*y;		* Calcula Beta1;
SQReg = t(Beta1)*t(Xc)*y;
gl_reg = k;
QMReg = SQReg/gl_reg;

SQRes = SQTotal - SQReg;
gl_res = n-k-1;
QMRes = SQRes/gl_res;

Fcalc1 = QMReg/QMRes;
p_valor1 = 1-cdf('F',Fcalc1,k,n-k-1);

print 'Seção 8.1: TESTE DE REGRESSÃO GLOBAL - H0: Beta1 = 0',;
print 'Regressão' gl_reg   SQReg[format=10.4] QMReg[format=10.4] Fcalc1[format=10.3] p_valor1[format=10.6],,
      'Resíduo  ' gl_res   SQRes[format=10.4] QMRes[format=10.4],,
      'Total    ' gl_total SQTotal[format=10.4],,,,;
```
:::

#### Teste para um subconjunto de $\beta's$

Neste caso, estamos interessados em testar a hipótese de que um subconjunto das variáveis regressoras $x's$ não é importante para predizer $y$.

Como hipóteses do teste de regressão, assumiremos:

$$
\begin{align}
H_0&: \boldsymbol{\beta_2} = 0 \\
H_a&: \boldsymbol{\beta_2} \ne 0
\end{align}
$$

O **modelo completo**, que inclui todas as variáveis, pode ser escrito como:

$$
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
=
\begin{bmatrix}
\mathbf{X_1} & \mathbf{X_2}
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{\beta_1} \\ \boldsymbol{\beta_2}
\end{bmatrix}
+
\boldsymbol{\epsilon}
=
\mathbf{X_1} \boldsymbol{\beta_1} + \mathbf{X}_2 \boldsymbol{\beta_2} + \boldsymbol{\epsilon}
$$

onde $\boldsymbol{\beta_2}$ apresenta os parâmetros a serem testados em $H_0: \boldsymbol{\beta_2} = 0$.

Já o **modelo reduzido** pela hipótese $H_0: \boldsymbol{\beta_2} = 0$ será:

$$
\mathbf{y} = \mathbf{X_1} \boldsymbol{\beta_1}^* + \boldsymbol{\epsilon}^*
$$

Seja $h$ o número de parâmetros em $\boldsymbol{\beta_2}$, temos:

-   $\mathbf{X_2}$ uma matriz $n \times h$ e $\boldsymbol{\beta_2}$ $h \times 1$;

-   $\mathbf{X_1}$ uma matriz $n \times (k+1-h)$ e $\boldsymbol{\beta_1}$ $(k+1-h) \times 1$.

::: panel-tabset
### R

No caso do nosso exemplo, sob a hipótese $H_0: \boldsymbol{\beta_2} = 0$, os vetores de parâmetros ficam:

$$
\boldsymbol{\beta_1} = 
\begin{bmatrix}
\beta_0 \\ \beta_1
\end{bmatrix}
\text{ e }
\boldsymbol{\beta_2} = \beta_2
$$

```{r}
X01 <- cbind(x0, x1)  # Variáveis importantes em X1Beta1
X01

X2 <- as.matrix(x2)   # Variáveis desprezíveis em X2Beta2
X2
```

Para desenvolver uma estatística para testar $H_0: \boldsymbol{\beta_2} = 0$, precisamos escrever as somas de quadrados em termos de formas quadráticas de $\mathbf{y}$, dado por:

$$
SQ(\boldsymbol{\beta_2} \mid \beta_1) = \mathbf{y'} [\mathbf{A_1-A_2}] \mathbf{y}
$$

onde $A_1 = \mathbf{X(X'X)^{-1}X'}$ e $A_2 = \mathbf{X_1(X'_1X_1)^{-1}X'_1}$.

```{r}
A1 <- X %*% solve(t(X) %*% X) %*% t(X) 
A1

A2 <- X01 %*% solve(t(X01) %*% X01) %*% t(X01)
A2

SQB2_B1 <- t(y) %*% (A1 - A2) %*% y
SQB2_B1
```

Tendo a soma de quadrados $SQ(\boldsymbol{\beta_2} \mid \beta_1)$, podemos calcular o seu quadrado médio ($QM(\boldsymbol{\beta_2} \mid \beta_1)$), bem como a estatística F e o p-valor.

```{r}
h <- ncol(X2)         # Número de parâmetros x em Beta2 (g.l de Beta2|Beta1)
h

QMB2_B1 <- SQB2_B1 / h
QMB2_B1

Fcalc2 <- QMB2_B1 / QMRes
Fcalc2

p_valor2 <- 1 - pf(Fcalc2, h, n - k - 1)
p_valor2
```

Dessa forma, o quadro da ANOVA para o teste $H_0: \boldsymbol{\beta_2} = 0$ fica:

```{r}
#| echo: false

options(knitr.kable.NA = '')

ftab2 <- qf(0.95, h, n - k - 1)

data.frame(
  "FV" = c("Devida a Beta2 ajust. Beta1", "Resíduo", "Total"),
  "gl" = c(h, gl_res, gl_tot),
  "SQ" = c(SQB2_B1, SQRes, SQTot) |> round(3),
  "QM" = c(QMB2_B1, QMRes, NA) |> round(3),
  "Fcal" = c(Fcalc2, NA, NA) |> round(3),
  "Ftab" = c(ftab2, NA, NA) |> round(2),
  "p.valor" = c(p_valor2, NA, NA) |> round(6)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "Teste de Regressão de Subconjunto - H0: Beta2 = 0",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que o $F_{cal} > F_{tab}$, para $F(0.05, 1, 9)$, há evidência para rejeitarmos $H_0: \boldsymbol{\beta_2} = 0$ ao nível de 5% de significância, ou seja, os termos de segunda ordem (em $\boldsymbol{\beta_2}$) não são todos nulos e são importantes para a predição de $\mathbf{y}$.

### SAS

```{r}
#| eval: false

* SQ´s para testar hipótese H0: Beta2 = 0;
X1 = X[,1:2]; 	* Variáveis importantes em X1Beta1;
X2 = X[,3];   	* Variáveis desprezíveis em X2Beta2; 
A1 = X*inv(t(X)*X)*t(X);
A2 = X1*inv(t(X1)*X1)*t(X1);
h = ncol(X2);
SQB2_B1 = t(y)*(A1-A2)*y;
QMB2_B1 = SQB2_B1/h;

Fcalc2 = QMB2_B1/QMRes;
p_valor2 = 1-cdf('F',Fcalc2,h,n-k-1);

print 'Seção 8.2: TESTE PARA SUBCONJUNTO DOS BETA´S - H0: Beta2 = 0',
      '                (MODELO COMPLETO versus MODELO REDUZIDO)',;
print 'Beta2 | Beta1' h SQB2_B1[format=10.4] QMB2_B1[format=10.4] Fcalc2[format=10.4] p_valor2[format=10.4],
      'Resíduo      ' gl_res SQRes[format=10.4] QMRes[format=10.4],,,,;
```
:::

### Hipótese Linear Geral {#sec-hlg}

Na hipótese linear geral, assumimos a hipótese $H_0: \mathbf{C} \boldsymbol{\beta} = 0$, onde $\mathbf{C}$ é uma matriz de coeficientes $q \times (k+1)$ e de posto $q \le k+1$.

$$
\begin{align}
H_0 &: \mathbf{C} \boldsymbol{\beta} = 0 \\
H_a &: \mathbf{C} \boldsymbol{\beta} \ne 0 
\end{align}
$$

Dessa maneira, podemos expressar qualquer tipo de hipótese, de acordo com os coeficientes presentes na matriz $\mathbf{C}$.

Para o exemplo até então utilizado, podemos formular a hipótese $H_0: \boldsymbol{\beta_1} = \boldsymbol{\beta_2} = 0$ usando a hipótese linear geral.

$$
\begin{align}
H_0 &: \boldsymbol{\beta_1} = \boldsymbol{\beta_2} = 0 \\
H_0 &: \mathbf{C} \boldsymbol{\beta} = 
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix} \\
H_0 &:
\begin{bmatrix}
\beta_1 \\ \beta_2
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix}
\end{align}
$$

::: panel-tabset
### R

```{r}
C <- matrix(data = c(0, 1, 0, 0, 0, 1), ncol = 3, byrow = TRUE)
C

Beta <- solve(t(X) %*% X) %*% t(X) %*% y
Beta

CBeta <- C %*% Beta
CBeta
```

A soma de quadrados da hipótese ($SQHip$) é dado por:

$$
SQHip = (\mathbf{C} \hat{\boldsymbol{\beta}})'[\mathbf{C (X'X)^{-1} C'}]^{-1} (\mathbf{C} \hat{\boldsymbol{\beta}})
$$

com $q = \text{ posto}(\mathbf{C}) = \text{número linhas de }\mathbf{C}$ graus de liberdade.

```{r}
SQHip <- t(CBeta) %*% solve(C %*% solve(t(X) %*% X) %*% t(C)) %*% CBeta
SQHip

gl_hip <- nrow(C)
gl_hip

QMHip <- SQHip / gl_hip
QMHip

Fcalc3 <- QMHip / QMRes
Fcalc3

p_valor3 <- 1 - pf(Fcalc3, gl_hip, n - k - 1)
p_valor3
```

Dessa forma, o quadro da ANOVA para o teste $H_0: \boldsymbol{\beta_1} = \boldsymbol{\beta_2} = 0$ utilizando a hipótese linear geral, fica:

```{r}
#| echo: false

options(knitr.kable.NA = '')

ftab3 <- qf(0.95, gl_hip, n - k - 1)

data.frame(
  "FV" = c("H0:B1=B2=0", "Resíduo", "Total"),
  "gl" = c(gl_hip, gl_res, gl_tot),
  "SQ" = c(SQHip, SQRes, SQTot) |> round(3),
  "QM" = c(QMHip, QMRes, NA) |> round(3),
  "Fcal" = c(Fcalc3, NA, NA) |> round(3),
  "Ftab" = c(ftab3, NA, NA) |> round(2),
  "p.valor" = c(p_valor3, NA, NA) |> round(6)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "Teste de Regressão para hipótese linear geral - H0: Beta1 = Beta2 = 0",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que o $F_{cal} > F_{tab}$, para $F(0.05, 2, 9)$, há evidência para rejeitarmos $H_0: \boldsymbol{\beta_1} = \boldsymbol{\beta_2} = 0$ ao nível de 5% de significância.

### SAS

```{r}
#| eval: false

*Hipótese Linear Geral H0: C*Beta = 0 ou H0: beta1=beta2=0;
C = {0 1 0, 0 0 1};
gl_hip = nrow(C);
Beta = inv(t(X)*X)*t(X)*y;
SQHip = t(C*Beta)*inv(C*inv(t(X)*X)*t(C))*C*Beta;
QMHip = SQHip/gl_hip;
Fcalc3 = QMHip/QMRes;
p_valor3 = 1-cdf('F',Fcalc3,gl_hip,n-k-1);

print 'Seção 8.3: H0: B1 = B2 = 0 usando HIPÓTESE LINEAR GERAL',;
print 'H0: B1 = B2 = 0   ' gl_hip SQHip[format=10.4] QMHip[format=10.4] Fcalc3[format=10.3] p_valor3[format=10.6],,
      'Resíduo           ' gl_res SQRes[format=10.4] QMRes[format=10.4],,,,;
```
:::

## Teste de Hipótese - Exemplo 2 {#sec-th2}

```{r}
#| echo: false

options(scipen = 999999, knitr.kable.NA = '')
```

Nesta seção, traremos outro exemplo de teste de hipótese em regressão linear múltipla.

Considere uma reação química para formar um determinado material desejado. Seja a variável dependente $y_2$ o percentual convertido no material desejado e $y_1$ e $y_1$ o percentual de material não convertido.

Como variáveis preditoras temos:

-   $x_1$ = Temperatura (°C);

-   $x_2$ = Concentração do reagente (%);

-   $x_3$ = Tempo de reação (horas).

O **modelo completo** de regressão múltipla de $y_2$ é dado por:

$$
\begin{align}
y_2 = &\beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \beta_4x_1^2 + \beta_5x_2^2 + \beta_6x_3^2 + \beta_7x_1x_2 +\\
&+ \beta_8x_1x_3 + \beta_9x_2x_3 + \epsilon
\end{align}
$$

Os dados estão descritos a seguir:

```{r}
#| echo: false

data.frame(
  y1 = c(41.5,33.8,27.7,21.7,19.9,15.0,12.2,4.3,19.3,6.4,37.6,18.0,26.3,9.9,25.0,14.1,15.2,15.9,19.6),
  y2 = c(45.9,53.3,57.5,58.8,60.6,58.0,58.6,52.4,56.9,55.4,46.9,57.3,55.0,58.9,50.3,61.1,62.9,60.0,60.6),
  x1 = c(162,162,162,162,172,172,172,172,167,177,157,167,167,167,167,177,177,160,160),
  x2 = c(23,23,30,30,25,25,30,30,27.5,27.5,27.5,32.5,22.5,27.5,27.5,20,20,34,34),
  x3 = c(3,8,5,8,5,8,5,8,6.5,6.5,6.5,6.5,6.5,9.5,3.5,6.5,6.5,7.5,7.5)
) |> 
  kableExtra::kbl(align = "c") |> 
  kableExtra::kable_styling(position = "center")
```

::: panel-tabset
### R

```{r}
y1 <- c(41.5,33.8,27.7,21.7,19.9,15.0,12.2,4.3,19.3,6.4,37.6,18.0,26.3,9.9,25.0,14.1,15.2,15.9,19.6)

y2 <- c(45.9,53.3,57.5,58.8,60.6,58.0,58.6,52.4,56.9,55.4,46.9,57.3,55.0,58.9,50.3,61.1,62.9,60.0,60.6)

n <- length(y2)
x0 <- rep(1, n)

x1 <- c(162,162,162,162,172,172,172,172,167,177,157,167,167,167,167,177,177,160,160)

x2 <- c(23,23,30,30,25,25,30,30,27.5,27.5,27.5,32.5,22.5,27.5,27.5,20,20,34,34)

x3 <- c(3,8,5,8,5,8,5,8,6.5,6.5,6.5,6.5,6.5,9.5,3.5,6.5,6.5,7.5,7.5)

x11 <- x1*x1
  
x22 <- x2*x2

x33 <- x3*x3

x12 <- x1*x2

x13 <- x1*x3

x23 <- x2*x3
```

```{r}
X <- cbind(x0, x1, x2, x3, x11, x22, x33, x12, x13, x23)
X
```

```{r}
Jnn <- matrix(data = 1, nrow = n, ncol = n)

In <- diag(n)
```

### SAS

```{r}
#| eval: false

options nodate nocenter ps=1000 ls=120;

proc iml;
reset noprint;
*pág.245;
y1 = {41.5,33.8,27.7,21.7,19.9,15.0,12.2,4.3,19.3,6.4,37.6,18.0,26.3,9.9,25.0,14.1,15.2,15.9,19.6};
* y1 = % de material que não reagiu;
y2 = {45.9,53.3,57.5,58.8,60.6,58.0,58.6,52.4,56.9,55.4,46.9,57.3,55.0,58.9,50.3,61.1,62.9,60.0,60.6};
* y2 = % convertida ao material desejado;
n = nrow(y1);
print n;
In = I(n);
Jnn = J(n,n,1);
* c1 = temperatura; * c2 = concentração de reagente; * c3 = tempo de reação;
c0 = j(n,1,1);
c1 = {162,162,162,162,172,172,172,172,167,177,157,167,167,167,167,177,177,160,160};  
c2 = {23,23,30,30,25,25,30,30,27.5,27.5,27.5,32.5,22.5,27.5,27.5,20,20,34,34};	    
c3 = {3,8,5,8,5,8,5,8,6.5,6.5,6.5,6.5,6.5,9.5,3.5,6.5,6.5,7.5,7.5};					
c11 = c1#c1; * eleva C1 ao quadrado;
c22 = c2#c2; 
c33 = c3#c3; 
c12 = c1#c2; 
c13 = c1#c3;
c23 = c2#c3;
X = c0||c1||c2||c3||c11||c22||c33||c12||c13||c23;
```
:::

### Modelo Completo

Para estimar os parâmetros $\beta$ do **modelo completo**, utilizamos o Método dos Mínimos Quadrados Ordinários (MQO):

$$
\hat{\boldsymbol{\beta}} = (\mathbf{X'X})^{-1} \mathbf{X}' \mathbf{y}
$$

Além disso, calcularemos as somas de quadrados e o quadrado médio do modelo completo.

$$
\begin{align}
&SQReg = \mathbf{y'}\left[\mathbf{X(X'X)}^{-1}\mathbf{X'} - \frac{1}n \mathbf{J}\right]\mathbf{y}, \\
&\text{ com k graus de liberdade}
\end{align}
$$

$$
SQTot = \mathbf{y'}[\mathbf{I} - \frac{1}n \mathbf{J}]\mathbf{y} \space, \quad \text{ com n-1 graus de liberdade}
$$

$$
SQRes = SQTot - SQReg = \mathbf{y'} (\mathbf{I - X(X'X)^{-1}X')y} \space, \quad \text{ com (n-k-1) graus de liberdade}
$$

::: panel-tabset
### R

```{r}
Beta <- solve(t(X) %*% X) %*% t(X) %*% y2
Beta |> round(4)
```

```{r}
SQTotal <- t(y2) %*% (In - (1 / n) * Jnn) %*% y2
SQTotal

gltotal <- n - 1
gltotal
```

```{r}
SQReg <- t(y2) %*% (X %*% solve(t(X) %*% X) %*% t(X) - (1 / n) * Jnn) %*% y2
SQReg

k <- ncol(X) - 1
gl_reg <- k
gl_reg
```

```{r}
SQRes <- SQTotal - SQReg
SQRes

gl_res <- n - k - 1
gl_res

QMRes <- SQRes / gl_res
QMRes
```

### SAS

```{r}
#| eval: false

Beta = inv(t(X)*X)*t(X)*y2;
print Beta[format=12.4];
```

```{r}
#| eval: false

* Modelo completo;
SQTotal = t(y2)*(In-(1/n)*Jnn)*y2;
gltotal = n-1;
SQReg = t(y2)*(X*inv(t(X)*X)*t(X)-(1/n)*Jnn)*y2;
k = ncol(X)-1;
gl_reg = k;
SQRes = SQTotal-SQReg;
glres = n-k-1;
QMRes = SQRes/glres;

print 'Modelo completo:',,, gl_reg SQReg[format=10.4] SQRes[format=10.4] SQTotal[format=10.4],,,;
```
:::

### Hipótese 1

Como primeira hipótese ao nosso modelo, assumiremos $H_0: \beta_4 = \beta_5 = \beta_6 = \beta_7 = \beta_8 = \beta_9 = 0$, ou seja, vamos testar se os termos de segunda ordem do modelo não são importantes na predição de $y_2$.

$$
\begin{align}
H_0&: \beta_4 = \beta_5 = \beta_6 = \beta_7 = \beta_8 = \beta_9 = 0 \\
H_a&: \text{Pelo menos um dos } \beta's \text{ não é nulo}
\end{align}
$$

Com essa hipótese, temos como **modelo reduzido**:

$$y_2 = \beta_0^* + \beta_1^*x_1 + \beta_2^*x_2 + \beta_3^*x_3 + \epsilon^*$$

::: panel-tabset
### R

```{r}
X1 <- cbind(x0, x1, x2, x3)
X1
```

A seguir, calcularemos os graus de liberdade, somas de quadrados, quadrados médios, estatística F e p-valor do modelo reduzido.

```{r}
SQReg1 <- t(y2) %*% (X1 %*% solve(t(X1) %*% X1) %*% t(X1) - (1 / n) * Jnn) %*% y2
SQReg1

gl_regB1 <- ncol(X1) - 1
gl_regB1

SQB2_B1 <- SQReg - SQReg1
SQB2_B1

gl_regB2 <- gl_reg - gl_regB1
h <- gl_regB2
h

QMB2_B1 <- SQB2_B1 / h
QMB2_B1

Fcalc1 <- QMB2_B1 / QMRes
Fcalc1

p_valor1 <- 1 - pf(Fcalc1, h, gl_res)
p_valor1
```

Dessa forma, o quadro da ANOVA sob a hipótese $H_0: \beta_4 = \beta_5 = \beta_6 = \beta_7 = \beta_8 = \beta_9 = 0$ usando a abordagem de modelo completo e modelo reduzido, fica:

```{r}
#| echo: false

options(knitr.kable.NA = '')

ftab1 <- qf(0.95, gl_regB2, gl_res)

data.frame(
  "FV" = c("H0", "Resíduo", "Total"),
  "gl" = c(gl_regB2, gl_res, gltotal),
  "SQ" = c(SQB2_B1, SQRes, SQTotal) |> round(3),
  "QM" = c(QMB2_B1, QMRes, NA) |> round(3),
  "Fcal" = c(Fcalc1, NA, NA) |> round(3),
  "Ftab" = c(ftab1, NA, NA) |> round(2),
  "p.valor" = c(p_valor1, NA, NA) |> round(6)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "Teste de Regressão Global - H0: Beta4 = Beta5 = Beta6 = Beta7 = Beta8 = Beta9 = 0",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que o $F_{cal} > F_{tab}$, para $F(0.05, 6, 9)$, há evidência para rejeitarmos $H_0: \beta_4 = \beta_5 = \beta_6 = \beta_7 = \beta_8 = \beta_9 = 0$ ao nível de 5% de significância, ou seja, os termos de segunda ordem não são todos nulos e são importantes para a predição de $\mathbf{y_2}$.

### SAS

```{r}
#| eval: false

* ----------------------------------------------------------------------------;
* Modelo completo vs reduzido para testar H0: B11 = B22 = B33 = B12 = B13 = B23 = 0 ;
* ----------------------------------------------------------------------------;
X1 = c0||c1||c2||c3;
gl_regB1 = ncol(X1)-1;
* Modelo reduzido (1);
SQReg1 = t(y2)*(X1*inv(t(X1)*X1)*t(X1)-(1/n)*Jnn)*y2;
SQB2_B1 = SQReg - SQReg1;
gl_regB2 = gl_reg - gl_regB1;
h = gl_regB2;
QMB2_B1 = SQB2_B1/h;
Fcalc1 = QMB2_B1/QMRes;
p_valor1 = 1-cdf('F',Fcalc1,h,glres);

print '------------------------------------------------------------',
      'Teste da hipótese H01: B11 = B22 = B33 = B12 = B13 = B23 = 0',
      'usando abordagem Modelo completo x Modelo reduzido',
	  '------------------------------------------------------------';
print 'g.l. do modelo completo .............................................' gl_reg,
      'g.l. do modelo reduzido por H0: B11 = B22 = B33 = B12 = B13 = B23 = 0' gl_regB1,
      'g.l. da diferença (Modelo completo - Modelo reduzido)................' h,,;
print 'Quadro de ANOVA para testar H01: B11 = B22 = B33 = B12 = B13 = B23 = 0',,
'H01      ' h       SQB2_B1[format=10.4] QMB2_B1[format=10.4] Fcalc1[format=10.4] p_valor1[format=10.4],,
'Resíduo  ' glres   SQRes[format=10.4]   QMRes[format=10.4],,
'Total    ' gltotal SQTotal[format=10.4],,,,;
```
:::

### Hipótese 2

Também podemos testar se todos os termos lineares do modelo são nulos, ou seja, $H_0: \beta_1 = \beta_2 = \beta_3 = 0$.

$$
\begin{align}
H_0&: \beta_1 = \beta_2 = \beta_3 = 0 \\
H_a&: \text{Pelo menos um dos } \beta's \text{ não é nulo}
\end{align}
$$

Com essa hipótese, temos como **modelo reduzido**:

$$y_2 = \beta_4^*x_1^2 + \beta_5^*x_2^2 + \beta_6^*x_3^2 + \beta_7^*x_1x_2 + \beta_8^*x_1x_3 + \beta_9^*x_2x_3 + \epsilon^*$$

::: panel-tabset
### R

```{r}
X1 <- cbind(x0, x11, x22, x33, x12, x13, x23)
X1
```

Novamente, calcularemos os graus de liberdade, somas de quadrados, quadrados médios, estatística F e p-valor para o novo modelo reduzido.

```{r}
SQReg1 <- t(y2) %*% (X1 %*% solve(t(X1) %*% X1) %*% t(X1) - (1 / n) * Jnn) %*% y2
SQReg1

gl_regB1 <- ncol(X1) - 1
gl_regB1

SQB2_B1 <- SQReg - SQReg1
SQB2_B1

gl_regB2 <- gl_reg - gl_regB1
h <- gl_regB2
h

QMB2_B1 <- SQB2_B1 / h
QMB2_B1

Fcalc2 <- QMB2_B1 / QMRes
Fcalc2

p_valor2 <- 1 - pf(Fcalc2, h, gl_res)
p_valor2
```

Dessa forma, o quadro da ANOVA sob a hipótese $H_0: \beta_1 = \beta_2 = \beta_3 = 0$ usando a abordagem de modelo completo e modelo reduzido, fica:

```{r}
#| echo: false

options(knitr.kable.NA = '')

ftab2 <- qf(0.95, gl_regB2, gl_res)

data.frame(
  "FV" = c("H0", "Resíduo", "Total"),
  "gl" = c(gl_regB2, gl_res, gltotal),
  "SQ" = c(SQB2_B1, SQRes, SQTotal) |> round(3),
  "QM" = c(QMB2_B1, QMRes, NA) |> round(3),
  "Fcal" = c(Fcalc2, NA, NA) |> round(3),
  "Ftab" = c(ftab2, NA, NA) |> round(2),
  "p.valor" = c(p_valor2, NA, NA) |> round(6)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "Teste de Regressão Global - H0: Beta1 = Beta2 = Beta3 = 0",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que o $F_{cal} > F_{tab}$, para $F(0.05, 3, 9)$, há evidência para rejeitarmos $H_0: \beta_1 = \beta_2 = \beta_3 = 0$ ao nível de 5% de significância, ou seja, pelo menos um dos termos lineares do modelo não são nulos ($x_1$, $x_2$ ou $x_3$), sendo importante para predizer $\mathbf{y_2}$.

### SAS

```{r}
#| eval: false

* ---------------------------------------------------------------;
*  Modelo completo vs reduzido para testar H02: B1 = B2 = B3 = 0 ;
* ---------------------------------------------------------------;
* Modelo reduzido (2);
X1 = c0||c11||c22||c33||c12||c13||c23;
gl_regB1 = ncol(X1)-1;
SQReg1 = t(y2)*(X1*inv(t(X1)*X1)*t(X1)-(1/n)*Jnn)*y2;
SQB2_B1 = SQReg - SQReg1;
gl_regB2 = gl_reg - gl_regB1;
h = gl_regB2;
QMB2_B1 = SQB2_B1/h;
Fcalc2 = QMB2_B1/QMRes;
p_valor2 = 1-cdf('F',Fcalc2,h,glres);

print '--------------------------------------------------',
      'Teste da Hipótese H02: B1 = B2 = B3 = 0           ',
      'usando abordagem Modelo completo x Modelo reduzido',
      '--------------------------------------------------',;

print 'g.l. do modelo completo ................................' gl_reg,
      'g.l. do modelo reduzido por H0: B1 = B2 = B3 = 0 .......' gl_regB1,
      'g.l. da diferença (Modelo completo - Modelo reduzido)...' h,,;
print 'Quadro de ANOVA para testar H02: B1 = B2 = B3 = 0',,
'H02      ' h       SQB2_B1[format=10.4] QMB2_B1[format=10.4] Fcalc2[format=10.4] p_valor2[format=10.4],,
'Resíduo  ' glres   SQRes[format=10.4]   QMRes[format=10.4],,
'Total    ' gltotal SQTotal[format=10.4],,,,;
```
:::

### Hipótese Linear Geral

Os detalhes sobre a hipótese linear geral estão na @sec-hlg.

$$
\begin{align}
H_0 &: \mathbf{C} \boldsymbol{\beta} = 0 \\
H_a &: \mathbf{C} \boldsymbol{\beta} \ne 0 
\end{align}
$$

Aqui, realizaremos o teste sobre a hipótese $H_0: \beta_1 = \beta_2 = \beta_3 = 0$ com a seguinte proposta de matriz de coeficientes $\mathbf{C}$.

$$
\begin{align}
H_0 &: \beta_1 = \beta_2 = \beta_3 = 0 \\
H_0 &: \mathbf{C} \boldsymbol{\beta} = 
\begin{bmatrix}
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix} \\
H_0 &:
\begin{bmatrix}
\beta_1 \\ \beta_2 \\ \beta_3
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix}
\end{align}
$$

::: panel-tabset
### R

```{r}
C <- matrix(
  c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0), 
  ncol = 10, nrow = 3, byrow = TRUE
)
C
```

```{r}
CBeta <- C %*% Beta
CBeta

SQHip <- t(CBeta) %*% solve(C %*% solve(t(X) %*% X) %*% t(C)) %*% CBeta
SQHip

q <- nrow(C)

QMHip <- SQHip / q
QMHip

Fcalc3 <- QMHip / QMRes
Fcalc3

p_valor3 <- 1 - pf(Fcalc3, q, gl_res)
p_valor3
```

O quadro da ANOVA sob a hipótese $H_0: \beta_1 = \beta_2 = \beta_3 = 0$ usando a abordagem da hipótese linear geral, fica:

```{r}
#| echo: false

options(knitr.kable.NA = '')

ftab3 <- qf(0.95, q, gl_res)

data.frame(
  "FV" = c("H0", "Resíduo", "Total"),
  "gl" = c(q, gl_res, gltotal),
  "SQ" = c(SQHip, SQRes, SQTotal) |> round(3),
  "QM" = c(QMHip, QMRes, NA) |> round(3),
  "Fcal" = c(Fcalc3, NA, NA) |> round(3),
  "Ftab" = c(ftab3, NA, NA) |> round(2),
  "p.valor" = c(p_valor3, NA, NA) |> round(6)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "Teste de Regressão com hipótese linear geral - H0: Beta1 = Beta2 = Beta3 = 0",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que o $F_{cal} > F_{tab}$, para $F(0.05, 3, 9)$, há evidência para rejeitarmos $H_0: \beta_1 = \beta_2 = \beta_3 = 0$ ao nível de 5% de significância, ou seja, pelo menos um dos termos lineares do modelo não são nulos ($x_1$, $x_2$ ou $x_3$), sendo importante para predizer $\mathbf{y_2}$.

Com isso, nota-se que tanto a abordagem de modelo completo e modelo reduzido, como a da hipótese linear geral geram o mesmo resultado.

### SAS

```{r}
#| eval: false

* ----------------------------------------------------------------;
* Hipótese linear geral (C*Beta=0) para testar H0:B1 = B2 = B3 = 0;
* ----------------------------------------------------------------;
C = {0 1 0 0 0 0 0 0 0 0,
     0 0 1 0 0 0 0 0 0 0,
	   0 0 0 1 0 0 0 0 0 0};
CBeta = C*Beta;
SQHip = t(CBeta)*inv(C*inv(t(X)*X)*t(C))*CBeta;
q = nrow(C);
QMHip = SQHip/q;
FCalc3 = QMHip/QMRes;
p_valor3 = 1-cdf('F',Fcalc3,q,glres);
print 'Hipótese H0: B1 = B2 = B3 = 0 usando abordagem C*Beta = 0',,,
'Hipótese H0' q       SQHip[format=10.4] QMHip[format=10.4] Fcalc3[format=10.4] p_valor3[format=10.4],,
'Resíduo    ' glres   SQRes[format=10.4] QMRes[format=10.4],,
'Total      ' gltotal SQTotal[format=10.4],,,,;
```
:::

Agora, demonstraremos que a matriz de coeficientes $\mathbf{C}$ pode assumir mais de uma forma para o teste de uma mesma hipótese. Construiremos três matrizes de coeficientes $\mathbf{C}$ (`C1`, `C2` e `C3`), tendo como hipótese $H_0: \beta_1 = \beta_2 = \beta_3$.

$$H_0: \beta_1 = \beta_2 = \beta_3$$

$$
\begin{align}
H_0 &: \mathbf{C_1} \boldsymbol{\beta} = 
\begin{bmatrix}
0 & 1 & -1 &  0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 &  1 & -1 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \\ \beta_5 \\ \beta_6 \\ \beta_7 \\ \beta_8 \\ \beta_9
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix} \\
H_0 &:
\begin{bmatrix}
\beta_1 - \beta_2 \\ \beta_2 - \beta_3
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix}
\iff
\beta_1 = \beta2 \quad ; \quad \beta_2 = \beta3
\end{align}
$$

$$
\begin{align}
H_0 &: \mathbf{C_2} \boldsymbol{\beta} = 
\begin{bmatrix}
0 & 2 & -1 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 &  1 & -1 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \\ \beta_5 \\ \beta_6 \\ \beta_7 \\ \beta_8 \\ \beta_9
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix} \\
H_0 &:
\begin{bmatrix}
2\beta_1 - \beta_2 - \beta_3 \\ \beta_2 - \beta_3
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix}
\iff
2\beta_1 = \beta2 + \beta_3 \quad ; \quad \beta_2 = \beta3
\end{align}
$$

$$
\begin{align}
H_0 &: \mathbf{C_3} \boldsymbol{\beta} = 
\begin{bmatrix}
0 & 1 & 0 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & -1 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \\ \beta_5 \\ \beta_6 \\ \beta_7 \\ \beta_8 \\ \beta_9
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix} \\
H_0 &:
\begin{bmatrix}
\beta_1 - \beta_3 \\ \beta_2 - \beta_3
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix}
\iff
\beta_1 = \beta3 \quad ; \quad \beta_2 = \beta3
\end{align}
$$

::: panel-tabset
### R

```{r}
C1 <- matrix(
  c(0, 1, -1,  0, 0, 0, 0, 0, 0, 0,
    0, 0,  1, -1, 0, 0, 0, 0, 0, 0), 
  ncol = 10, nrow = 2, byrow = TRUE
)
C1
```

```{r}
C2 <- matrix(
  c(0, 2, -1, -1, 0, 0, 0, 0, 0, 0,
    0, 0,  1, -1, 0, 0, 0, 0, 0, 0), 
  ncol = 10, nrow = 2, byrow = TRUE
)
C2
```

```{r}
C3 <- matrix(
  c(0, 1, 0,  -1, 0, 0, 0, 0, 0, 0,
    0, 0, 1, -1, 0, 0, 0, 0, 0, 0), 
  ncol = 10, nrow = 2, byrow = TRUE
)
C3
```

Para aplicarmos cada uma das matrizes de coeficientes, criaremos uma função.

```{r}
hlg <- function(C) {

  # Calculando CBeta
  CBeta <- C %*% Beta |> round(3)
  
  # Calculando SQHip
  SQHip <- t(CBeta) %*% solve(C %*% solve(t(X) %*% X) %*% t(C)) %*% CBeta
  
  # Obtendo o número de linhas de C
  q <- nrow(C)
  
  # Calculando QMHip
  QMHip <- SQHip / q
  
  # Calculando Fcalc
  Fcalc3 <- QMHip / QMRes
  
  # Calculando p_valor
  p_valor3 <- 1 - pf(Fcalc3, q, gl_res)
  
  # Retornando resultados
  return(list(CBeta = CBeta, SQHip = SQHip, QMHip = QMHip, Fcalc = Fcalc3, p_valor = p_valor3))
}
```

Para cada um dos $\mathbf{C}'s$ (`C1`, `C2` e `C3`), aplicaremos o teste com a função criada `hlg()`.

```{r}
# Definindo os valores para C
C_lista <- list(C1 = C1, C2 = C2, C3 = C3)

# Criando uma lista para armazenar os resultados
resultados <- list()

# Calculando os resultados para cada objeto C
for (i in 1:length(C_lista)) {
  resultados[[paste0("C", i)]] <- hlg(C_lista[[i]])
}
```

```{r}
tabela_resultados <- data.frame(
  SQHip = sapply(resultados, function(x) x$SQHip),
  QMHip = sapply(resultados, function(x) x$QMHip),
  Fcalc = sapply(resultados, function(x) x$Fcalc),
  p_valor = sapply(resultados, function(x) x$p_valor)
)

tabela_resultados
```

Apesar de diferentes matrizes de coeficientes $\mathbf{C}$, obtemos os mesmos valores para os testes de hipótese. Como conclusão, não temos evidências para rejeitar a hipótese nula $H_0: \beta_1 = \beta_2 = \beta_3$, ao nível de 5% de significância.

### SAS

```{r}
#| eval: false

* ------------------------------------------------------------------------- ;
* Hipótese linear geral (C*Beta=0) para testar H0: B1 = B2 = B3 (SOLUÇÃO 1) ;
*                           (Matriz C não é única!)                         ;
* ------------------------------------------------------------------------- ;
C = {0 1 -1  0 0 0 0 0 0 0,
     0 0  1 -1 0 0 0 0 0 0};
CBeta = C*Beta;
SQHip = t(CBeta)*inv(C*inv(t(X)*X)*t(C))*CBeta;
q = nrow(C);
QMHip = SQHip/q;
FCalc3 = QMHip/QMRes;
p_valor3 = 1-cdf('F',Fcalc3,q,glres);
print 'H0: B1 = B2 = B3 usando C*Beta = 0 (SOLUÇÃO 1)',,,
'Hipótese H0' q       SQHip[format=10.4] QMHip[format=10.4] Fcalc3[format=10.4] p_valor3[format=10.4],,
'Resíduo    ' glres   SQRes[format=10.4] QMRes[format=10.4],,
'Total      ' gltotal SQTotal[format=10.4],,,,;
```

```{r}
#| eval: false

* ------------------------------------------------------------------------- ;
* Hipótese linear geral (C*Beta=0) para testar H0: B1 = B2 = B3 (SOLUÇÃO 2) ;
* ------------------------------------------------------------------------- ;
C = {0 2 -1 -1 0 0 0 0 0 0,
     0 0  1 -1 0 0 0 0 0 0};
CBeta = C*Beta;
SQHip = t(CBeta)*inv(C*inv(t(X)*X)*t(C))*CBeta;
q = nrow(C);
QMHip = SQHip/q;
FCalc3 = QMHip/QMRes;
p_valor3 = 1-cdf('F',Fcalc3,q,glres);
print 'H0: B1 = B2 = B3 usando C*Beta = 0 (SOLUÇÃO 2)',,,
'Hipótese H0' q       SQHip[format=10.4] QMHip[format=10.4] Fcalc3[format=10.4] p_valor3[format=10.4],,
'Resíduo    ' glres   SQRes[format=10.4] QMRes[format=10.4],,
'Total      ' gltotal SQTotal[format=10.4],,,,;
```

```{r}
#| eval: false

* -------------------------------------------------------------------------;
* Hipótese linear geral (C*Beta=0) para testar H0: B1 = B2 = B3 (SOLUÇÃO 3);
* -------------------------------------------------------------------------;
C = {0 1  0 -1 0 0 0 0 0 0,
     0 0  1 -1 0 0 0 0 0 0};
CBeta = C*Beta;
SQHip = t(CBeta)*inv(C*inv(t(X)*X)*t(C))*CBeta;
q = nrow(C);
QMHip = SQHip/q;
FCalc3 = QMHip/QMRes;
p_valor3 = 1-cdf('F',Fcalc3,q,glres);
print 'H0: B1 = B2 = B3 usando C*Beta = 0 (SOLUÇÃO 3)',,,
'Hipótese H0' q       SQHip[format=10.4] QMHip[format=10.4] Fcalc3[format=10.4] p_valor3[format=10.4],,
'Resíduo    ' glres   SQRes[format=10.4] QMRes[format=10.4],,
'Total      ' gltotal SQTotal[format=10.4],,,,;
quit;
```
:::

## Teste de Hipótese - Exemplo 3

Daremos continuidade à Hipótese Linear Geral, realizando novas hipóteses e combinações de coeficientes da matriz $\mathbf{C}$.

$$
\begin{align}
H_0 &: \mathbf{C} \boldsymbol{\beta} = 0 \\
H_a &: \mathbf{C} \boldsymbol{\beta} \ne 0 
\end{align}
$$

Utilizaremos o mesmo caso abordado no @sec-th2, referente a uma reação química para formar um determinado material desejado.

Aqui, consideraremos o seguinte modelo:

$$
y_{1i} = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \epsilon_i
$$

Realizaremos testes sobre a variável $y_1$, referente ao percentual de material não convertido.

::: panel-tabset
### R

```{r}
y1 <- c(41.5,33.8,27.7,21.7,19.9,15.0,12.2,4.3,19.3,6.4,37.6,18.0,26.3,9.9,25.0,14.1,15.2,15.9,19.6)

n <- length(y1)
x0 <- rep(1, n)

x1 <- c(162,162,162,162,172,172,172,172,167,177,157,167,167,167,167,177,177,160,160)

x2 <- c(23,23,30,30,25,25,30,30,27.5,27.5,27.5,32.5,22.5,27.5,27.5,20,20,34,34)

x3 <- c(3,8,5,8,5,8,5,8,6.5,6.5,6.5,6.5,6.5,9.5,3.5,6.5,6.5,7.5,7.5)
```

```{r}
Jnn <- matrix(data = 1, nrow = n, ncol = n)

In <- diag(n)
```

```{r}
X <- cbind(x0, x1, x2, x3)
X
```

```{r}
k <- ncol(X) - 1	     # número de variáveis regressoras
k
```

Os parâmetros estimados de $\beta$ do modelo completo é dado por:

```{r}
Beta <- solve(t(X) %*% X) %*% t(X) %*% y1
Beta |> round(4)
```

$$
\hat{y}_{1} = 332,111 - 1,546 x_{1} - 1,425 x_{2} - 2,237 x_{3} + \epsilon
$$

### SAS

```{r}
#| eval: false

data ChemReaction;
* x1=temperature x2=concentration x3=time y1=unchanged y2=converted;
input x1 x2 x3 y1 y2;
cards;
162    23.0    3.0    41.5    45.9
162    23.0    8.0    33.8    53.3
162    30.0    5.0    27.7    57.5
162    30.0    8.0    21.7    58.8
172    25.0    5.0    19.9    60.6
172    25.0    8.0    15.0    58.0
172    30.0    5.0    12.2    58.6
172    30.0    8.0     4.3    52.4
167    27.5    6.5    19.3    56.9
177    27.5    6.5     6.4    55.4
157    27.5    6.5    37.6    46.9
167    32.5    6.5    18.0    57.3
167    22.5    6.5    26.3    55.0
167    27.5    9.5     9.9    58.9
167    27.5    3.5    25.0    50.3
177    20.0    6.5    14.1    61.1
177    20.0    6.5    15.2    62.9
160    34.0    7.5    15.9    60.0
160    34.0    7.5    19.6    60.6
;

proc iml;
*pág.262;
* Outra forma de leitura dos dados: a partir de um dataset já criado;
use ChemReaction;
read all var{x1} into x1;
read all var{x2} into x2;
read all var{x3} into x3;
read all var{y1} into y1;
read all var{y2} into y2;

y = y1;
n = nrow(y);
jn = j(n,1,1);
In = I(n);
X = jn||x1||x2||x3;
k = ncol(X)-1;	* k = número de variáveis regressoras;

Beta = inv(t(X)*X)*t(X)*y;
print Beta [format=12.4];
```
:::

### Hipótese 1

A primeira hipótese que testaremos é:

$$
\begin{align}
H_0 &: 2\beta_1 - 2\beta_2 = 2\beta_2 - \beta_3 = 0 \\
H_0 &: \mathbf{C} \boldsymbol{\beta} = 
\begin{bmatrix}
0 & 1 & -1 &  0 \\
0 & 0 &  2 & -1  \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix} \\
H_0 &: \begin{cases}
\beta_1 - \beta_2 = 0 \\
2\beta_2 - \beta_3 = 0
\end{cases} 
\iff
H_0: 2\beta_1 = 2\beta_2 = \beta_3 = 0
\end{align}
$$

::: panel-tabset
### R

```{r}
C <- matrix(
  c(0, -2,  2,  0, 
    0,  0,  2, -1),
  nrow = 2, byrow = TRUE
)
C
```

```{r}
CBeta <- C %*% Beta
CBeta

SQHip <- t(CBeta) %*% solve(C %*% (solve(t(X) %*% X)) %*% t(C)) %*% CBeta
SQHip

gl_Hip <- nrow(C)
gl_Hip

QMHip <- SQHip / gl_Hip
QMHip
```

```{r}
SQRes <- t(y1) %*% (In - X %*% solve(t(X) %*% X) %*% t(X)) %*% y1
SQRes

gl_res <- n - k - 1
gl_res

QMRes <- SQRes / gl_res
QMRes
```

```{r}
Fcalc1 <- QMHip / QMRes
Fcalc1

Ftab1 <- qf(0.95, gl_Hip, gl_res)
Ftab1

p_valor1 <- 1 - pf(Fcalc1, gl_Hip, gl_res) 
p_valor1
```

Dessa forma, o quadro da ANOVA sob a hipótese $H_0: 2\beta_1 - 2\beta_2 = 2\beta_2 - \beta_3 = 0$ ou $H_0: 2\beta_1 = 2\beta_2 = \beta_3$, fica:

```{r}
#| echo: false

options(knitr.kable.NA = '')

ftab1 <- qf(0.95, gl_Hip, gl_res)

data.frame(
  "FV" = c("H0", "Resíduo"),
  "gl" = c(gl_Hip, gl_res),
  "SQ" = c(SQHip, SQRes) |> round(3),
  "QM" = c(QMHip, QMRes) |> round(3),
  "Fcal" = c(Fcalc1, NA) |> round(3),
  "Ftab" = c(ftab1, NA) |> round(3),
  "p.valor" = c(p_valor1, NA) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "Teste de Regressão Global - H0: 2Beta1 = 2Beta2 = 2Beta3 = 0",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que o $F_{cal} < F_{tab}$, para $F(0.05, 2, 15)$, não podemos rejeitar $H_0: \beta_1 - \beta_2 = 2\beta_2 - \beta_3 = 0$ ou $H_0: 2\beta_1 = 2\beta_2 = \beta_3$ ao nível de 5% de significância.

### SAS

```{r}
#| eval: false

C = {0 -2 2  0, 
     0 0  2 -1};
gl_H0 = nrow(C);
CBeta = C*Beta;
SQH0 = t(CBeta)*inv(C*(inv(t(X)*X))*t(C))*CBeta;
QMH0 = SQH0/gl_H0;

SQRes = t(y)*(In - X*inv(t(X)*X)*t(X))*y;
gl_res = n-k-1;
QMRes = SQRes/gl_res;

Fcalc = QMH0/QMRes;
p_valor = 1-cdf('F',Fcalc,gl_H0,gl_res);

print 'Exemplo 8.4.1(b): Exemplo com dados de reação química (Tabela 7.4)',,
	  'Teste H0: 2B1 = 2B2 = B3 ou H0: B1 - B2 = 2B2 - B3 = 0',;
print 'H0        ' gl_H0  SQH0[format=8.4]  QMH0[format=8.4]  Fcalc[format=8.4] p_valor[format=8.4],,
      'Resíduo   ' gl_res SQRes[format=8.4] QMRes[format=8.4],,,,;
```
:::

### Hipótese 2

Agora, testaremos a hipótese $H_0: \beta_1 = \beta_2 = \beta_3$ utilizando diferentes matrizes $\mathbf{C}$:

$$
H_0: \beta_1 = \beta_2 = \beta_3
$$

$$
\begin{align}
H_0 &: \mathbf{C_1} \boldsymbol{\beta} = 
\begin{bmatrix}
0 & 1 & -1 &  0 \\
0 & 0 &  1 & -1  \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix} \\
H_0 &: \begin{cases}
\beta_1 - \beta_2 = 0 \\
\beta_2 - \beta_3 = 0
\end{cases}
\iff
\beta_1 = \beta2 \quad ; \quad \beta_2 = \beta_3
\end{align}
$$

$$
\begin{align}
H_0 &: \mathbf{C_2} \boldsymbol{\beta} = 
\begin{bmatrix}
0 & 1 & -1 &  0 \\
0 & 1 &  0 & -1  \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix} \\
H_0 &: \begin{cases}
\beta_1 - \beta_2 = 0 \\
\beta_1 - \beta_3 = 0
\end{cases}
\iff
\beta_1 = \beta_2 \quad ; \quad \beta_1 = \beta_3
\end{align}
$$

$$
\begin{align}
H_0 &: \mathbf{C_3} \boldsymbol{\beta} = 
\begin{bmatrix}
0 & 2 & -1 & -1 \\
0 & 0 &  1 & -1  \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix} \\
H_0 &: \begin{cases}
2\beta_1 - \beta_2 - \beta_3 = 0 \\
\beta_2 - \beta_3 = 0
\end{cases}
\iff
2\beta_1 = \beta_2 + \beta_3 \quad ; \quad \beta_2 = \beta_3
\end{align}
$$

::: panel-tabset
### R

```{r}
C1 <- matrix(
  c(0, 1, -1,  0, 
    0, 0,  1, -1),
  nrow = 2, byrow = TRUE
)
C1

SQ_C1Beta <- t(C1 %*% Beta) %*% solve(C1 %*% (solve(t(X) %*% X)) %*% t(C1)) %*% C1 %*% Beta
SQ_C1Beta
```

```{r}
C2 <- matrix(
  c(0, 1, -1,  0, 
    0, 1,  0, -1),
  nrow = 2, byrow = TRUE
)
C2

SQ_C2Beta <- t(C2 %*% Beta) %*% solve(C2 %*% (solve(t(X) %*% X)) %*% t(C2)) %*% C2 %*% Beta
SQ_C2Beta
```

```{r}
C3 <- matrix(
  c(0, 2, -1, -1, 
    0, 0,  1, -1),
  nrow = 2, byrow = TRUE
)
C3

SQ_C3Beta <- t(C3 %*% Beta) %*% solve(C3 %*% (solve(t(X) %*% X)) %*% t(C3)) %*% C3 %*% Beta
SQ_C3Beta
```

```{r}
#| echo: false

data.frame(
  SQ_C1 = SQ_C1Beta,
  SQ_C2 = SQ_C2Beta,
  SQ_C3 = SQ_C3Beta
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "Teste de Regressão Hipótese Linear Geral para diversas matrizes C - H0: Beta1 = Beta2 = Beta3 = 0"
  ) |> 
  kableExtra::kable_styling(position = "center")
```

### SAS

```{r}
#| eval: false

* Testar a hipótese H0: B1=B2=B3 usando a hipótese linear geral;
C1 = {0  1 -1  0, 
      0  0  1 -1};
SQ_C1Beta = t(C1*Beta)*inv(C1*(inv(t(X)*X))*t(C1))*C1*Beta;
C2 = {0  1 -1  0, 
      0  1  0 -1};
SQ_C2Beta = t(C2*Beta)*inv(C2*(inv(t(X)*X))*t(C2))*C2*Beta;
C3 = {0  2 -1 -1, 
      0  0  1 -1};
SQ_C3Beta = t(C3*Beta)*inv(C3*(inv(t(X)*X))*t(C3))*C3*Beta;

print 'SQH0 para a hipótese H0: B1=B2=B3 usando diferentes matrizes C, em C*Beta=0',,,
     SQ_C1Beta[format=12.4] SQ_C2Beta[format=12.4]SQ_C3Beta[format=12.4];
```
:::

### Hipótese 3

O próximo teste de hipótese a ser realizado é:

$$
H_0: \beta_1 = \beta_2
$$

Realizaremos o teste utilizando a hipótese linear geral e a abordagem do modelo completo e modelo reduzido.

::: panel-tabset
### R

Pela hipótese linear geral, dada a matriz $\mathbf{C} = [0, 1, -1, 0]$, temos:

```{r}
C <- matrix(c(0, 1, -1, 0), nrow = 1, byrow = TRUE)
C
```

```{r}
CBeta <- C %*% Beta
CBeta

gl_Hip <- nrow(C)
gl_Hip

SQHip <- t(CBeta) %*% solve(C %*% (solve(t(X) %*% X)) %*% t(C)) %*% CBeta
SQHip

QMHip <- SQHip / gl_Hip
QMHip

Fcalc2 <- QMHip / QMRes
Fcalc2

p_valor2 <- 1 - pf(Fcalc2, gl_Hip, gl_res)
p_valor2
```

```{r}
#| echo: false

options(knitr.kable.NA = '')

ftab2 <- qf(0.95, gl_Hip, gl_res)

data.frame(
  "FV" = c("H0", "Resíduo"),
  "gl" = c(gl_Hip, gl_res),
  "SQ" = c(SQHip, SQRes) |> round(3),
  "QM" = c(QMHip, QMRes) |> round(3),
  "Fcal" = c(Fcalc2, NA) |> round(3),
  "Ftab" = c(ftab2, NA) |> round(3),
  "p.valor" = c(p_valor2, NA) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "Teste de Regressão com Hipótese Linear Geral - H0: Beta1 = Beta2",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Pelo método do modelo completo e modelo reduzido, note que a soma de quadrados da hipótese é a mesma da hipótese linear geral.

```{r}
x12 <- x1 + x2           # Modelo reduzido: y = B0 + B12(x1+x2) + B3x3 + e
x12

Xr <- cbind(x0, x12, x3) # Matriz Xr do modelo reduzido
Xr

SQHip2 <- t(y1) %*% (X %*% solve(t(X) %*% X) %*% t(X) - Xr %*% solve(t(Xr) %*% Xr) %*% t(Xr)) %*% y1
SQHip2
```

### SAS

```{r}
#| eval: false

* Teste da hipótese H0: Beta1=Beta2;
* (1) Usando C*Beta=0;
C = {0  1 -1  0};
gl_H0 = nrow(C);

CBeta = C*Beta;
SQH0 = t(CBeta)*inv(C*(inv(t(X)*X))*t(C))*CBeta;
QMH0 = SQH0/gl_H0;
Fcalc = QMH0/QMRes;
p_valor = 1-cdf('F',Fcalc,gl_H0,gl_res);

print 'Teste H0: B1 = B2 usando Hipótese Linear Geral',;
print 'H0        ' gl_H0  SQH0[format=8.4]  QMH0[format=8.4]  Fcalc[format=8.4] p_valor[format=8.4],,
      'Resíduo   ' gl_res SQRes[format=8.4] QMRes[format=8.4],,,,;

* (2) Incorporando H0: Beta1 = Beta2 ao modelo;
x12=x1+x2; * Modelo reduzido: y = B0 + B12(x1+x2) + B3x3 + e;
Xr = jn||x12||x3; * Matriz Xr do modelo reduzido;

SQH0r = t(y)*(X*inv(t(X)*X)*t(X)-Xr*inv(t(Xr)*Xr)*t(Xr))*y;
print 'SQ de H0:B1=B2, usando modelo completo x modelo reduzido:'
    ,,SQH0r[format=12.4]; 

quit;
```
:::

## Método de Bonferroni e Método de Scheffé

Em muitos dos casos, estamos interessados em realizar **diversos** testes de hipótese **separados**, como:

$$
\begin{align}
H_0 &: \beta_j = 0 , \quad \text{para  j = 1,2,...,k} \\
&\text{ou} \\
H_0 &: \boldsymbol{a'_i\beta} = 0 , \quad \text{para  i = 1,2,...}
\end{align}
$$

Quando testamos diversas hipóteses como as apresentadas anteriormente, temos dois diferentes níveis de significância ($\alpha$):

-   Nível de significância **geral** (*Familywise*) ($\alpha_f$);

-   Nível de significância **por comparação** (*Comparison-wise*) ($\alpha_c$).

Quando realizamos um único teste de hipótese (como os apresentados nos capítulos anteriores), utilizamos um nível de significância **geral** $\alpha_f$.

Contudo, ao realizar mais de um teste de hipótese para um mesmo caso, utilizamos um nível de significância **por comparação** $\alpha_c$. Isso acarreta em um **aumento** do nível de significância geral ($\alpha_f$). Probabilisticamente, tem-se:

$$\alpha_f = 1 - (1 - \alpha_c)^k$$

em que $k$ é o número de execuções do teste de hipótese ao nível $\alpha_c$ de significância por teste realizado.

Com isso, fixando um nível de significância $\alpha_c = 0,05$ para cada teste, conforme aumentarmos o número de testes, o nível de significância geral ($\alpha_f$) aumenta.

```{r}
#| echo: false

n_teste <- 1:10

tibble::tibble(
  n_teste = n_teste,
  alpha_f = 1 - (1 - 0.05)^(n_teste) |> round(4)
) |> 
  tidyr::pivot_wider(names_from = n_teste, values_from = alpha_f) |> 
  dplyr::mutate(`Nº testes` = "Alpha f", .before = 1) |> 
  kableExtra::kbl(align = "c") |> 
  kableExtra::kable_styling(position = "center")
```

Para isso, serão apresentados dois métodos de comparação que protegem a inflação do nível $\alpha$-global quando diversos testes são feitos: **método de Bonferroni** e **método de Scheffé**.

Como exemplo, novamente, utilizaremos os dados do @sec-rlmest.

```{r}
#| echo: false

data.frame(
  Obs = 1:12,
  y = c(2, 3, 2, 7, 6, 8, 10, 7, 8, 12, 11, 14),
  x1 = c(0, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8),
  x2 = c(2, 6, 7, 5, 9, 8, 7, 10, 11, 9, 15, 13)
) |> 
  kableExtra::kbl(align = "c") |> 
  kableExtra::kable_styling(position = "center")
```

::: panel-tabset
### R

```{r}
y <- c(2,3,2,7,6,8,10,7,8,12,11,14)

n <- length(y)

x0 <- rep(1, n)

x1 <- c(0,2,2,2,4,4,4,6,6,6,8,8)

x2 <- c(2,6,7,5,9,8,7,10,11,9,15,13)

X <- cbind(x0, x1, x2)
X

k <- ncol(X) - 1

In <- diag(n)
```

```{r}
Beta <- solve(t(X) %*% X) %*% t(X) %*% y
Beta

a1 <- matrix(data = c(0, 1, 0), ncol = 1, byrow = TRUE)
a1

Beta1 <- t(a1) %*% Beta
Beta1

a2 <- matrix(data = c(0,0,1), ncol = 1, byrow = TRUE)
a2

Beta2 <- t(a2) %*% Beta
Beta2
```

```{r}
SQRes <- t(y) %*% (In - X %*% solve(t(X) %*% X) %*% t(X)) %*% y
SQRes

gl_res <- n - k - 1
gl_res

QMRes <- SQRes / gl_res
QMRes     # QMRes = s²
```

Vamos assumir que os testes para $H_0: \beta_j = 0$, para $j = 1,2,\dots,k$, serão executados sem considerar se a hipótese global $H_0: \boldsymbol{\beta_1 = 0}$ foi rejeitada.

Usamos a seguinte estatística:

$$
t_j = \frac{\hat{\beta}_j}{s\sqrt{g_{(j+1,j+1)}}}
$$

```{r}
s <- sqrt(QMRes)
s

g <- solve(t(X) %*% X)
g

t1 <- Beta1 / (s %*% sqrt(t(a1) %*% g %*% a1))
t1

t2 <- Beta2 / (s %*% sqrt(t(a2) %*% g %*% a2))
t2

t_tab <- qt(0.975, n - k - 1)
t_tab

p_valor_t1 <- 2 * (1 - pt(abs(t1), gl_res))
p_valor_t1

p_valor_t2 <- 2 * (1 - pt(abs(t2), gl_res))
p_valor_t2
```

Aqui, consideramos que foram realizados dois testes, com nível de significância de 5% para cada teste.

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "Teste" = c("H0: Beta1 = 0", "H0: Beta2 = 0"),
  "t_calc" = c(t1, t2),
  "t_tab" = c(t_tab, t_tab),
  "p_valor" = c(p_valor_t1, p_valor_t2) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    col.names = c("Teste", "t_calc", "t_tab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

```{r}
p <- 1 - 0.05 / (2 * k)
p

t_tab_Bon <- qt(p, n - k - 1)
t_tab_Bon

t_tab_Scheffe <- sqrt((k + 1) %*% qf(0.95, k + 1, n - k - 1)) |> as.numeric()
t_tab_Scheffe
```

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "Parâmetros" = c("Beta1", "Beta2"),
  "t_calc" = c(t1, t2),
  "t_tab" = c(t_tab, t_tab),
  "t_Bonferroni" = c(t_tab_Bon, t_tab_Bon),
  "t_Scheffé" = c(t_tab_Scheffe, t_tab_Scheffe)
) |> 
  kableExtra::kbl(align = "c") |> 
  kableExtra::kable_styling(position = "center")
```

Nota-se que o valor do $t$ calculado (`t_calc`) para o estimador de $\beta_1$ é maior do que as estatísticas de $t$ tabelado, de $t$ de Bonferroni e de $t$ de Scheffé, assim, rejeita-se a hipótese $H_0: \beta_1 = 0$, com um nível de significância geral $\alpha_f = 0, 05$.

Por outro lado, para o estimador de $\beta_2$, obteve-se um valor absoluto de $t$ maior que o $t$ tabelado, porém inferior a do $t$ de Bonferroni e do $t$ de Scheffé. Assim, utilizando os dois métodos de nível de significância por comparação, não temos evidências para rejeitar a hipótese $H_0: \beta_2 = 0$.

### SAS

```{r}
#| eval: false

proc iml;
y = {2,3,2,7,6,8,10,7,8,12,11,14};
n = nrow(y);
x0 = j(n,1,1);
x1 = {0,2,2,2,4,4,4,6,6,6,8,8};
x2 = {2,6,7,5,9,8,7,10,11,9,15,13};
X = x0||x1||x2;
k = ncol(X)-1;
In = I(n);
```

```{r}
#| eval: false

Beta = inv(t(X)*X)*t(X)*y;
a0 = {1,0,0}; Beta0 = t(a0)*Beta; 
a1 = {0,1,0}; Beta1 = t(a1)*Beta;
a2 = {0,0,1}; Beta2 = t(a2)*Beta;
```

```{r}
#| eval: false

SQRes = t(y)*(In - X*inv(t(X)*X)*t(X))*y;
gl_res = n-k-1;
QMRes = SQRes/gl_res;
s = sqrt(QMRes);
G = inv(t(X)*X);
```

```{r}
#| eval: false

t1 = Beta1/(s*sqrt(t(a1)*G*a1));
t2 = Beta2/(s*sqrt(t(a2)*G*a2));
p_valor_t1 = 2*(1-cdf('t',abs(t1),gl_res));
p_valor_t2 = 2*(1-cdf('t',abs(t2),gl_res));
```

```{r}
#| eval: false

p = 1-0.05/(2*k);
t_tab = tinv(0.975,n-k-1);  					* calcula t-tabelado;
t_Bon = tinv(p,n-k-1);  						* calcula t-tabelado para Método de Bonferroni;
t_Scheffe = sqrt((k+1)*finv(0.95,k+1,n-k-1));	* calcula t-tabelado para Método de Scheffé;
```

```{r}
#| eval: false

print 'Exemplo 8.5.2' ,, 'Testes de hipótese H0: Bi = 0 vs Ha: Bi dif 0',,
	  'H01: B1 = 0  ' 't_cal1 =' t1[format=8.4] '     p-valor = ' p_valor_t1[format=10.4],,
	  'H02: B2 = 0  ' 't_cal2 =' t2[format=8.4] '     p-valor = ' p_valor_t2[format=10.4],,,,
	  '----------------------------------------------',
	  'alfa = 5% => t(0,025; 9 g.l.) ='  t_tab[format=12.4],
	  '----------------------------------------------',,,
	  'Método de Bonferroni', 't(0,0125; 9 g.l.) =' t_Bon[format=12.4],,,,
	  'Método de Scheffé   ', 't-Scheffé         =' t_Scheffe[format=12.4];
quit;
```
:::

# Modelos de Análise de Variância (ANOVA)

## Capítulo 13 - ANOVA com um fator: Caso Balanceado

```{r}
#| echo: false
#| message: false
#| warning: false

options(scipen = 999999, knitr.kable.NA = '')

library(MASS)
```

### Introdução

O modelo de análise de variância (ANOVA) consiste em comparar as médias de tratamentos para alguma variável resposta. O modelo matemático pode ser representado da seguinte maneira:

$$
y_{ij} = \mu + \tau_i + \epsilon_{ij} \space \text{ , para } i = 1,\dots,a \space \text{ e } \space j = 1,\dots,n
$$

em que:

-   $y_{ij}$ é o valor observado na $j$−ésima repetição do $i$−ésimo nível de tratamento;

-   $\mu$ é uma constante comum a todas as observações, geralmente a média geral;

-   $\tau_i$ é o efeito do $i$−ésimo nível de tratamento;

-   $\epsilon_{ij}$ é o erro experimental observado na $j$−ésima repetição do $i$−ésimo nível de tratamento.

Quando o modelo é representado na forma matricial $\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$, a matriz de delineamento $\mathbf{X}$ não é de posto completo (**posto incompleto**), sendo assim um modelo dito **superparametrizado**. Dessa forma, é necessário adequar a metodologia aplicada nos modelos de regressão.

Neste primeiro momento, trataremos dos modelos ANOVA **balanceados**, ou seja, todos os tratamentos apresentam número igual de observações, com **um fator de tratamento** (*one-way model*).

Como exemplo, temos o seguinte caso: *Um pesquisador deseja comparar dois aditivos usados para melhorar o desempenho da gasolina. Suponhamos que:*

-   Sem aditivos: carro percorre $\mu$ quilômetros por litro;

-   Com aditivo 1: acréscimo de $tau_1$ quilômetros por litro;

-   Com aditivo 2: acréscimo de $tau_2$ quilômetros por litro.

Considerando seis carros idênticos e escolhendo, aleatoriamente, três carros para adicionar o aditivo 1 e três carros para adicionar o aditivo 2, ou seja, três repetições para cada tipo de aditivo, o modelo matemático fica:

$$
y_{ij} = \mu + \tau_i + \epsilon_{ij}, \space i = 1,2 \space j = 1,2,3
$$

em que $y_{ij}$ é a quilometragem por litro observada no $j$-ésimo carro que recebeu o $i$-ésimo aditivo.

Matricialmente, $\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$, temos:

$$
\begin{cases}
\mu + \tau_1 + \epsilon_{11} \\
\mu + \tau_1 + \epsilon_{12} \\
\mu + \tau_1 + \epsilon_{13} \\
\mu + \tau_2 + \epsilon_{21} \\
\mu + \tau_2 + \epsilon_{22} \\
\mu + \tau_2 + \epsilon_{23} \\
\end{cases}
=
\begin{bmatrix}
y_{11} \\ y_{12} \\ y_{13} \\ y_{21} \\ y_{22} \\ y_{23}
\end{bmatrix}
=
\begin{bmatrix}
1 & 1 & 0 \\
1 & 1 & 0 \\
1 & 1 & 0 \\
1 & 0 & 1 \\
1 & 0 & 1 \\
1 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
\mu \\ \tau_1 \\ \tau_2
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_{11} \\ \epsilon_{12} \\ \epsilon_{13} \\ \epsilon_{21} \\ \epsilon_{22} \\ \epsilon_{23}
\end{bmatrix}
$$

Note que a matriz $\mathbf{X}$, de dimensão $6 \times 3$, é de posto incompleto (posto($\mathbf{X}$) = 2), assim, não é possível estimar diretamente os parâmetros do vetor $\boldsymbol{\beta} = [\mu,\tau_1,\tau_2]'$, pois apresentam infinitas soluções (**não apresenta solução única**).

A seguir, serão apresentados métodos para estimar os parâmetros do vetor $\boldsymbol{\beta}$, considerando os seguintes valores de $y_{ij}$:

```{r}
#| echo: false

data.frame(
  ad1 = c(14, 16, 15),
  ad2 = c(18, 19, 17)
) |> 
  kableExtra::kbl(align = "c", col.names = c("Aditivo 1 (y1)", "Aditivo 2 (y2)")) |> 
  kableExtra::kable_styling(position = "center")
```

### Estimação dos parâmetros

Os dados descritos anteriormente são os seguintes:

::: panel-tabset
### R

```{r}
y <- as.vector(c(14,16,15,18,19,17))
y

X <- matrix(c(
  1, 1, 0, 
  1, 1, 0,
  1, 1, 0, 
  1, 0, 1, 
  1, 0, 1, 
  1, 0, 1
),
ncol = 3, byrow = TRUE
)
X
```

A matriz $\mathbf{X'X}$ (`XLX`) e o vetor $\mathbf{X'y}$ (`XLy`) são calculados a seguir:

```{r}
XLX <- t(X) %*% X
XLX

XLy <- t(X) %*% y
XLy
```

Com a matriz $\mathbf{X'X}$, obtemos o número de observações totais (6) e o número de observações dos tratamentos (3 para cada aditivo). O vetor $\mathbf{X'y}$ nos retorna o total dos valores observados por tratamento (45 km/L para o aditivo 1 e 54 km/L para o aditivo 2), além do total global (99 km/L).

Para calcular o posto das matrizes $\mathbf{X}$ e $\mathbf{X'X}$, utilizaremos a função `ginv()` do pacote `MASS`, que calcula a inversa generalizada de Moore-Penrose de uma matriz.

```{r}
#| eval: false

install.packages("MASS")
library(MASS)
```

O posto de uma matriz idempotente é o traço do produto de uma matriz pela sua inversa generalizada, ou seja, é a soma dos elementos da diagonal.

```{r}
rank_X <- sum(diag(X %*% ginv(X)))
rank_X

rank_XLX <- sum(diag(XLX %*% ginv(XLX)))
rank_XLX
```

Fazendo a diferença entre o número de parâmetros do modelo (`p`) com o posto da matriz de delineamento $\mathbf{X}$ (`k`), confirmamos que se trata de uma matriz de posto incompleto (`defRank`).

```{r}
# Número de parâmetros
p <- ncol(X)
p

# Posto(X)
k <- rank_X
k

# Deficiência de rank - mostra que é de posto incompleto
defRank <- p - k
defRank
```

Para estimar o vetor de parâmetros $\boldsymbol{\beta} = [\mu,\tau_1,\tau_2]'$, utilizamos o Método dos Mínimos Quadrados Ordinários (MQO) que busca o $\boldsymbol{\hat\beta}$ que minimize a soma de quadrados dos desvios. O sistema de equações normais é dado por:

$$
\mathbf(X'X) \boldsymbol{\hat\beta} = \mathbf{X' y}
$$

Aqui, considera-se que o modelo matricial $\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$ tem $E(\mathbf{y}) = \mathbf{X} \boldsymbol{\beta}$, $cov(\mathbf{y}) = \sigma^2 \mathbf{I}$ e que $\mathbf{X}$ é $n \times p$ de posto $k < p \le n$.

Para obter uma possível solução de $\boldsymbol{\hat\beta}$, podemos utilizar qualquer **inversa generalizada** de $\mathbf{(X'X)^-}$, a partir da seguinte operação:

$$\boldsymbol{\hat\beta} = \mathbf{(X'X)^- X' y}$$

```{r}
Beta0 <- ginv(XLX) %*% t(X) %*% y
Beta0

y_hat0 <- X %*% Beta0
y_hat0
```

Outro método de estimação de $\boldsymbol{\hat\beta}$ é impondo retrições nos parâmetros (**condições marginais**). Abordaremos três tipos:

-   Solução com restrição $\tau_1 = 0$: é a retrição imposta pelo software R.

```{r}
TR <- c(0, 1, 0)
TR

W <- rbind(X, TR)
W

z <- matrix(c(y, 0), ncol = 1)
z

Beta1 <- round(solve(t(W) %*% W) %*% t(W) %*% z, 2)
Beta1

y_hat1 <- X %*% Beta1
y_hat1
```

-   Solução com restrição $\tau_2 = 0$: é a retrição imposta pelo software SAS.

```{r}
TS <- c(0, 0, 1)
TS

W <- rbind(X, TS)
W

z <- matrix(c(y, 0), ncol = 1)
z

Beta2 <- round(solve(t(W) %*% W) %*% t(W) %*% z, 2)
Beta2

y_hat2 <- X %*% Beta2
y_hat2
```

-   Solução com restrição $\tau_1 + \tau_2 = 0$: é a retrição comumente utilizada na estatística experimental.

```{r}
TE <- c(0, 1, 1)
TE

W <- rbind(X, TE)
W

z <- matrix(c(y, 0), ncol = 1)
z

Beta3 <- round(solve(t(W) %*% W) %*% t(W) %*% z, 2)
Beta3

y_hat3 <- X %*% Beta3
y_hat3
```

O resumo dos resultados estão descritos a seguir:

```{r}
#| echo: false

data.frame(ig = Beta0, t1 = Beta1, t2 = Beta2, t3 = Beta3) |> 
  kableExtra::kbl(
    align = "c", caption = "Estimação de beta de acordo com a restrição",
    col.names = c("Inv. Generaliz.", "tau1 = 0", "tau2 = 0", "tau1 + tau2 = 0")
  ) |> 
  kableExtra::kable_styling(position = "center")

data.frame(y = y, ig = y_hat0, t1 = y_hat1, t2 = y_hat2, t3 = y_hat3) |> 
  kableExtra::kbl(
    align = "c", caption = "Estimação de y (y_hat) de acordo com a restrição",
    col.names = c("y", "Inv. Generaliz.", "tau1 = 0", "tau2 = 0", "tau1 + tau2 = 0")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Note que, independentemente do $\boldsymbol{\beta}$ estimado, temos a mesma estimativa de $\mathbf{y}$ (`y_hat` ou $\hat{y}$).

### SAS

```{r}
#| eval: false

proc iml;
reset fuzz;
y = {14,16,15,18,19,17}; * vetor coluna y;
X = {1 1 0, 
     1 1 0,
     1 1 0, 
     1 0 1, 
     1 0 1, 
     1 0 1};
XLX = t(X)*X;
XLy = t(X)*y;
print XLX XLy;

rank_X = trace(X*ginv(X));
rank_XLX = trace(XLX*ginv(XLX));
print rank_X rank_XLX;

p = ncol(X);
k = round(trace(Ginv(XLX)*XLX));  * Cálculo do rank da matriz X;
defRank = p-k;
print 'Número de parâmetros =' p,,
      'Posto(X) =' k,,    
      'Deficiência de rank =' defRank;

Beta0 = Ginv(XLX)*t(X)*y; * Solução usando inversa-G;
y_hat0 = X*Beta0;
print 'Solução com inversa-G:       ' Beta0 y y_hat0;

TR = {0 1 0}; * Restrição do R: t1=0;
z = y//{0};
W = X//TR;
Beta1 = inv(t(W)*W)*t(W)*z;
y_hat1 = X*Beta1;
print 'Solução com restrição: t1=0 (R)        ' Beta1 y_hat1;

TS = {0 0 1}; * Restrição do SAS: t2=0;
z = y//{0};
W = X//TS;
Beta2 = inv(t(W)*W)*t(W)*z;
y_hat2 = X*Beta2;
print 'Solução com restrição: t2=0 (SAS)      ' Beta2 y_hat2;

T = {0 1 1}; * Restrição Estatística Experimental: t1+t2=0;
z = y//{0};
W = X//T;
Beta3 = inv(t(W)*W)*t(W)*z;
y_hat3 = X*Beta3;
print 'Solução com restrição: t1+t2=0 (EstExp)' Beta3 y_hat3;
```
:::

### Funções estimáveis

Uma função $\boldsymbol{\lambda'\beta}$ é estimável no modelo $\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$ se existe um vetor $\mathbf{a}$ tal que $E(\mathbf{a'y}) = \boldsymbol{\lambda'\beta}$.

A seguir, verificaremos se algumas combinações lineares são estimáveis de forma única.

::: panel-tabset
### R

-   $\beta = \mu + \tau_1$:

```{r}
L1 <- c(1, 1, 0)
L1

L1Beta0 <- L1 %*% Beta0
L1Beta0

L1Beta1 <- L1 %*% Beta1
L1Beta1

L1Beta2 <- L1 %*% Beta2
L1Beta2

L1Beta3 <- L1 %*% Beta3
L1Beta3
```

Dado que o produto da combinação linear pelos vetores estimados de $\boldsymbol{\beta}$ são iguais (**invariantes**), dizemos que $\beta = \mu + \tau_1$ é estimável. Portanto, independentemente da estimativa do vetor de parâmetros $\boldsymbol{\beta}$, as estimativas são iguais.

-   $\beta = \tau_1 + \tau_2$:

```{r}
L2 <- c(0, 1, 1)
L2

L2Beta0 <- L2 %*% Beta0
L2Beta0

L2Beta1 <- L2 %*% Beta1
L2Beta1

L2Beta2 <- L2 %*% Beta2
L2Beta2

L2Beta3 <- L2 %*% Beta3
L2Beta3
```

Dado que o produto da combinação linear pelos vetores estimados de $\boldsymbol{\beta}$ são diferentes, dizemos que $\beta = \tau_1 + \tau_2$ não é estimável. Portanto, funções não estimáveis não são invariantes e resultam em estimativas diferentes.

### SAS

```{r}
#| eval: false

L1 = t({1,1,0}); * mi+t1 é estimável!;
L1Beta0 = L1*Beta0;
L1Beta1 = L1*Beta1;
L1Beta2 = L1*Beta2;
L1Beta3 = L1*Beta3;
print '-------------------------------',
      'Função estimável: LBeta = mi+t1',
      '-------------------------------',;
print '  Estimativa R:     ' L1Beta1,, 
      '  Estimativa SAS:   ' L1Beta2,,
      '  Estimativa EstExp:' L1Beta3;

L2 = {0 1 1}; * t1+t2 NÃO é estimável!;
L2Beta0 = L2*Beta0;
L2Beta1 = L2*Beta1;
L2Beta2 = L2*Beta2;
L2Beta3 = L2*Beta3;
print '-----------------------------------------',
      'Função NÃO estimável: L2Beta = T1 + T2   ',
      '-----------------------------------------',;
print '  Estimativa R:     ' L2Beta1,, 
      '  Estimativa SAS:   ' L2Beta2,, 
      '  Estimativa EstExp:' L2Beta3;
quit;
```
:::

### Teste de Hipótese

```{r}
#| echo: false
#| message: false
#| warning: false

options(scipen = 999999, knitr.kable.NA = '')

library(MASS)
```

#### Exemplo

Para exemplificar o teste de hipótese em modelos ANOVA balanceada com um fator, considere o seguinte caso: *Três métodos (A, B, C) de armazenar alimentos congelados foram comparados. A variável resposta é a quantidade de ácido ascórbico (mg/100g) e os dados estão representados a seguir:*

```{r}
#| echo: false

data.frame(
  A = c(14.29, 19.10, 19.09, 16.25, 15.09, 16.61, 19.63),
  B = c(20.06, 20.64, 18.00, 19.56, 19.47, 19.07, 18.38),
  C = c(20.04, 26.23, 22.74, 24.04, 23.37, 25.02, 23.27)
) |> 
  kableExtra::kbl(align = "c") |> 
  kableExtra::kable_styling(position = "center")
```

Como suposições do modelo $\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$ consideraremos:

-   $E(\boldsymbol{\epsilon}) = \mathbf{0}$;

-   $var(\boldsymbol{\epsilon}) = \sigma^2 \mathbf{I}$;

-   $cov(\mathbf{\boldsymbol{\epsilon}}) = 0$;

-   $\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2 \mathbf{I})$.

Novamente, utilizaremos a função `ginv()` do pacote `MASS`.

```{r}
#| eval: false

install.packages("MASS")
library(MASS)
```

#### Estimação de parâmetros

::: panel-tabset
### R

```{r}
y <- as.vector(
  c(14.29, 19.10, 19.09, 16.25, 15.09, 16.61, 19.63,
    20.06, 20.64, 18.00, 19.56, 19.47, 19.07, 18.38,
    20.04, 26.23, 22.74, 24.04, 23.37, 25.02, 23.27)
)
y
```

A matriz de delineamento $\mathbf{X}$ do modelo matricial $\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$ é dada por:

```{r}
X <- matrix(c(
  rep(1,21),
  rep(1,7),rep(0,14),
  rep(0,7),rep(1,7),rep(0,7),
  rep(0,14),rep(1,7)
),
ncol = 4, byrow = FALSE)
X
```

No objeto `kn`, guardaremos o número de valores observados; no `p`, o número de variáveis; e no `k`, o posto de $\mathbf{X}$.

```{r}
kn <- length(y)
kn

p <- ncol(X)
p

k <- sum(diag(X %*% ginv(X)))
k

p - k
```

Note que a diferença entre `p - k` resulta no déficit de rank. Como o resultado é 1, $\mathbf{X}$ é uma matriz de posto incompleto.

Calculando $\mathbf{X'X}$ e $\mathbf{X'y}$, temos:

```{r}
XLX <- t(X) %*% X
XLX

XLy <- t(X) %*% y
XLy
```

Com os resultados anteriores, calcularemos duas estimativas de $\boldsymbol{\beta}$ utilizando duas inversas generalizadas distintas:

$$\boldsymbol{\hat\beta} = \mathbf{(X'X)^- X' y}$$

-   Inversa generalizada de Moore-Penrose:

```{r}
Beta <- ginv(XLX) %*% XLy
Beta
```

-   Inversa generalizada simples (Searle):

```{r}
XLX

igXLX <- (1/7) * matrix(
  c(0, 0, 0, 0,
    0, 1, 0, 0,
    0, 0, 1, 0,
    0, 0, 0, 1),
  ncol = 4, byrow = TRUE
)
igXLX |> fractions()

Beta2 <- igXLX %*% XLy
Beta2
```

A partir das duas soluções de $\mathbf{\beta}'s$, verificaremos se as seguintes funções são estimáveis:

-   $\beta = \alpha_1 - \alpha_2$:

```{r}
L1 <- c(0, 1, -1, 0)
L1

L1Beta <- t(L1) %*% Beta
L1Beta

L1Beta2 <- t(L1) %*% Beta2
L1Beta2
```

-   $\beta = \alpha_1 + \alpha_2 + \alpha_3$:

```{r}
L2 <- c(0, 1, 1, 1)
L2

L2Beta <- t(L2) %*% Beta
L2Beta

L2Beta2 <- t(L2) %*% Beta2
L2Beta2
```

Como se pode notar, $\beta = \alpha_1 - \alpha_2$ é uma função estimável e $\beta = \alpha_1 + \alpha_2 + \alpha_3$ é uma função não estimável.

### SAS

```{r}
#| eval: false

options nodate nocenter ps=1000;

proc iml;
*reset print;
reset fuzz;
y = {14.29,19.10,19.09,16.25,15.09,16.61,19.63,
     20.06,20.64,18.00,19.56,19.47,19.07,18.38,
     20.04,26.23,22.74,24.04,23.37,25.02,23.27};
X = {1 1 0 0,1 1 0 0,1 1 0 0,1 1 0 0,1 1 0 0,1 1 0 0,1 1 0 0,
     1 0 1 0,1 0 1 0,1 0 1 0,1 0 1 0,1 0 1 0,1 0 1 0,1 0 1 0,
     1 0 0 1,1 0 0 1,1 0 0 1,1 0 0 1,1 0 0 1,1 0 0 1,1 0 0 1};
print X[format=5.0] y[format=8.2];
kn = nrow(y); 
p = ncol(X); 
k = round(trace(X*ginv(X))); * Calcula o posto de XlinhaX;
print 'rank(X) =' k;
XLX = t(X)*X;
XLy = t(X)*y;
print 'Sistema de equações normais:' XLX XLy;

Beta = ginv(XLX)*XLy; 	* Usando inversa generalizada de Moore-Penrose;

igXLX = (1/7)*{0 0 0 0,
               0 1 0 0, 
               0 0 1 0, 
               0 0 0 1}; 
Beta2 = igXLX*XLy;
print 'Duas soluções:' Beta[format=12.4] Beta2[format=12.4];

L = {0, 1,-1,0}; * Lambda da função alfa1-alfa2 (estimável!);
LBeta = t(L)*Beta;
LBeta2 = t(L)*Beta2;
print 'Função estimável: alfa1-alfa2:',,LBeta[format=12.4] LBeta2[format=12.4];

L = {0,1,1,1}; * Lambda da função alfa1+alfa2+alfa3 (NÃO estimável!);
LBeta = t(L)*Beta;
LBeta2 = t(L)*Beta2;
print 'Função NÃO estimável: alfa1+alfa2+alfa3:',,LBeta[format=12.4] LBeta2[format=12.4];
```
:::

#### Teste de hipótese

A seguir, testaremos a seguinte hipótese, seguindo o **modelo reparametrizado de médias de caselas**, $y_{ij} = \mu_{i} + \epsilon_{ij}$, em que $\mu_i = \mu + \alpha_i$:

$$
\begin{align}
H_0&: \mu_1 = \mu_2 = \mu_3 \\
H_a&: \text{Pelo menos duas médias diferem entre si}
\end{align}
$$

Como $\mu_i = \mu + \alpha_i$, a hipótese anterior é equivalente a seguinte:

$$
\begin{align}
H_0&: \alpha_1 = \alpha_2 = \alpha_3 \\
H_a&: \text{Pelo menos dois tratamentos diferem entre si}
\end{align}
$$

Utilizaremos duas abordagens para a realização do teste de hipótese: **modelo completo x modelo reduzido** e **contrastes**.

##### Modelo Completo x Modelo Reduzido

::: panel-tabset
### R

```{r}
In <- diag(kn)
Jn <- matrix(1, nrow = kn, ncol = kn)
```

A soma de quadrados total (`SQTotal`) é dada por:

$$
SQTotal = \mathbf{y'} \left(\mathbf{I} - \frac{1}n \mathbf{J}\right) \mathbf{y}
$$

E seus graus de liberdade, pelo traço do produto de sua matriz núcleo pela inversa generalizada.

```{r}
# Total
Tot <- In - (1 / kn) * Jn

SQTotal <- t(y) %*% Tot %*% y
SQTotal

gl_total <- round(sum(diag(Tot %*% ginv(Tot))))
gl_total
```

A soma de quadrados para os $\alpha's$ ajustada para $\mu$ ($SQ(\alpha \mid \mu)$) pode ser expressa como uma forma quadrática de $\mathbf{y}$:

$$
SQ(\alpha \mid \mu) = \mathbf{y'} \left[\mathbf{X(X'X)^-X'} - \frac{1}n \mathbf{J}\right] \mathbf{y}
$$

```{r}
# Tratamentos
A <- X %*% ginv(t(X) %*% X) %*% t(X) - (1 / kn) * Jn

SQTrat <- t(y) %*% A %*% y
SQTrat

gl_trat <- round(sum(diag(A %*% ginv(A))))
gl_trat

QMTrat <- SQTrat / gl_trat
QMTrat
```

A soma de quadrados dos resíduos (`SQRes`) é dada por:

$$
SQRes = \mathbf{y'} \left[\mathbf{I} - \mathbf{X(X'X)^-X'}\right] \mathbf{y}
$$

```{r}
# Resíduo
B <- In - X %*% ginv(t(X) %*% X) %*% t(X)

SQRes <- t(y) %*% B %*% y
SQRes

gl_res <- round(sum(diag(B %*% ginv(B))))
gl_res

QMRes <- SQRes / gl_res
QMRes
```

Com os quadrados médios de tratamento (`QMTrat`) e dos resíduos (`QMRes`), calculamos a estatística F, bem como o p-valor.

```{r}
Fcalc <- QMTrat / QMRes
Fcalc

ftab <- qf(0.95, gl_trat, gl_res)
ftab

p_valor <- 1 - pf(Fcalc, gl_trat, gl_res)
p_valor
```

A seguir, os resultado estão descritos no quadro de ANOVA:

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "FV" = c("Tratamento", "Resíduo", "Total"),
  "gl" = c(gl_trat, gl_res, gl_total),
  "SQ" = c(SQTrat, SQRes, SQTotal) |> round(3),
  "QM" = c(QMTrat, QMRes, NA) |> round(3),
  "Fcal" = c(Fcalc, NA, NA) |> round(3),
  "Ftab" = c(ftab, NA, NA) |> round(3),
  "p.valor" = c(p_valor, NA, NA) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "ANOVA para os dados de ácido ascórbico. H0: mi1 = mi2 = mi3",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que $F_{cal} > F_{tab}$, para $F_{(0,05;2;18)}$, rejeita-se a hipótese $H_0: \mu_1 = \mu_2 = \mu_3$, indicando que as médias de pelo menos dois métodos de congelamento diferem entre si.

### SAS

```{r}
#| eval: false

T = I(kn)-(1/kn)*J(kn,kn,1);	* Matriz núcleo;
SQTotal = t(y)*T*y;				* Calcula SQTotal corrigida pela média;
gl_total = round(trace(T*ginv(T))); 

A = X*ginv(t(X)*X)*t(X) -(1/kn)*J(kn,kn,1); 	* Matriz núcleo;
SQTrat = t(y)*A*y;								* Calcula SQTrat;
gl_trat = round(trace(A*ginv(A)));
QMTrat = SQTrat/gl_trat;

B = I(kn) - X*ginv(t(X)*X)*t(X);	* Matriz núcleo;
SQRes = t(y)*B*y;					* Calcula SQResiduo;
gl_res = round(trace(B*ginv(B)));
QMRes = SQRes/gl_res;

Fcalc = QMTrat/QMRes;
p_valor = 1-cdf('F',Fcalc,gl_trat,gl_Res);

print 'TABELA 12.3 - ANOVA para os dados de ácido ascórbico da Tabela 13.2',; 
print 'Método ' gl_trat  SQTrat[format=12.4] QMTrat[format=12.4] Fcalc[format=10.4] p_valor[format=12.4],,
      'Resíduo' gl_res   SQRes[format=12.4]  QMRes[format=12.4],,
	  'Total  ' gl_total SQTotal[format=12.4];
```
:::

##### Contrastes

#### Caso 1: As linhas são linearmente independentes (l.i.) e ortogonais {.unnumbered}

Considerando os **constrastes ortogonais** $2\mu_1 - \mu_2 - \mu_3$ e $\mu_2 - \mu_3$, podemos representá-los da seguinte maneira:

$$
\begin{align}
H_{01} &:2\mu_1 - \mu_2 - \mu_3 = 2\alpha_1 - \alpha_2 - \alpha_3 = [0,2,-1,-1]\boldsymbol{\beta} = \mathbf{c'_1}\boldsymbol{\beta}\\
H_{02} &:\mu_2 - \mu_3 = \alpha_2 - \alpha_3 = [0,0,1,-1]\boldsymbol{\beta} = \mathbf{c'_2}\boldsymbol{\beta}
\end{align}
$$

As hipóteses $H_{01}: \mathbf{c'_1}\boldsymbol{\beta} = 0$ e $H_{02}: \mathbf{c'_2}\boldsymbol{\beta} = 0$ comparam a média do primeiro tratamento com a dos outros dois e a média do segundo tratamento com a do terceiro, respectivamente.

$$
\begin{align}
H_{01} &:\mathbf{c'_1}\boldsymbol{\beta} = 0 \Leftrightarrow \mu_1 = \frac{\mu_2 + \mu_3}2 \\
H_{02} &: \mathbf{c'_2}\boldsymbol{\beta} = 0 \Leftrightarrow \mu_2 = \mu_3
\end{align}
$$

::: panel-tabset
### R

```{r}
C1 <- c(0, 2, -1, -1)
C1Beta <- C1 %*% Beta
C1Beta

C2 <- c(0, 0, 1, -1)
C2Beta <- C2 %*% Beta
C2Beta
```

```{r}
# Partição da SQTrat
SQC1Beta <- t(C1Beta) %*% solve(t(C1) %*% ginv(t(X) %*% X) %*% C1) %*% (C1Beta)
SQC1Beta

SQC2Beta <- t(C2Beta) %*% solve(t(C2) %*% ginv(t(X) %*% X) %*% C2) %*% (C2Beta)
SQC2Beta
```

```{r}
SomaSQ <- SQC1Beta + SQC2Beta

SQTrat
```

```{r}
#| echo: false

cat(
  "Soma das SQ de contrastes ortogonais =", SomaSQ, "\n",
  "SQTrat =", SQTrat
  )
```

Aqui, note que a soma das somas de quadrados dos contrastes (`SomaSQ`) são iguais a soma de quadrados de tratamentos (`SQTrat`).

```{r}
# QM
QMC1Beta <- SQC1Beta / 1
QMC1Beta

QMC2Beta <- SQC2Beta / 1
QMC2Beta
```

```{r}
# F e p-valor
FC1 <- QMC1Beta / QMRes
FC1

FC2 <- QMC2Beta / QMRes
FC2

ftab <- qf(0.95, 1, 18)
ftab

p_valorC1 <- 1 - pf(FC1, 1, gl_res)
p_valorC1

p_valorC2 <- 1 - pf(FC2, 1, gl_res)
p_valorC2
```

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "FV" = c("Contraste c'1", "Contraste c'2", "Resíduo", "Total"),
  "gl" = c(1, 1, gl_res, gl_total),
  "SQ" = c(SQC1Beta, SQC2Beta, SQRes, SQTotal) |> round(3),
  "QM" = c(QMC1Beta, QMC2Beta, QMRes, NA) |> round(3),
  "Fcal" = c(FC1, FC2, NA, NA) |> round(3),
  "Ftab" = c(ftab, ftab, NA, NA) |> round(3),
  "p.valor" = c(p_valorC1, p_valorC2, NA, NA) |> round(5)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "ANOVA para contrastes ortogonais dos dados de ácido ascórbico",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Ambos os $F_{cal}$ são superiores ao valor tabelado $F_{(0,05;1,18)}$. Assim, ambas as hipóteses $H_{01}$ e $H_{02}$ são rejeitadas e se conclui que:

$$
\mu_1 \ne \frac{\mu_2 + \mu_3}2 \quad e \quad \mu_2 \ne \mu_3
$$

### SAS

```{r}
#| eval: false

C1 ={0 2 -1 -1};
C2 ={0 0  1 -1};
C1Beta = C1*Beta;
C2Beta = C2*Beta;
SQC1Beta = t(C1*Beta)*inv(C1*ginv(t(X)*X)*t(C1))*C1*Beta;
SQC2Beta = t(C2*Beta)*inv(C2*ginv(t(X)*X)*t(C2))*C2*Beta;
Soma = SQC1Beta + SQC2Beta;
print 'Partição da SQTrat usando k=2 contrastes l.i. e ortogonais',,
      'SQcontraste1 = ' SQC1Beta[format=8.4],, 'SQcontraste2 = ' SQC2Beta[format=8.4],,
      'Soma SQs     = ' Soma[format=8.4] '    SQTrat = ' SQTrat[format=8.4];
```
:::

#### Caso 2: As linhas são l.i. e não ortogonais {.unnumbered}

Agora, consideraremos os **constrastes não ortogonais** $2\mu_1 - \mu_2 - \mu_3$ e $\mu_1 - \mu_3$, podemos representá-los da seguinte maneira:

$$
\begin{align}
2\mu_1 - \mu_2 - \mu_3 &= 2\alpha_1 - \alpha_2 - \alpha_3 = [0,2,-1,-1]\boldsymbol{\beta} = \mathbf{c'_1}\boldsymbol{\beta}\\
\mu_1 - \mu_3 &= \alpha_1 - \alpha_3 = [0,1,0,-1]\boldsymbol{\beta} = \mathbf{c'_3}\boldsymbol{\beta}
\end{align}
$$

As hipóteses $H_{01}: \mathbf{c'_1}\boldsymbol{\beta} = 0$ e $H_{03}: \mathbf{c'_3}\boldsymbol{\beta} = 0$ são:

$$
\begin{align}
H_{01} &:\mathbf{c'_1}\boldsymbol{\beta} = 0 \Leftrightarrow \mu_1 = \frac{\mu_2 + \mu_3}2 \\
H_{03} &: \mathbf{c'_3}\boldsymbol{\beta} = 0 \Leftrightarrow \mu_1 = \mu_3
\end{align}
$$

::: panel-tabset
### R

```{r}
C1 <- c(0, 2, -1, -1)
C1Beta <- C1 %*% Beta
C1Beta

C3 <- c(0, 1, 0, -1)
C3Beta <- C3 %*% Beta
C3Beta
```

```{r}
# Partição da SQTrat
SQC1Beta <- t(C1Beta) %*% solve(t(C1) %*% ginv(t(X) %*% X) %*% C1) %*% (C1Beta)
SQC1Beta

SQC3Beta <- t(C3Beta) %*% solve(t(C3)%*% ginv(t(X) %*% X) %*% C3) %*% (C3Beta)
SQC3Beta
```

```{r}
SomaSQ <- SQC1Beta + SQC3Beta

SQTrat
```

```{r}
#| echo: false

cat(
  "Soma das SQ de contrastes não ortogonais =", SomaSQ, "\n",
  "SQTrat =", SQTrat
  )
```

Ao contrário do caso dos constrastes ortogonais, a soma das somas de quadrados dos **contrastes não ortogonais** (`SomaSQ`) não resulta na soma de quadrados dos tratamentos (`SQTrat`), o que prejudica a interpretação da análise de variância, dada a não independência entre os contrastes.

### SAS

```{r}
#| eval: false

C1 ={0  2 -1 -1};
C2 ={0  1  0 -1};
C1Beta = C1*Beta;
C2Beta = C2*Beta;
SQC1Beta = t(C1*Beta)*inv(C1*ginv(t(X)*X)*t(C1))*C1*Beta;
SQC2Beta = t(C2*Beta)*inv(C2*ginv(t(X)*X)*t(C2))*C2*Beta;
Soma = SQC1Beta + SQC2Beta;
print 'Partição da SQTrat usando contrastes l.i. mas não ortogonais',,
      'SQcontraste1 = ' SQC1Beta[format=8.4],, 'SQcontraste2 = 'SQC2Beta[format=8.4],,
      'Soma SQs     = ' Soma[format=8.4] '    SQTrat = ' SQTrat[format=8.4];
quit;
```
:::

## Capítulo 14 - ANOVA com dois fatores: Caso Balanceado

```{r}
#| echo: false
#| message: false
#| warning: false

options(scipen = 999999, knitr.kable.NA = '')

library(MASS)
```

### Introdução

Nesta seção, será apresentada a análise de variância de modelos **balanceados** com **dois fatores de tratamento** (*two-way* ANOVA). Seu modelo pode ser representado como:

$$
\begin{align}
y_{ijk} &= \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk} \\
i = 1,2,\dots,a & \quad\quad j = 1,2,\dots,b \quad\quad k = 1,2,\dots,n
\end{align}
$$

-   $\alpha_i$ é o efeito do $i$-ésimo nível do fator $A$;

-   $\beta_j$ é o efeito do $j$-ésimo nível do fator $B$;

-   $\gamma_{ij}$ é o efeito da interação entre o $i$-ésimo nível do fator $A$ e o $j$-ésimo nível do fator $B$.

Como suposições, assumiremos:

-   $E(\epsilon_{ijk}) = 0$ para todo $i,j,k$;

-   $var(\epsilon_{ijk}) = \sigma^2$ para todo $i,j,k$;

-   $cov(\epsilon_{ijk}, \epsilon_{i'j'k'}) = 0$ para todo $(i,j,k) \ne (i',j',k')$;

-   $\epsilon_{ijk} \sim N(0, \sigma^2)$ para todo $i,j,k$.

Dessa maneira, podemos reescrever o modelo balanceado na **forma reparametrizada**, conhecido como **modelo de médias de caselas**:

$$
\begin{align}
y_{ijk} &= \mu_{ij} + \epsilon_{ijk} \\
i = 1,2,\dots,a \quad\quad j &= 1,2,\dots,b \quad\quad k = 1,2,\dots,n
\end{align}
$$

Em que $\mu_{ij} = E(y_{ijk})$ é a média de uma observação $k$ na casela $(ij)$.

Para ilustrar, considere o seguinte exemplo: *O conteúdo da mistura de três tipos de queijo produzidos por dois métodos foi anotado por Marcuse (1949) (formato alterado). Duas peças de queijo foram medidas para cada tipo e cada método. Designando Método como o fator A e Tipo como o fator B, então a = 2, b = 3 e n = 2.*

| Método \\ Tipo |   1   |   2   |   3   |
|:--------------:|:-----:|:-----:|:-----:|
|       1        | 39,02 | 35,74 | 37,02 |
|       1        | 38,79 | 35,41 | 36,00 |
|       2        | 38,96 | 35,58 | 35,70 |
|       2        | 39,01 | 35,52 | 36,04 |

-   **Fator A (Tratamento A)**: Método de produção de queijo, com 2 níveis (`a = 2`);

-   **Fator B (Tratamento B)**: Tipo de queijo, com 3 níveis (`b = 3`);

-   **Repetições**: 2 repetições por tratamento (caso balanceado).

Novamente, utilizaremos a função `ginv()` do pacote `MASS`.

```{r}
#| eval: false

install.packages("MASS")
library(MASS)
```

### Estimação dos parâmetros

::: panel-tabset
### R

```{r}
y <- c(39.02, 38.79, 35.74, 35.41, 37.02, 36.00, 38.96, 39.01, 35.58, 35.52, 35.70, 36.04)
y

X <- matrix(c(
  1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
  1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
  1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
  1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
  1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
  1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
  1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
  1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
  1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
  1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
  1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
  1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1
), 
nrow = 12, byrow = TRUE)
X
```

O modelo matricial $\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$ é dado por:

$$
\begin{cases}
y_{111} = \mu + \alpha_1 + \beta_1 + \gamma_{11}\\ 
y_{112} = \mu + \alpha_1 + \beta_1 + \gamma_{11}\\ 
y_{121} = \mu + \alpha_1 + \beta_2 + \gamma_{12}\\ 
y_{122} = \mu + \alpha_1 + \beta_2 + \gamma_{12}\\ 
y_{131} = \mu + \alpha_1 + \beta_3 + \gamma_{21}\\ 
y_{132} = \mu + \alpha_1 + \beta_3 + \gamma_{21}\\ 
y_{211} = \mu + \alpha_2 + \beta_1 + \gamma_{22}\\ 
y_{212} = \mu + \alpha_2 + \beta_1 + \gamma_{22}\\ 
y_{221} = \mu + \alpha_2 + \beta_2 + \gamma_{31}\\ 
y_{222} = \mu + \alpha_2 + \beta_2 + \gamma_{31}\\ 
y_{231} = \mu + \alpha_2 + \beta_3 + \gamma_{32}\\ 
y_{232} = \mu + \alpha_2 + \beta_3 + \gamma_{32}
\end{cases}
$$

$$
\begin{bmatrix}
y_{111} \\ y_{112} \\ y_{121} \\ y_{122} \\ y_{131} \\ y_{132} \\ y_{211} \\ y_{212} \\ y_{221} \\ y_{222} \\ y_{231} \\ y_{232}
\end{bmatrix}
=
\begin{bmatrix}
39,02 \\ 38,79 \\ 35,74 \\ 35,41 \\ 37,02 \\ 36,00 \\ 38,96 \\ 39,01 \\ 35,58 \\ 35,52 \\ 35,70 \\ 36,04
\end{bmatrix}
=
\begin{bmatrix}
1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\mu \\ \alpha_1 \\ \alpha_2 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \gamma_{11} \\ \gamma_{12} \\ \gamma_{21} \\ \gamma_{22} \\ \gamma_{31} \\ \gamma_{32}
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_{111} \\ \epsilon_{112} \\ \epsilon_{121} \\ \epsilon_{122} \\ \epsilon_{131} \\ \epsilon_{132} \\ \epsilon_{211} \\ \epsilon_{212} \\ \epsilon_{221} \\ \epsilon_{222} \\ \epsilon_{231} \\ \epsilon_{232}
\end{bmatrix}
$$

Nota-se que o modelo apresenta 12 parâmetros e o posto($\mathbf{X}$) = 6, ou seja, a matriz $\mathbf{X}$ é de posto incompleto.

```{r}
npar <- ncol(X)                      # número de parâmetros
npar

rank_X <- sum(diag(ginv(X) %*% X))   # posto de X
rank_X

deficit_rank <- npar - rank_X        # déficit de rank
deficit_rank
```

```{r}
a <- 2    # níveis do fator A
b <- 3    # níveis do fator B
n <- 2    # número de repeticões

abn <- a * b * n    # número total de observacões
abn
```

Para visualizar as submatrizes da matriz $\mathbf{X}$ respectivas à constante $\mu$, ao parâmetro do fator A ($\alpha_i$), ao parâmetro do fator B ($\beta_j$) e ao de interação entre os fatores ($\gamma_{ij}$), procedemos da seguinte maneira:

```{r}
X0 <- X[, 1]        # constante
X0

XA <- X[, 2:3]      # fator A
XA

XB <- X[, 4:6]      # fator B
XB

XAB <- X[, 7:12]    # combinacão dos níveis dos dois fatores
XAB
```

A seguir, estimaremos o vetor de parâmetros $\boldsymbol{\beta}$ usando duas inversas generalizadas diferentes.

$$
\hat{\boldsymbol{\beta}} = \mathbf{(X'X)^- X'y}
$$

-   Inversa generalizada de Moore-Penrose:

```{r}
Beta <- ginv(t(X) %*% X) %*% t(X) %*% y
Beta
```

-   Inversa generalizada simples (Searle):

```{r}
XLX <- t(X) %*% X
XLX

iXLX <- (1 / 2) * kronecker(
  matrix(c(0,0,0,1), byrow = TRUE, ncol = 2), diag(6)
)
fractions(iXLX)

Betag <- iXLX %*% t(X) %*% y
Betag
```

A tabela abaixo traz as duas estimativas de $\boldsymbol{\beta}$ calculadas anteriormente.

```{r}
#| echo: false

data.frame(
  params <- c("$\\hat{\\mu}$", "$\\hat{\\alpha_1}$", "$\\hat{\\alpha_2}$", "$\\hat{\\beta_1}$", "$\\hat{\\beta_2}$", "$\\hat{\\beta_3}$", "$\\hat{\\gamma_{11}}$", "$\\hat{\\gamma_{12}}$", "$\\hat{\\gamma_{21}}$", "$\\hat{\\gamma_{22}}$", "$\\hat{\\gamma_{31}}$", "$\\hat{\\gamma_{32}}$"),
  beta_mp = Beta |> round(4),
  beta_g = Betag |> round(4)
) |> 
  kableExtra::kbl(
    align = "c", 
    col.names = c("Parâmetros", "Beta MP", "Beta Simples"), 
    format = "html",
    escape = FALSE,
    booktabs = TRUE
  ) |> 
  kableExtra::kable_styling(position = "center")
```

### SAS

```{r}
#| eval: false

proc iml;
reset fuzz;
y = {39.02,38.79,35.74,35.41,37.02,36.00,38.96,39.01,35.58,35.52,35.70,36.04};
X = {1 1 0 1 0 0 1 0 0 0 0 0,
	 1 1 0 1 0 0 1 0 0 0 0 0,
	 1 1 0 0 1 0 0 1 0 0 0 0,
	 1 1 0 0 1 0 0 1 0 0 0 0,
	 1 1 0 0 0 1 0 0 1 0 0 0,
	 1 1 0 0 0 1 0 0 1 0 0 0,
	 1 0 1 1 0 0 0 0 0 1 0 0,
	 1 0 1 1 0 0 0 0 0 1 0 0,
	 1 0 1 0 1 0 0 0 0 0 1 0,
	 1 0 1 0 1 0 0 0 0 0 1 0,
	 1 0 1 0 0 1 0 0 0 0 0 1,
	 1 0 1 0 0 1 0 0 0 0 0 1};
print X[format=4.0] y[format=12.2];

rank_X = round(trace(ginv(X)*X)); 	* Calcula o posto da matriz X;
npar = ncol(X);
a=2; 	* Número de níveis do fator A;
b=3;	* Número de níveis do fator B;
n=2;	* Número de repetições;
abn = a*b*n;	* Número total de observações;
X0 = X[,1];
XA = X[,2:3];
XB = X[,4:6];
XAB = X[,7:12];
print X[format=8.0],,XA[format=8.0],,XB[format=8.0],,XAB[format=8.0];
* ----------------------------------------------------------------------------;

* Estima Beta usando inversa de Moore-Penrose;
BetaMP = ginv(t(X)*X)*t(X)*y;		* Beta usando invG (Moore Penrose));
XLX = t(X)*X;
iXLX = (1/2)*{0 0, 0 1}@I(6);		
BetaG = iXLX*t(X)*y;				* Beta usando invG mais simples (Searle);

print BetaMP BetaG;

* ----------------------------------------------------------------------------;
```
:::

### Teste de hipótese {#sec-fatorial-th}

O primeiro teste que realizamos em ANOVA com dois fatores é o **teste de interação** entre os fatores A e B, cuja hipótese é:

$$
\begin{align}
H_0 &: \text{Não há efeito da interação entre os fatores A e B} \\
H_1 &: \text{Há efeito da interação entre os fatores A e B}
\end{align}
$$

Caso a hipótese de **interação** entre os fatores seja **não significativa** (não se rejeita $H_0$), ou seja, os fatores são **independentes**, devemos analisar os **efeitos principais** dos fatores A e B, presentes no mesmo quadro de ANOVA construído para avaliar a interação. Assim, as hipóteses para os fatores A e B são dadas por:

$$
\begin{align}
H_0 &: \text{Não há efeito do fator A} \\
H_1 &: \text{Há efeito do fator A para algum dos níveis de A}
\end{align}
$$

$$
\begin{align}
H_0 &: \text{Não há efeito do fator B} \\
H_1 &: \text{Há efeito do fator B para algum dos níveis de B}
\end{align}
$$

Por outro lado, caso a hipótese de **interação** entre os fatores seja **significativa** (rejeita-se $H_0: \text{Não há efeito da interação entre os fatores A e B}$), devemos avaliar os **efeitos simples** de cada fator, ou seja, avaliar o efeito do fator A dentro de cada nível do fator B e/ou o efeito do fator B dentro de cada nível do fator A. Para isso, devemos realizar uma segunda ANOVA.

A seguir, demonstraremos um teste de hipótese em que a **interação não é significativa**, a partir das abordagens de **modelo completo x modelo reduzido** e **hipótese linear geral**.

#### Modelo Completo x Modelo Reduzido

::: panel-tabset
### R

Novamente, utilizaremos as seguintes formas quadráticas para calcular a soma de quadrados total (`SQTotal`) e a soma de quadrados dos resíduos (`SQRes`).

$$
SQTotal = \mathbf{y'} \left(\mathbf{I} - \frac{1}n \mathbf{J}\right) \mathbf{y}
$$

$$
SQRes = \mathbf{y'} \left[\mathbf{I} - \mathbf{X(X'X)^-X'}\right] \mathbf{y}
$$

```{r}
In <- diag(abn)
Jn <- matrix(1, nrow = abn, ncol = abn)
```

```{r}
# Total
Tot <- In - (1 / abn) * Jn

SQTotal <- t(y) %*% Tot %*% y
SQTotal

gl_total <- round(sum(diag(Tot %*% ginv(Tot))))
gl_total
```

```{r}
# Resíduo
PR <- In - X %*% ginv(t(X) %*% X) %*% t(X)

SQRes <- t(y) %*% PR %*% y
SQRes

gl_res <- round(sum(diag(PR %*% ginv(PR))))
gl_res

QMRes <- SQRes / gl_res
QMRes
```

Para calcular a soma de quadrados da interação entre os fatores A e B, utilizamos a seguinte forma quadrática:

$$
SQ_{A\times B} = \mathbf{y'} [\boldsymbol{X(X'X)^-X' - Z(X'X)^-Z'}] \mathbf{y}
$$

onde $\mathbf{Z}$ é a submatriz da matriz de delineamento $\mathbf{X}$ respectiva à constante $\mu$, ao parâmetro do fator A ($\alpha_i$, $i = 1,2$) e ao parâmetro do fator B ($\beta_j$, $j = 1,2,3$).

```{r}
# Interação AxB
X1 <- cbind(X0, XA, XB)
X1

PAB <- X %*% ginv(t(X) %*% X) %*% t(X) - X1 %*% ginv(t(X1) %*% X1) %*% t(X1)

SQAB <- t(y) %*% PAB %*% y
SQAB

glAB <- round(sum(diag(ginv(PAB) %*% PAB)))
glAB

QMAB <- SQAB / glAB
QMAB

FAB <- QMAB / QMRes
FAB

ftabAB <- qf(0.95, glAB, gl_res)
ftabAB

p_valorAB <- 1 - pf(FAB, glAB, gl_res)
p_valorAB
```

A soma de quadrados do efeito principal do fator A é dado por:

$$
SQ_A = \mathbf{y'} \left[\boldsymbol{A(A'A)^-A'} - \frac{\mathbf{J}}n \right] \mathbf{y}
$$

onde $\mathbf{A}$ é a submatriz da matriz de delineamento $\mathbf{X}$ respectiva ao parâmetro do fator A ($\alpha_i$, $i = 1,2$).

```{r}
# Fator A
PA <- XA %*% ginv(t(XA) %*% XA) %*% t(XA) - Jn/abn

SQA <- t(y) %*% PA %*% y
SQA

glA <- round(sum(diag(ginv(PA) %*% PA)))
glA

QMA <- SQA / glA
QMA

FA <- QMA / QMRes
FA

ftabA <- qf(0.95, glA, gl_res)
ftabA

p_valorA <- 1 - pf(FA, glA, gl_res)
p_valorA
```

De modo análogo, soma de quadrados do efeito principal do fator B é dado por:

$$
SQ_B = \mathbf{y'} \left[\boldsymbol{B(B'B)^-B'} - \frac{\mathbf{J}}n \right] \mathbf{y}
$$

onde $\mathbf{A}$ é a submatriz da matriz de delineamento $\mathbf{X}$ respectiva ao parâmetro do fator A ($\alpha_i$, $i = 1,2$).

```{r}
# Fator B
PB <- XB %*% ginv(t(XB) %*% XB) %*% t(XB) - Jn/abn

SQB <- t(y) %*% PB %*% y
SQB

glB <- round(sum(diag(ginv(PB) %*% PB)))
glB

QMB <- SQB / glB
QMB

FB <- QMB / QMRes
FB

ftabB <- qf(0.95, glB, gl_res)
ftabB

p_valorB <- 1 - pf(FB, glB, gl_res)
p_valorB
```

Os resultados do teste de interação e de efeitos principais estão descritos no quadro de ANOVA a seguir.

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "FV" = c("Método (A)", "Tipo (B)", "Interação (AxB)", "Resíduo", "Total"),
  "gl" = c(glA, glB, glAB, gl_res, gl_total),
  "SQ" = c(SQA, SQB, SQAB, SQRes, SQTotal) |> round(3),
  "QM" = c(QMA, QMB, QMAB, QMRes, NA) |> round(3),
  "Fcal" = c(FA, FB, FAB, NA, NA) |> round(3),
  "Ftab" = c(ftabA, ftabB, ftabAB, NA, NA) |> round(3),
  "p.valor" = c(p_valorA, p_valorB, p_valorAB, NA, NA) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "ANOVA: Fatorial 2x3 (caso balanceado) - Interação e efeitos principais",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que o $F_{cal} < F_{tab}$ para a **interação**, considerando $F_{(0,05;2;6)}$, a interação não foi significativa, logo **não se rejeita** $H_0: \text{Não há efeito da interação entre os fatores A e B}$. Dessa forma, os fatores A e B podem ser considerados independentes, sem efeito de interação.

Visto que não há interação entre os fatores, podemos analisar os **efeitos principais** de cada um dos fatores. No caso do fator A (Método), também não se observou significância estatística para o efeito do método na fabricação de queijo. Dessa forma, não se rejeita $H_0: \text{Não há efeito do fator A}$. Por outro lado, o fator B (Tipo) foi significativo, assim, rejeita-se $H_0: \text{Não há efeito do fator B}$, permitindo dizer que entre, pelo menos, dois tipos de queijo há diferenças no processo de fabricação.

### SAS

```{r}
#| eval: false

* Cálculo da soma de quadrados total - SQTotal;
P = I(abn) - J(abn,abn,1)/abn;
SQTotal = t(y)*(P)*y;
glTotal = round(trace(ginv(P)*P));

* Cálculo da soma de quadrados de resíduos - SQRes;
PR = I(abn) - X*ginv(t(X)*X)*t(X);
SQRes = t(y)*(PR)*y;
glRes = round(trace(ginv(PR)*PR));
QMRes = SQRes/glRes;

* Cálculo SQAxB - forma quadrática;
X1 = X0||XA||XB;
PAB = X*ginv(t(X)*X)*t(X) - X1*ginv(t(X1)*X1)*t(X1);
SQAB = t(y)*PAB*y; 					* Calcula SQ(AB);
glAB = round(trace(ginv(PAB)*PAB)); * Calcula gl da interação AxB;
QMAB = SQAB/glAB;
FAB = QMAB/QMRes;
p_valorAB = 1-cdf('F',FAB,glAB,glRes);

* Cálculo da soma de quadrados do fator A - SQ(A);
PA = XA*ginv(t(XA)*XA)*t(XA) - J(abn,abn,1)/abn;
SQA = t(y)*PA*y;
glA = round(trace(ginv(PA)*PA));
QMA = SQA/glA;
FA = QMA/QMRes;
p_valorA = 1-cdf('F',FA,glA,glRes);

* Cálculo da soma de quadrados do fator B - SQ(B);
PB = XB*ginv(t(XB)*XB)*t(XB) - J(abn,abn,1)/abn;
SQB = t(y)*PB*y;
glB = round(trace(ginv(PB)*PB));
QMB = SQB/glB;
FB = QMB/QMRes;
p_valorB = 1-cdf('F',FB,glB,glRes);

* Imprime o quadro de análise de variância – ANOVA (pág. 252);
print 'QUADRO DE ANOVA: Exemplo 14.4.2: Fatorial 2x3 (caso balanceado)';
print 'Método    ' glA[format=8.0]     SQA[format=12.4]   QMA[format=12.4]  FA[format=12.4]  p_valorA[format=12.4];
print 'Tipo      ' glB[format=8.0]     SQB[format=12.4]   QMB[format=12.4]  FB[format=12.4]  p_valorB[format=12.4];
print 'Interação ' glAB[format=8.0]    SQAB[format=12.4]  QMAB[format=12.4] FAB[format=12.4] p_valorAB[format=12.4];
print 'Resíduo   ' glRes[format=8.0]   SQRes[format=12.4] QMRes[format=12.4];
print 'Total     ' glTotal[format=8.0] SQTotal[format=12.4] ;
```
:::

#### Hipótese Linear Geral

Também podemos calcular as somas de quadrados utilizando a abordagem da **hipótese linear geral**. A soma de quadrados obtida a partir desta abordagem é dada por:

$$
SQ_{Hip} = (\boldsymbol{C\hat{\beta}})' \left[\mathbf{C(X'X)^-C'}\right]^{-1} (\boldsymbol{C\hat{\beta}})
$$

onde $\mathbf{C}$ é a matriz de coeficientes que reproduz, matricialmente, a hipótese de interesse.

::: panel-tabset
### R

```{r}
CAxB <- matrix(
  c(0, 0, 0, 0, 0, 0, 1, -1, 0, -1, 1, 0,
    0, 0, 0, 0, 0, 0, 1, 0, -1, -1, 0, 1), nrow = 2, byrow = TRUE
)
CAxB

SQ_CAxB <- t(CAxB %*% Beta) %*% solve(CAxB %*% ginv(t(X) %*% X) %*% t(CAxB)) %*% (CAxB %*% Beta)
SQ_CAxB
```

```{r}
CA <- 
  (1/3) * matrix(
    c(0, 3, -3, 0, 0, 0, 1, 1, 1, -1, -1, -1), nrow = 1, byrow = TRUE
  )
fractions(CA)

SQ_CA <- t(CA %*% Beta) %*% solve(CA %*% ginv(t(X) %*% X) %*% t(CA)) %*% (CA %*% Beta)
SQ_CA
```

```{r}
CB <- 
  (1/2) * matrix(
    c(0, 0, 0, 2, -2, 0, 1, -1, 0, 1, -1, 0,
      0, 0, 0, 2, 0, -2, 1, 0, -1, 1, 0, -1), nrow = 2, byrow = TRUE
  )
fractions(CB)

SQ_CB <- t(CB %*% Beta) %*% solve(CB %*% ginv(t(X) %*% X) %*% t(CB)) %*% (CB %*% Beta)
SQ_CB
```

Note que as somas de quadrados obtidas a partir da hipótese linear geral são iguais às obtidas pelo método do modelo completo x modelo reduzido.

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  metodo = c("Completo x Reduzido", "Hip.Lin.Geral"),
  SQ_A = c(SQA, SQ_CA),
  SQ_B = c(SQB, SQ_CB),
  SQ_AxB = c(SQAB, SQ_CAxB)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "Somas de quadrados entre as abordagens de Modelo completo x reduzido e Hipótese linear geral",
    col.names = c("Método", "SQ A", "SQ B", "SQ AxB")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

### SAS

```{r}
#| eval: false

* ----------------------------------------------;
* Cálculo das SQ usando Hipótese Linear Geral;
* ----------------------------------------------;

*            mi a1 a2 b1 b2 b3 g11 g12 g13 g21 g22 g23;      
CA   = (1/3)*{0  3 -3  0  0  0   1   1   1  -1  -1  -1};

CB   = (1/2)*{0  0  0  2 -2  0   1  -1   0   1  -1   0,
              0  0  0  2  0 -2   1   0  -1   1   0  -1};

CAxB =       {0  0  0  0  0  0   1  -1   0  -1   1   0,
              0  0  0  0  0  0   1   0  -1  -1   0   1}; 

SQ_CA = t(CA*Beta)*inv(CA*ginv(t(X)*X)*t(CA))*CA*Beta;
SQ_CB = t(CB*Beta)*inv(CB*ginv(t(X)*X)*t(CB))*CB*Beta;
SQ_CAxB = t(CAxB*Beta)*inv(CAxB*ginv(t(X)*X)*t(CAxB))*CAxB*Beta;

print 'Somas de quadrados usando Hipótese Linear Geral:',,
    SQ_CA[format=12.4] SQ_CB[format=12.4] SQ_CAxB[format=12.4];
```
:::

### Estimabilidade no modelo superparametrizado

#### Sem restrições nos parâmetros

Verificaremos se funções são estimáveis no modelo superparametrizado **sem** restrições nos parâmetros.

::: panel-tabset
### R

Neste primeiro caso, utilizaremos a abordagem de que se

$$
(\mathbf{X'X})(\mathbf{X'X})^-\boldsymbol{\lambda} = \boldsymbol{\lambda}
$$

a função é estimável.

-   $\beta = \alpha_1 - \alpha_2$:

```{r}
L1 <- matrix(c(0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0), ncol = 1)
L1

ver <- t(X) %*% X %*% ginv(t(X) %*% X)

verL1 <- ver %*% L1 |> round(2)
verL1

L1Beta <- t(L1) %*% Beta
L1Beta
```

Verifica-se que $\beta = \alpha_1 - \alpha_2$ **não é estimável** no modelo sem restrição nos parâmetros.

-   $\beta = \alpha_1 - \alpha_2 + (\frac{1}3(\gamma_{11} + \gamma_{12} + \gamma_{13}) - (\frac{1}3)(\gamma_{21} = \gamma_{22} + \gamma_{23})$:

```{r}
L2 <- (1/3) * t(matrix(c(0, 3, -3, 0, 0, 0, 1, 1, 1, -1, -1, -1), nrow = 1))
round(L2, 2)

ver <- t(X) %*% X %*% ginv(t(X) %*% X)

verL2 <- ver %*% L2 |> round(2)
verL2

L2Beta <- t(L2) %*% Beta
L2Beta
```

Verifica-se que $\beta = \alpha_1 - \alpha_2 + (\frac{1}3(\gamma_{11} + \gamma_{12} + \gamma_{13}) - (\frac{1}3)(\gamma_{21} = \gamma_{22} + \gamma_{23})$ **é estimavel** no modelo superparametrizado sem restrição nos parâmetros.

### SAS

```{r}
#| eval: false

L1 = t({0 1 -1 0 0 0 0 0 0 0 0 0});
ver = t(X)*X*ginv(t(X)*X);
verL1 = ver*L1;
L1BetaMP = t(L1)*BetaMP;
print 'Mostra que L1Beta = a1-a2 NÃO É estimável no modelo', 'SEM restrição nos parâmetros';
print L1 verL1 L1BetaMP[format=12.4];

print 'Mostra que L2Beta = a1-a2 + (1/3(g11+g12+g13)-(1/3)(g21+g22+g23) É estimável no modelo',
      'SEM restrição nos parâmetros';
L2 = (1/3)*t({0 3 -3 0 0 0 1 1 1 -1 -1 -1});
verL2 = ver*L2;
L2BetaMP = t(L2)*BetaMP;
print L2 verL2 L2BetaMP[format=12.4];
```
:::

#### Com restrições nos parâmetros

Agora, verificaremos se funções são estimáveis no modelo superparametrizado **com** restrições nos parâmetros.

::: panel-tabset
### R

Primeiramente, assumiremos as seguintes condições marginais (restrições):

$$
\sum_i \hat{\alpha}_i = 0 \quad , \quad \sum_j \hat{\beta}_j = 0 \quad , \quad \sum_i \hat{\gamma}_{ij} = 0 \quad e \quad \sum_j \hat{\gamma}_{ij} = 0
$$

e expressas na matriz $T$ da seguinte forma:

$$
\boldsymbol{T\hat{\beta}} = 
\begin{bmatrix}
0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\mu \\ \alpha_1 \\ \alpha_2 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \gamma_{11} \\ \gamma_{12} \\ \gamma_{21} \\ \gamma_{22} \\ \gamma_{31} \\ \gamma_{32}
\end{bmatrix}
$$

```{r}
t <- matrix(
  c(0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
    0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1), 
  nrow = 7, ncol = 12, byrow = TRUE)
t

rank_T <- sum(diag(ginv(t) %*% t))
rank_T
```

A matriz de coeficientes $\mathbf{T}$ tem dimensão $7 \times 12$ e posto$(\mathbf{W}) = 6$.

Juntando a matriz $\mathbf{X}$ (de posto incompleto) com a matriz $\mathbf{T}$, obtemos uma matriz $\mathbf{W}$ de posto completo (posto$(\mathbf{W}) = 12$).

```{r}
W <- rbind(X, t)
W

rank_W <- sum(diag(ginv(W) %*% W))
rank_W
```

Ao vetor de valores observados, também colocamos restrições.

```{r}
yr <- c(y, rep(0, 7))
yr
```

Calculando o $\boldsymbol{\beta}$ sujeito às condições marginais, obtemos:

```{r}
Beta_R <- solve(t(W) %*% W) %*% t(W) %*% yr
Beta_R
```

Com o $\boldsymbol{\beta}$ com restrições nos parâmetros, verificaremos se as mesmas funções utilizadas no caso anterior (estimação de funções **sem** restrição) são estimáveis.

-   $\beta = \alpha_1 - \alpha_2$:

```{r}
L1 <- t(matrix(c(0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0), nrow = 1))
L1

ver <- t(W) %*% W %*% solve(t(W) %*% W)

verL1 <- ver %*% L1
round(verL1)

L1Beta_r <- t(L1) %*% Beta_R
L1Beta_r
```

No caso sem restrição nos parâmetros, $\beta = \alpha_1 - \alpha_2$ é não estimável. Contudo, ao impor restrições nos parâmetros, a mesma função passa a ser **estimável**.

-   $\beta = \alpha_1 - \alpha_2 + (\frac{1}3(\gamma_{11} + \gamma_{12} + \gamma_{13}) - (\frac{1}3)(\gamma_{21} = \gamma_{22} + \gamma_{23})$:

```{r}
L22 <- (1/3) * t(matrix(c(0, 3, -3, 0, 0, 0, 1, 1, 1, -1, -1, -1), nrow = 1))
L22 |> round(3)

verL22 <- ver %*% L22 |> round(3)
verL22

L2Beta_R <- t(L22) %*% Beta_R
L2Beta_R
```

Bem como no caso sem restrição nos parâmetros, verifica-se que $\beta = \alpha_1 - \alpha_2 + (\frac{1}3(\gamma_{11} + \gamma_{12} + \gamma_{13}) - (\frac{1}3)(\gamma_{21} = \gamma_{22} + \gamma_{23})$ continua a ser **estimável** no modelo com restrição nos parâmetros.

### SAS

```{r}
#| eval: false

* Matriz T de condições marginais: T*Beta = 0;
T = {0 1 1 0 0 0 0 0 0 0 0 0,
	 0 0 0 1 1 1 0 0 0 0 0 0,
	 0 0 0 0 0 0 1 1 1 0 0 0,
	 0 0 0 0 0 0 0 0 0 1 1 1,
	 0 0 0 0 0 0 1 0 0 1 0 0,
	 0 0 0 0 0 0 0 1 0 0 1 0,
	 0 0 0 0 0 0 0 0 1 0 0 1};
rank_T = round(trace(ginv(T)*T)); 	* Determina o posto da matriz T;
W = X//T; 							* Junta as matrizes X e T;
rank_W = round(trace(ginv(W)*W)); 	* Calcula o posto da matriz W = X//T;
print npar rank_X rank_T rank_W;
yr = y//j(7,1,0); 					* Completa o vetor y com 7 zeros;
Beta_R = inv(t(W)*W)*t(W)*yr; 		* Beta sujeito às condições marginais;
print BetaMP[format=12.4] Beta_R[format=12.4];

print '------------------------------------------------------------',
      ' ESTIMABILIDADE NO MODELO SUPERPARAMETRIZADO COM RESTRIÇÕES ',
      '------------------------------------------------------------',;
L1 = t({0 1 -1 0 0 0 0 0 0 0 0 0});
ver = t(W)*W*inv(t(W)*W);
verL1 = ver*L1;
L1Beta_r = t(L1)*Beta_R;
print 'Mostra que L1Beta = a1-a2 É estimável no modelo', 'COM restrição nos parâmetros';
print L1 verL1 L1Beta_R[format=12.4];

print 'Mostra que L2Beta = a1-a2 +(1/3(g11+g12+g13)-(1/3)(g21=g22+g23) É estimável no modelo',
      'COM restrição nos parâmetros';
L2 = (1/3)*t({0 3 -3 0  0  0  1  1  1 -1 -1 -1});
verL2 = ver*L2;
L2Beta_R = t(L2)*Beta_R;
print L2 verL2 L2Beta_R[format=12.4];	
```
:::

#### Hipótese linear geral

Agora, utilizaremos a abordagem da hipótese linear geral para verificar a estimabilidade de funções no modelo superparametrizado **sem restrição** e **com restrição**.

::: panel-tabset
### R

```{r}
CA <- (1 / 3) * c(0,  3, -3,  0,  0,  0,  1,  1,  1, -1, -1, -1)
fractions(CA)
```

-   Sem restrição:

```{r}
CABeta <- CA %*% Beta
CABeta

SQ_A <- t(CABeta) %*% solve(t(CA) %*% ginv(t(X) %*% X) %*% CA) %*% CABeta
SQ_A
```

-   Com restrição:

```{r}
CABeta_R <- CA %*% Beta_R
CABeta_R

SQ_A_R = t(CABeta_R) %*% solve(t(CA) %*% solve(t(W) %*% W) %*% CA) %*% CABeta_R
SQ_A_R
```

Nota-se que, ao utilizar a matriz de coeficientes $\mathbf{C}$ da hipótese linear geral, obtemos o mesmo resultado de soma de quadrados tanto para o caso sem restrição, como para o caso com restrição.

### SAS

```{r}
#| eval: false

* Cálculo de SQA usando hipótese linear geral;

* (1) No modelo SEM restrição;
* CA = {0  1 -1  0  0  0  0  0  0  0  0  0};
CA = (1/3)*{0  3 -3  0  0  0  1  1  1 -1 -1 -1};
CABeta = CA*Beta_R;
print '-----------------------------------------------------------',
      ' ESTIMABILIDADE NO MODELO SUPERPARAMETRIZADO SEM RESTRIÇÃO ',
      '-----------------------------------------------------------',
      CA[format=6.2],,CABeta[format=12.4],,;

SQ_A = t(CABeta)*inv(CA*ginv(t(X)*X)*t(CA))*CABeta;

* (1) No modelo COM restrição;
CABeta_R = CA*Beta_R;
print 'No modelo COM restrição nos parâmetros:',,CA[format=6.2],,CABeta_R[format=12.4],,;

SQ_A_R = t(CABeta_r)*inv(CA*inv(t(W)*W)*t(CA))*CABeta_r;
print SQ_A[format=12.4] SQ_A_R[format=12.4];

quit;
```
:::

## Capítulo 15 - ANOVA para dados desbalanceados

### ANOVA com um fator

```{r}
#| echo: false
#| message: false
#| warning: false

options(scipen = 999999, knitr.kable.NA = '')

library(MASS)
```

Nesta seção, trataremos da ANOVA com **um fator** em conjuntos de dados **desbalanceados**, em que o número de observações por tratamento não é o mesmo.

O modelo desbalanceado com um fator é dado por:

$$
\begin{align}
y_{ij} = \mu + \tau_i + \epsilon_{ij} = \mu_{i} + \epsilon_{ij} \\
i = 1,\dots,k \space \text{ e } \space j = 1,\dots,n_i
\end{align}
$$

Para realizar inferências, assumiremos $\epsilon_{ij} \sim N(0, \sigma^2)$ são independentes e identicamente distribuídos.

Como exemplo, temos o seguinte caso: *Os pesos líquidos de latas enchidas por cinco máquinas de enchimento são apresentados:*

```{r}
#| echo: false

data.frame(
  A = c(11.95, 12, 12.25, 12.10),
  B = c(12.18, 12.11, "-", "-"),
  C = c(12.16, 12.15, 12.08, "-"),
  D = c(12.25, 12.30, 12.10, "-"),
  E = c(12.10, 12.04, 12.02, 12.02)
) |> 
  kableExtra::kbl(align = "c") |> 
  kableExtra::kable_styling(position = "center")
```

O modelo matricial de médias de caselas $\mathbf{y} = \mathbf{W}\boldsymbol{\mu} + \boldsymbol{\epsilon}$ é dado por:

$$
\begin{bmatrix}
y_{11} \\ y_{12} \\ y_{13} \\ y_{14} \\ y_{21} \\ y_{22} \\ y_{31} \\ y_{32} \\ y_{33} \\ y_{41} \\ y_{42} \\ y_{43} \\ y_{51} \\ y_{52} \\ y_{53} \\ y_{54}
\end{bmatrix}
=
\begin{bmatrix}
11,95 \\ 12,00 \\ 12,25 \\ 12,10 \\ 12,18 \\ 12,11 \\ 12,16 \\ 12,15 \\ 12,08 \\ 12,25 \\ 12,30 \\ 12,10 \\ 12,10 \\ 12,04 \\ 12,02 \\ 12,02
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
\mu_1 \\ \mu_2 \\ \mu_3 \\ \mu_4 \\ \mu_5
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_{11} \\ \epsilon_{12} \\ \epsilon_{13} \\ \epsilon_{14} \\ \epsilon_{21} \\ \epsilon_{22} \\ \epsilon_{31} \\ \epsilon_{32} \\ \epsilon_{33} \\ \epsilon_{41} \\ \epsilon_{42} \\ \epsilon_{43} \\ \epsilon_{51} \\ \epsilon_{52} \\ \epsilon_{53} \\ \epsilon_{54}
\end{bmatrix}
$$

::: panel-tabset
### R

```{r}
#| eval: false

install.packages("MASS")
library(MASS)
```

```{r}
y <- c(11.95,12.00,12.25,12.10,12.18,12.11,12.16,12.15,12.08,12.25,12.30,12.10,12.10,12.04,12.02,12.02)
y

W <- matrix(
  c(rep(1, 4), rep(0, 12),
  rep(0, 4), rep(1, 2), rep(0, 10),
  rep(0, 6), rep(1, 3), rep(0, 7),
  rep(0, 9), rep(1, 3), rep(0, 4),
  rep(0, 12), rep(1, 4)),
  ncol = 5, byrow = FALSE
)
W
```

Note que a matriz $\mathbf{W}$ é de posto completo (posto$(\mathbf{W}) = k = 5$).

```{r}
k <- ncol(W)                         # Número de tratamentos
k

rank_W <- sum(diag(ginv(W) %*% W))   # posto da matriz W
rank_W

deficit_rank <- k - rank_W           # déficit de rank
deficit_rank
```

Dessa forma, o sistema de equações normais $\mathbf{(W'W)} \boldsymbol{\hat\mu} = \mathbf{(W'y)}$ tem solução única:

$$
\boldsymbol{\hat\mu} = \mathbf{(W'W)}^{-1} \mathbf{W'y} = \mathbf{\bar{y}} =
\begin{bmatrix}
\bar{y}_{1.} \\ \bar{y}_{2.} \\ \bar{y}_{3.} \\ \bar{y}_{4.} \\ \bar{y}_{5.}
\end{bmatrix}
$$

### SAS

```{r}
#| eval: false

options nodate nocenter ps=1000;
proc iml;
reset fuzz;
* Exemplo 15.2.1 Os pesos líquidos de latas enchidas por cinco máquinas; 
* de enchimento (filling machines) são apresentados na Tabela 14.2;

y = {11.95,12.00,12.25,12.10,12.18,12.11,12.16,12.15,
     12.08,12.25,12.30,12.10,12.10,12.04,12.02,12.02};

Trat = {1,1,1,1,2,2,3,3,3,4,4,4,5,5,5,5};
W = design(Trat); 	* W do modelo de médias de caselas;
k = ncol(W);		* Número de tratamentos;
N = nrow(W);		* Número total de repetições;
print W y[format=10.2];
```
:::

#### Teste de hipótese

Para testar a hipótese $H_0: \mu_1 = \mu_2 = \mu_3 = \mu_4 = \mu_5$, podemos utilizar a abordagem do **modelo completo x modelo reduzido** e **contrastes**.

##### Modelo completo x Modelo reduzido

::: panel-tabset
### R

```{r}
N <- nrow(W)   # Número total de repetições
N

Jnn <- matrix(1, nrow = N, ncol = N)
In <- diag(1, nrow = N)
```

A soma de quadrados total (`SQTotal`) é dada por:

$$
SQTotal = \mathbf{y'} \left(\mathbf{I} - \frac{1}n \mathbf{J}\right) \mathbf{y}
$$

```{r}
# Total
SQTotal <- t(y) %*% (In - Jnn / N) %*% y
SQTotal

gl_total <- N - 1
gl_total
```

A soma de quadrados do **modelo completo** de média de caselas é dado por:

$$
SQ(\mu_1,\mu_2,\mu_3,\mu_4,\mu_5) = \boldsymbol{\hat\mu' W'y}
$$

```{r}
# Modelo completo
mi <- solve(t(W) %*% W) %*% t(W) %*% y
mi

SQcompleto <- t(mi) %*% t(W) %*% y
SQcompleto
```

O **modelo reduzido** pode ser escrito, matricialmente, da seguinte maneira:

$$
\mathbf{y} = \mu \mathbf{j} + \boldsymbol{\epsilon}^*, \quad \text{onde } \mathbf{j} \text{ é } N \times 1
$$

em que $\mathbf{j}$ é um vetor coluna de 1's com dimensão $N \times 1$, sendo $N$ o número total de observações (sem contar os valores ausentes).

Dessa forma, a soma de quadrados do **modelo reduzido** é dado por:

$$
SQ(\mu) = \hat\mu \mathbf{j}' \mathbf{y}
$$

```{r}
# Modelo reduzido
jn <- matrix(1, ncol = 1, nrow = N)
jn

mi_reduzido <- solve(t(jn) %*% jn) %*% t(jn) %*% y
mi_reduzido

SQreduzido <- t(mi_reduzido) %*% t(jn) %*% y
SQreduzido
```

Com as somas de quadrados dos modelos completo e reduzido, obtemos a soma de quadrados **entre grupos**, calculada pela diferença entre a soma de quadrados do modelo completo (`SQcompleto`) e a soma de quadrados do modelo reduzido (`SQreduzido`). Além disso, apresenta $(k-1)$ graus de liberdade.

```{r}
# Entre grupos
SQEntre <- SQcompleto - SQreduzido
SQEntre

gl_entre <- k - 1
gl_entre

QMEntre <- SQEntre / gl_entre
QMEntre
```

Já a soma de quadrados dos resíduos (`SQRes`) é dada por:

$$
SQRes = \mathbf{y'y} - \boldsymbol{\hat\mu' W'y}
$$

```{r}
# Resíduo
SQRes <- t(y) %*% y - t(mi) %*% t(W) %*% y
SQRes

gl_res <- N - k
gl_res

QMRes <- SQRes / gl_res
QMRes
```

Com os quadrados médios, calcularemos a estatística F e o p-valor.

```{r}
Fcalc <- QMEntre / QMRes
Fcalc

Ftab <- qf(0.95, gl_entre, gl_res)
Ftab

p_valor <- 1 - pf(Fcalc, gl_entre, gl_res)
p_valor
```

A seguir, o quadro da ANOVA compila os resultados obtidos.

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "FV" = c("Hipótese", "Resíduo", "Total"),
  "gl" = c(gl_entre, gl_res, gl_total),
  "SQ" = c(SQEntre, SQRes, SQTotal) |> round(4),
  "QM" = c(QMEntre, QMRes, NA) |> round(4),
  "Fcal" = c(Fcalc, NA, NA) |> round(4),
  "Ftab" = c(Ftab, NA, NA) |> round(4),
  "p.valor" = c(p_valor, NA, NA) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "ANOVA desbalanceada: H0: m1=m2=m3=m4=m5",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que o $F_{cal} < F_{tab}$, considerando $F_{(0,05;4;11)}$, não se rejeita $H_0: \mu_1 = \mu_2 = \mu_3 = \mu_4 = \mu_5$, ou seja, não existem diferenças significativas entre as médias ponderadas dos pesos líquidos de latas enchidas pelas cinco máquinas. Ainda, podemos dizer que os pesos líquidos médios das latas enchidas pelas cinco máquinas não diferem entre si.

### SAS

```{r}
#| eval: false

Jnn = J(N,N,1);
In = I(N);
SQTotal = t(y)*(In-Jnn/N)*y;
gl_total = N-1;

mi = inv(t(W)*W)*t(W)*y;
SQcompleto = t(mi)*t(W)*y; 		* SQ do modelo completo: yij = mi(i) + eij;

jn = J(N,1,1);
mir = inv(t(jn)*jn)*t(jn)*y;
SQreduzido = t(mir)*t(jn)*y; 	* SQ do modelo reduzido: yij = mi + eij;

SQEntre = SQCompleto - SQreduzido;
gl_entre = k-1;
QMEntre = SQEntre/gl_entre;

SQRes = t(y)*y - t(mi)*t(W)*y;
gl_res = N-K;
QMRes = SQRes/gl_res;

Fcalc = QMEntre/QMRes;
p_valor = 1 - cdf('F', Fcalc,gl_entre, gl_res);

print '------------------------------',
      'Exemplo 15.2.1 Quadro de ANOVA',
      '------------------------------';
print 'Ho: m1=m2=m3=m4=m5'  gl_entre SQEntre[format=10.5] QMEntre[format=10.5] Fcalc[format=8.4] p_valor[format=8.3],,
      'Resíduo           '  gl_res   SQRes[format=10.5]   QMRes[format=10.5],,
      'Total             '  gl_total SQTotal[format=10.5]; 
```
:::

##### Contrastes

Definimos um contraste de $k$ médias populacionais como:

$$
\delta = c_1\mu_1 + c_2\mu_2 + \dots + c_k\mu_k = \boldsymbol{c'\mu}
$$

em que $\sum^k_{i=1} c_i = 0$.

Quando trabalhamos com ANOVA **balanceada**, dois contrastes

$$
\hat\delta = \sum^k_{i=1} a_i \bar{y}_{i.} \quad \text{e} \quad \hat\gamma = \sum^k_{i=1} b_i \bar{y}_{i.}
$$

são ditos ortogonais se $\sum^k_{i=1} a_i b_i = 0$, sendo esta a condição de independência entre os contrastes.

Para o caso **desbalanceado**, devemos utilizar **contrastes ortogonais ponderados**, afim de garantir a independência entre os contrastes. Dessa forma, os contrastes $\hat\delta$ e $\hat\gamma$ são independentes em modelos desbalanceados se e somente se

$$
\sum^k_{i=1} \frac{a_i b_i}{n_i} = 0
$$

sendo $n_i$ as respostas ao $i$-ésimo tratamento.

Contudo, na prática, os contrastes ortogonais ponderados são de menor interesse que os contrastes ortogonais não ponderados, não sendo necessário que as somas de quadrados sejam independentes para realizarmos os testes sobre os contrastes.

A seguir, veremos as diferenças entre os contrastes ortogonais não ponderados e os ponderados.

###### Contrastes ortogonais não ponderados

::: panel-tabset
### R

Os contrastes a seguir comparam as médias dos tratamentos da seguinte maneira:

$$
\begin{align}
H_{a01}&: 3\mu_1 - 2\mu_2 - 2\mu_3 + 3\mu_4 - 2\mu_5 = 0 \\
H_{a02}&: \mu_2 - 2\mu_3 + \mu_5 = 0 \\
H_{a03}&: \mu_1 - \mu_4 = 0 \\
H_{a04}&: \mu_2 - \mu_5 = 0 \\
\end{align}
$$

```{r}
a1 <- c(3,-2,-2, 3,-2)
a2 <- c(0, 1,-2, 0, 1)
a3 <- c(1, 0, 0,-1, 0)
a4 <- c(0, 1, 0, 0,-1)
```

As somas de quadrados são calculadas a partir da seguinte forma quadrática:

$$
SQ(\hat\delta) = \boldsymbol{(c'\mu)}' [\mathbf{c'(W'W)c}]^{-1} \boldsymbol{(c'\mu)}
$$

em que $\mathbf{c}$ é o vetor de coeficientes do contraste.

```{r}
SQa1 <- t(t(a1) %*% mi) %*% solve(t(a1) %*% solve(t(W) %*% W) %*% a1) %*% t(a1) %*% mi
F_a1 <- SQa1 / QMRes
p_valor_a1 <- 1 - pf(F_a1, 1, gl_res)

SQa2 <- t(t(a2) %*% mi) %*% solve(t(a2) %*% solve(t(W) %*% W) %*% a2) %*% t(a2) %*% mi
F_a2 <- SQa2 / QMRes
p_valor_a2 <- 1 - pf(F_a2, 1, gl_res)

SQa3 <- t(t(a3) %*% mi) %*% solve(t(a3) %*% solve(t(W) %*% W) %*% a3) %*% t(a3) %*% mi
F_a3 <- SQa3 / QMRes
p_valor_a3 <- 1 - pf(F_a3, 1, gl_res)

SQa4 <- t(t(a4) %*% mi) %*% solve(t(a4) %*% solve(t(W) %*% W) %*% a4) %*% t(a4) %*% mi
F_a4 <- SQa4 / QMRes
p_valor_a4 <- 1 - pf(F_a4, 1, gl_res)
```

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "Contrastes" = c("A,D vs. B,C,E", "B,E vs. C", "A vs. D", "B vs. E"),
  "SQ" = c(SQa1, SQa2, SQa3, SQa4) |> round(4), 
  "Fcal" = c(F_a1, F_a2, F_a3, F_a4) |> round(4),
  "p.valor" = c(p_valor_a1, p_valor_a2, p_valor_a3, p_valor_a4) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "ANOVA: Contrastes ortogonais não ponderados",
    col.names = c("Contrastes", "SQ", "Fcal", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que nenhum p-valor é menor que $\alpha = 0,05$, não rejeitamos qualquer uma das hipóteses $H_0: \sum_i c_i\mu_i = 0$ associadas aos contrastes definidos anteriormente.

Ao somar as somas de quadrados dos contrastes ortogonais **não ponderados**, não obtemos o mesmo resultado da soma de quadrados entre grupos (`SQEntre`)

```{r}
SQContrastes <- SQa1 + SQa2 + SQa3 + SQa4
SQContrastes

SQEntre
```

### SAS

```{r}
#| eval: false

* Contrastes ortogonais do tipo t(ai)*mi;
a1 = {3,-2,-2, 3,-2};
a2 = {0, 1,-2, 0, 1};
a3 = {1, 0, 0,-1, 0};
a4 = {0, 1, 0, 0,-1};

SQA1 = t(t(a1)*mi)*inv(t(a1)*inv(t(W)*W)*a1)*t(a1)*mi;
F_A1 = SQA1/QMRes;
p_valor_A1 = 1 - cdf('F', F_A1,1, gl_res);

SQa2 = t(t(a2)*mi)*inv(t(a2)*inv(t(W)*W)*a2)*t(a2)*mi;
F_a2 = SQa2/QMRes;
p_valor_a2 = 1 - cdf('F', F_a2,1, gl_res);

SQa3 = t(t(a3)*mi)*inv(t(a3)*inv(t(W)*W)*a3)*t(a3)*mi;
F_a3 = SQa3/QMRes;
p_valor_a3 = 1 - cdf('F', F_a3,1, gl_res);

SQa4 = t(t(A4)*mi)*inv(t(A4)*inv(t(W)*W)*A4)*t(A4)*mi;
F_A4 = SQA4/QMRes;
p_valor_A4 = 1 - cdf('F', F_A4,1, gl_res);

print '-------------------------------------',
      'Contrastes ortogonais não ponderados:',
	  '-------------------------------------',
      'A,D vs. B,C,E' SQA1[format=10.5] F_A1[format=12.4] p_valor_A1[format=12.3],,
      'B,E vs. C    ' SQA2[format=10.5] F_A2[format=12.4] p_valor_A2[format=12.3],,
	  'A vs. D      ' SQA3[format=10.5] F_A3[format=12.4] p_valor_A3[format=12.3],,
	  'B vs. E      ' SQA4[format=10.5] F_A4[format=12.4] p_valor_A4[format=12.3],,;

SQContrastes = SQA1 + SQA2 + SQA3 + SQA4;
print '----------------------------------------------------------------',
      'SQContrastes = SQA1 + SQA2 + SQA3 + SQA4 não é igual a SQEntre: ',,
       SQContrastes[format=10.5] SQEntre[format=10.5],,
      'porque os contrastes NÃO SÃO ORTOGONAIS!',
      '----------------------------------------------------------------',,,;
```
:::

###### Contrastes ortogonais ponderados

::: panel-tabset
### R

Utilizaremos o primeiro contraste ortogonal do exemplo anterior, $H_{a01}: 3\mu_1 - 2\mu_2 - 2\mu_3 + 3\mu_4 - 2\mu_5 = 0$, e o contraste ortogonal de $H_0: 2\mu_2 - 6\mu_3 + 4\mu_5 = 0$ para ilustrar os contrastes ortogonais **ponderados**. Portanto:

$$
\begin{align}
\delta &= \boldsymbol{a'\mu} = [3, -2, -2, 3, -2] \boldsymbol{\mu} \\
\gamma &= \boldsymbol{b'\mu} = [0, 2, -6, 0, 4] \boldsymbol{\mu}
\end{align}
$$

Neste caso, temos **contrastes ortogonais ponderados**:

$$
\sum^k_{i=1} \frac{a_ib_i}{n_i} = \frac{3(0)}4 - \frac{2(2)}2 - \frac{2(-6)}3 + \frac{3(0)}3 - \frac{2(4)}4 = 0
$$

```{r}
a2p <- c(0, 2, -6, 0, 4)

SQa2p <- t(t(a2p) %*% mi) %*% solve(t(a2p) %*% solve(t(W) %*% W) %*% a2p) %*% t(a2p) %*% mi

F_a2p <- SQa2p / QMRes

p_valor_a2p <- 1 - pf(F_a2p, 1, gl_res)
```

Assim, as somas de quadrados associadas aos dois contrastes são **independentes**, cujos valores estão descritos no seguinte quadro de ANOVA.

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "Contrastes" = c("A,D vs. B,C,E", "2B + 4E vs. 6C"),
  "SQ" = c(SQa1, SQa2p) |> round(4), 
  "Fcal" = c(F_a1, F_a2p) |> round(4),
  "p.valor" = c(p_valor_a1, p_valor_a2p) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "ANOVA: Contrastes ortogonais ponderados",
    col.names = c("Contrastes", "SQ", "Fcal", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

### SAS

```{r}
#| eval: false

A2p = {0, 2, -6, 0, 4};
SQA2p = t(t(a2p)*mi)*inv(t(a2p)*inv(t(W)*W)*a2p)*t(a2p)*mi;
F_a2p = SQa2p/QMRes;
p_valor_a2p = 1 - cdf('F', F_a2p,1, gl_res);

print '---------------------------------',
      'Contrastes ortogonais ponderados:',
	  '---------------------------------',,
      'A,D vs. B,C,E' SQA1[format=8.6]  F_A1[format=12.4]  p_valor_A1[format=8.3],,
	  '2B+4E vs. 6C ' SQA2p[format=8.6] F_A2p[format=12.4] p_valor_A2p[format=8.3],,,,;
quit;
```
:::

### ANOVA com dois fatores

```{r}
#| echo: false
#| message: false
#| warning: false

options(scipen = 999999, knitr.kable.NA = '')

library(MASS)
library(ExpDes.pt)
```

Agora, trataremos da ANOVA com **dois fatores** em conjuntos de dados **desbalanceados**. O modelo desbalanceado com dois fatores é dado por:

$$
\begin{align}
y_{ijk} = \mu + \alpha_i + \beta_j + \gamma{ij} + \epsilon_{ij} = \mu_{ij} + \epsilon_{ijk} \\
i = 1,\dots,a \space \text{ , } \space j = 1,\dots,b \space \text{ e } \space k = 1, \dots, n_{ij}
\end{align}
$$

Para realizar inferências, assumiremos $\epsilon_{ijk} \sim N(\mu_{ij}, \sigma^2)$ são independentes e identicamente distribuídos.

Para o caso fatorial desbalanceado, utilizaremos o modelo de médias de caselas, que fornece uma abordagem mais simples e sem ambiguidades para testar hipóteses, quando comparado ao modelo superparametrizado.

Como exemplo, considere: *Em um experimento de substituição do farelo de soja pelo farelo de girassol na ração de suínos, montou-se um experimento fatorial 2x5, com os fatores Sexo (1:Macho e 2:Fêmea) e Girassol (0, 25, 50, 75 e 100% de substituição). Foram utilizados 30 suínos (15 machos e 15 fêmeas) castrados da raça Duroc-Jersey, num delineamento inteiramente casualizado com 3 repetições. Na fase final do período experimental ocorreu a morte de três suínos. Os ganhos de peso dos animais aos 112 dias de experimento estão apresentados a seguir:*

|      |      | Macho |      |      |      |      | Fêmea |      |      |
|:----:|:----:|:-----:|:----:|:----:|:----:|:----:|:-----:|:----:|:----:|
|  0   |  25  |  50   |  75  | 100  |  0   |  25  |  50   |  75  | 100  |
|  \-  | 94,5 | 99,5  | 93,0 | 83,0 | 77,9 | 71,5 | 67,5  | 71,5 | 89,5 |
| 86,0 | 96,0 | 98,0  | 96,0 | 80,0 | 83,2 | 73,5 |  \-   | 70,8 | 91,8 |
| 84,0 | 95,8 |  \-   | 90,5 | 78,5 | 83,5 | 70,5 | 65,0  | 72,5 | 92,9 |

O modelo matemático é dado por:

$$
\begin{align}
y_{ijk}& = \mu_{ij} + \epsilon_{ijk} \\
i = 1,2 \space \text{ , } \space &j = 1,\dots,5 \space \text{ e } \space k = n_{ij}
\end{align}
$$

Já o modelo matricial de médias de caselas $\mathbf{y} = \mathbf{W}\boldsymbol{\mu} + \boldsymbol{\epsilon}$ é dado por:

$$
\begin{bmatrix}
y_{111} \\ y_{112} \\ y_{113} \\ y_{121} \\ y_{122} \\ y_{123} \\ y_{131} \\ y_{132} \\ y_{141} \\ y_{142} \\ y_{143} \\ y_{151} \\ y_{152} \\ y_{153} \\ y_{211} \\ y_{212} \\ y_{221} \\ y_{222} \\ y_{223} \\ y_{231} \\ y_{232} \\ y_{241} \\ y_{242} \\ y_{243} \\ y_{251} \\ y_{252} \\ y_{253}
\end{bmatrix}
=
\begin{bmatrix}
77.9 \\ 83.2 \\ 83.5 \\ 71.5 \\ 73.5 \\ 70.5 \\ 67.5 \\ 65 \\ 71.5 \\ 70.8 \\ 72.5 \\ 89.5 \\ 91.8 \\ 92.9 \\ 86 \\ 84 \\ 94.5 \\ 96 \\ 95.8 \\ 99.5 \\ 98 \\ 93 \\ 96 \\ 90.5 \\ 83 \\ 80 \\ 78.5
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
\mu_{11} \\ \mu_{12} \\ \mu_{13} \\ \mu_{14} \\ \mu_{15} \\ \mu_{21} \\ \mu_{22} \\ \mu_{23} \\ \mu_{24} \\ \mu_{25} 
\end{bmatrix}
\begin{bmatrix}
\epsilon_{111} \\ \epsilon_{112} \\ \epsilon_{113} \\ \epsilon_{121} \\ \epsilon_{122} \\ \epsilon_{123} \\ \epsilon_{131} \\ \epsilon_{132} \\ \epsilon_{141} \\ \epsilon_{142} \\ \epsilon_{143} \\ \epsilon_{151} \\ \epsilon_{152} \\ \epsilon_{153} \\ \epsilon_{211} \\ \epsilon_{212} \\ \epsilon_{221} \\ \epsilon_{222} \\ \epsilon_{223} \\ \epsilon_{231} \\ \epsilon_{232} \\ \epsilon_{241} \\ \epsilon_{242} \\ \epsilon_{243} \\ \epsilon_{251} \\ \epsilon_{252} \\ \epsilon_{253}
\end{bmatrix}
$$

::: panel-tabset
### R

```{r}
library(MASS)
```

```{r}
y <- c(77.9,83.2,83.5,71.5,73.5,70.5,67.5,65,71.5,70.8,72.5,89.5,91.8,92.9,86,84,94.5,96,95.8,99.5,98,93,96,90.5,83,80,78.5)

n <- length(y)
n

W <- matrix(c(
  1, 0,  0,  0,  0,  0,  0,  0,  0,  0,
  1, 0,  0,  0,  0,  0,  0,  0,  0,  0,
  1, 0,  0,  0,  0,  0,  0,  0,  0,  0,
  0, 1,  0,  0,  0,  0,  0,  0,  0,  0,
  0, 1,  0,  0,  0,  0,  0,  0,  0,  0,
  0, 1,  0,  0,  0,  0,  0,  0,  0,  0,
  0, 0,  1,  0,  0,  0,  0,  0,  0,  0,
  0, 0,  1,  0,  0,  0,  0,  0,  0,  0,
  0, 0,  0,  1,  0,  0,  0,  0,  0,  0,
  0, 0,  0,  1,  0,  0,  0,  0,  0,  0,
  0, 0,  0,  1,  0,  0,  0,  0,  0,  0,
  0, 0,  0,  0,  1,  0,  0,  0,  0,  0,
  0, 0,  0,  0,  1,  0,  0,  0,  0,  0,
  0, 0,  0,  0,  1,  0,  0,  0,  0,  0,
  0, 0,  0,  0,  0,  1,  0,  0,  0,  0,
  0, 0,  0,  0,  0,  1,  0,  0,  0,  0,
  0, 0,  0,  0,  0,  0,  1,  0,  0,  0,
  0, 0,  0,  0,  0,  0,  1,  0,  0,  0,
  0, 0,  0,  0,  0,  0,  1,  0,  0,  0,
  0, 0,  0,  0,  0,  0,  0,  1,  0,  0,
  0, 0,  0,  0,  0,  0,  0,  1,  0,  0,
  0, 0,  0,  0,  0,  0,  0,  0,  1,  0,
  0, 0,  0,  0,  0,  0,  0,  0,  1,  0,
  0, 0,  0,  0,  0,  0,  0,  0,  1,  0,
  0, 0,  0,  0,  0,  0,  0,  0,  0,  1,
  0, 0,  0,  0,  0,  0,  0,  0,  0,  1,
  0, 0,  0,  0,  0,  0,  0,  0,  0,  1
), ncol = 10, byrow = TRUE
)
```

Nota-se que posto$(W) = k = 10$, sendo $k$ o número de parâmetros do modelo.

```{r}
# Número de parâmetros
k <- ncol(W)
k

# Posto de W
rank_W <- sum(diag(ginv(W) %*% W))
rank_W

# Déficit de rank
deficit_rank <- k - rank_W
deficit_rank
```

Dessa forma, o sistema de equações normais $\mathbf{(W'W)} \boldsymbol{\hat\mu} = \mathbf{(W'y)}$ tem solução única:

$$
\boldsymbol{\hat\mu} = \mathbf{(W'W)}^{-1} \mathbf{W'y} = \mathbf{\bar{y}} =
\begin{bmatrix}
\bar{y}_{11.} \\ \bar{y}_{12.} \\ \bar{y}_{13.} \\ \bar{y}_{14.} \\ \bar{y}_{15.} \\ \bar{y}_{21.} \\ \bar{y}_{22.} \\ \bar{y}_{23.} \\ \bar{y}_{24.} \\ \bar{y}_{25.} 
\end{bmatrix}
$$

Onde o vetor $\mathbf{\bar{y}}$ contém as médias amostrais das caselas.

```{r}
Mi <- solve(t(W) %*% W) %*% t(W) %*% y
Mi
```

### SAS

```{r}
#| eval: false

data Girassol;
input Sexo Girassol Trat Rep GP;
* n11=2 n12=3 n13=2 n14=3 n15=3;
* n21=2 n22=3 n23=2 n24=3 n25=3;
cards;
  1   0   1   1  77.9
  1   0   1   2  83.2
  1   0   1   3  83.5
  1  25   2   1  71.5
  1  25   2   2  73.5
  1  25   2   3  70.5
  1  50   3   1  67.5
  1  50   3   3  65.0
  1  75   4   1  71.5
  1  75   4   2  70.8
  1  75   4   3  72.5
  1 100   5   1  89.5
  1 100   5   2  91.8
  1 100   5   3  92.9
  2   0   6   2  86.0
  2   0   6   3  84.0
  2  25   7   1  94.5
  2  25   7   2  96.0
  2  25   7   3  95.8
  2  50   8   1  99.5
  2  50   8   2  98.0
  2  75   9   1  93.0
  2  75   9   2  96.0
  2  75   9   3  90.5
  2 100  10   1  83.0
  2 100  10   2  80.0
  2 100  10   3  78.5
;

proc iml;
varNames = {"Sexo" "Girassol" "Trat" "Rep" "GP"};
use work.Girassol;
read all var varNames;
close work.Girassol;
print Sexo Girassol Trat Rep GP;

y = GP; 
n = nrow(y);

W = design(Trat);

Mi = inv(t(W)*W)*t(W)*GP;
print GP W Mi[format=8.2];
```
:::

#### Teste de hipótese

##### Modelo de médias

A seguir, testaremos a hipótese de igualdade entre as médias dos tratamentos utilizando a abordagem do **modelo completo x modelo reduzido**.

::: panel-tabset
### R

```{r}
Jnxn <- matrix(1, n, n)
Inxn <- diag(n)
```

A soma de quadrados total (`SQTotal`) é dada por:

$$
SQTotal = \mathbf{y'} \left(\mathbf{I} - \frac{1}n \mathbf{J}\right) \mathbf{y}
$$

```{r}
# Total
AT <- Inxn - (1 / n) * Jnxn

SQTotal <- t(y) %*% AT %*% y
SQTotal

gl_total <- sum(diag(AT %*% ginv(AT)))
gl_total
```

A soma de quadrados dos resíduos (`SQRes`) é dada por:

$$
SQRes = \mathbf{y'} [\mathbf{I} - \mathbf{W(W'W)^{-}W'}] \mathbf{y}
$$

```{r}
# Residuo
AR <- Inxn - W %*% ginv(t(W) %*% W) %*% t(W)

SQRes <- t(y) %*% AR %*% y
SQRes

gl_res <- sum(diag(AR %*% ginv(AR)))
gl_res

QMRes <- SQRes / gl_res
QMRes
```

A soma de quadrados de tratamento é:

$$
SQTrat = \mathbf{y'} \left[\mathbf{W(W'W)^{-}W'} - \frac{1}n \mathbf{J}\right] \mathbf{y}
$$

```{r}
# Tratamento

AG <- W %*% ginv(t(W) %*% W) %*% t(W) - (1/n) * Jnxn

SQTrat <- t(y) %*% AG %*% y
SQTrat

gl_trat <- sum(diag(AG %*% ginv(AG)))
gl_trat

QMTrat <- SQTrat / gl_trat
QMTrat
```

```{r}
# F e p-valor

Fcalc <- QMTrat / QMRes
Fcalc

Ftab <- qf(0.95, gl_trat, gl_res)
Ftab

p_valor <- 1 - pf(Fcalc, gl_trat, gl_res)
p_valor
```

A seguir, o quadro da ANOVA traz os resultados obtidos.

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "FV" = c("Tratamentos", "Resíduo", "Total"),
  "gl" = c(gl_trat, gl_res, gl_total),
  "SQ" = c(SQTrat, SQRes, SQTotal) |> round(4),
  "QM" = c(QMTrat, QMRes, NA) |> round(4),
  "Fcal" = c(Fcalc, NA, NA) |> round(4),
  "Ftab" = c(Ftab, NA, NA) |> round(4),
  "p.valor" = c(p_valor, NA, NA) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "ANOVA fatorial desbalanceada - modelo de médias: H0: ",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "Ftab", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que o $F_{cal} > F_{tab}$, considerando $F_{(0,05;9;17)}$, rejeita-se a hipótese nula de igualdade de médias, ou seja, pelo menos duas médias diferem entre si.

Com isso, precisamos analisar o efeito de interação entre os tratamentos.

### SAS

```{r}
#| eval: false

AT = I(n)-(1/n)*J(n,n,1);
SQTotal = t(Y)*AT*y;
gl_total = n-1;

ARes = I(n)- W*inv(t(W)*W)*t(W);
SQRes = t(y)*ARes*y;
gl_res = round(trace(ARes*ginv(ARes)));
QMRes = SQRes/gl_Res;

ATrat = W*inv(t(W)*W)*t(W) - (1/n)*J(n,n,1);
SQTrat = t(y)*ATrat*y;
gl_trat = round(trace(ATrat*ginv(ATrat)));
QMTrat = SQTrat/gl_trat;
F_trat = QMTrat/QMRes;
p_trat = 1-cdf('F',F_trat,gl_trat,gl_res);

print 'ANOVA - modelo de médias',,
'Tratamentos ' gl_trat SQTrat[format=12.4] QMTrat[format=12.4] F_trat[format=12.4] p_trat[format=12.4],,
'Resíduo     ' gl_res SQRes[format=12.4] QMRes[format=12.4],,
'Total       ' gl_total SQTotal[format=12.4];
```
:::

##### Efeito de interação

Para avaliar se há **interação** entre os fatores, realizaremos outra ANOVA, avaliando os **efeitos principais** de ambos os fatores (vide @sec-fatorial-th).

$$
\begin{align}
H_0 &: \text{Não há efeito da interação entre os fatores A e B} \\
H_1 &: \text{Há efeito da interação entre os fatores A e B}
\end{align}
$$

Para isso, utilizaremos a **hipótese linear geral**. A soma de quadrados obtida a partir desta abordagem é dada por:

$$
SQ_{Hip} = (\boldsymbol{C\hat{\beta}})' \left[\mathbf{C(X'X)^-C'}\right]^{-1} (\boldsymbol{C\hat{\beta}})
$$

::: panel-tabset
### R

A matriz de coeficientes $\mathbf{C}$ do fator Sexo (`CS`), do fator Girassol (`CG`) e da interação entre os fatores (`CSxG`) são dadas a seguir:

```{r}
CS <- matrix(c(1,  1,  1,  1,  1, -1, -1, -1, -1, -1), ncol = 10, byrow = TRUE)
CS

CG <- matrix(c(
  -2, -1,  0,  1,  2, -2, -1,  0,  1,  2,
   2, -1, -2, -1,  2,  2, -1, -2, -1,  2,
  -1,  2,  0, -2,  1, -1,  2,  0, -2,  1,
   1, -4,  6, -4,  1,  1, -4,  6, -4,  1
), ncol = 10, byrow = TRUE
)
CG

CSxG <- rbind(
  sweep(CS, 2, CG[1,], `*`),
  sweep(CS, 2, CG[2,], `*`),
  sweep(CS, 2, CG[3,], `*`),
  sweep(CS, 2, CG[4,], `*`)
)
CSxG
```

A matriz `CSxG` é construída a partir da mutiplicação, elemento a elemento, da matriz `CS` com a primeira, segunda, terceira e quarta linhas da matriz `CG`. Para isso, utilizamos a função `sweep()` para realizar esta operação e, posteriormente, com a `rbind()`, juntamos os resultados por linha.

```{r}
# Fator Sexo
SQSexo <- t(CS %*% Mi) %*% solve(CS %*% ginv(t(W) %*% W) %*% t(CS)) %*% (CS %*% Mi)
SQSexo

gl_sexo <- nrow(CS)
gl_sexo

QMSexo <- SQSexo / gl_sexo
QMSexo

F_sexo <- QMSexo / QMRes
F_sexo

p_sexo <- 1 - pf(F_sexo, gl_sexo, gl_res)
p_sexo
```

```{r}
# Fator Girassol
SQGirassol <- t(CG %*% Mi) %*% solve(CG %*% ginv(t(W) %*% W) %*% t(CG)) %*% (CG %*% Mi)
SQGirassol

gl_girassol <- nrow(CG)
gl_girassol

QMGirassol <- SQGirassol / gl_girassol
QMGirassol

F_girassol <- QMGirassol / QMRes
F_girassol

p_girassol <- 1 - pf(F_girassol, gl_girassol, gl_res)
p_girassol
```

```{r}
# Interação Sexo x Girassol
SQSxG <- t(CSxG %*% Mi) %*% solve(CSxG %*% ginv(t(W) %*% W) %*% t(CSxG)) %*% (CSxG %*% Mi)
SQSxG

gl_SxG <- nrow(CSxG)
gl_SxG

QMSxG <- SQSxG / gl_SxG
QMSxG

F_SxG <- QMSxG / QMRes
F_SxG

p_SxG <- 1 - pf(F_SxG, gl_SxG, gl_res)
p_SxG
```

No caso fatorial desbalanceado, a soma das somas de quadrados não será igual a soma de quadrados dos tratamentos, pois não utilizamos contrastes ortogonais ponderados.

```{r}
SSQ <- SQSexo + SQGirassol + SQSxG
SSQ

SQTrat
```

O quadro de ANOVA resume os resultados obtidos.

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "FV" = c("Sexo (A)", "Girassol (B)", "Interação (AxB)", "Resíduo", "Total"),
  "gl" = c(gl_sexo, gl_girassol, gl_SxG, gl_res, gl_total),
  "SQ" = c(SQSexo, SQGirassol, SQSxG, SQRes, SQTotal) |> round(3),
  "QM" = c(QMSexo, QMGirassol, QMSxG, QMRes, NA) |> round(3),
  "Fcal" = c(F_sexo, F_girassol, F_SxG, NA, NA) |> round(3),
  "p.valor" = c(p_sexo, p_girassol, p_SxG, NA, NA) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "ANOVA: Fatorial Sexo x Girassol (caso desbalanceado) - Interação e efeitos principais",
    col.names = c("FV", "gl", "SQ", "QM", "Fcal", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Dado que o p-valor da **interação** é menor que p-valor = 0,05, **rejeita-se** $H_0: \text{Não há efeito da interação entre os fatores A e B}$, ou seja, **há efeito de interação** entre os fatores Sexo e Girassol (há dependência).

Visto que há interação entre os fatores, precisamos avaliar os **efeitos simples** dos tratamentos.

### SAS

```{r}
#| eval: false

CS = { 1  1  1  1  1 -1 -1 -1 -1 -1};
*CG = {-4  1  1  1  1 -4  1  1  1  1,
       0 -3  1  1  1  0 -3  1  1  1,
       0  0 -2  1  1  0  0 -2  1  1,
       0  0  0 -1  1  0  0  0 -1  1}; 

CG = {-2 -1  0  1  2 -2 -1  0  1  2,
       2 -1 -2 -1  2  2 -1 -2 -1  2,
	  -1  2  0 -2  1 -1  2  0 -2  1,
	   1 -4  6 -4  1  1 -4  6 -4  1};
CSxG = CS#CG[1,]//CS#CG[2,]//CS#CG[3,]//CS#CG[4,];
print CS,, CG,, CSxG;

SQSexo = t(CS*Mi)*inv(CS*inv(t(W)*W)*t(CS))*(CS*Mi);
gl_Sexo = nrow(CS);
QMSexo = SQSexo/gl_Sexo;
F_sexo =QMSexo/QMRes;
p_sexo = 1-cdf('F',F_sexo,gl_sexo,gl_res);

SQGirassol = t(CG*Mi)*inv(CG*inv(t(W)*W)*t(CG))*(CG*Mi);
gl_Girassol = nrow(CG);
QMGirassol = SQGirassol/gl_Girassol;
F_Girassol = QMGirassol/QMRes;
p_girassol = 1-cdf('F',F_Girassol,gl_Girassol,gl_res);

SQSxG = t(CSxG*Mi)*inv(CSxG*inv(t(W)*W)*t(CSxG))*(CSxG*Mi);
gl_SxG = nrow(CSxG);
QMSxG = SQSxG/gl_SxG;
F_SxG = QMSxG/QMRes;
p_SxG = 1-cdf('F',F_SxG,gl_SxG,gl_res);

SQTrats = SQSexo + SQGirassol + SQSxG;

print 'ANOVA - FATORIAL SEXO x GIRASSOL',,
'Sexo            ' gl_sexo SQSexo[format=12.4] QMSexo[format=12.4] F_sexo[format=12.4] p_sexo[format=12.4],,
'Girassol      ' gl_girassol SQGirassol[format=12.4] QMGirassol[format=12.4] F_Girassol[format=12.4] p_Girassol[format=12.4],,
'Interação SxG   ' gl_SxG SQSxG[format=12.4] QMSxG[format=12.4] F_SxG[format=12.4] p_SxG[format=12.4],,
'Resíduo         ' gl_res SQRes[format=12.4] QMRes[format=12.4],,
'Total           ' gl_total SQTotal[format=12.4],,;

print 'SQTrats = SQS + SQG + SQSxG = ' SQTrats[format=12.4] SQTrat[format=12.4];

```
:::

##### Efeitos Simples

Dado que há interação entre os fatores, devemos avaliar os **efeitos simples** de cada fator, ou seja, avaliar o efeito do fator A (Sexo) dentro de cada nível do fator B (Girassol) e/ou o efeito do fator B (Girassol) dentro de cada nível do fator A (Sexo).

Para isso, utilizaremos **contrastes ortogonais** não ponderados.

###### Sexo dentro de Girassol

Primeiramente, compararemos as médias dos dois sexos dentro de cada nível do fator Girassol.

$$
H_0: \mu_{1j} = \mu_{2j}, \quad \text{para } j = 1,2,3,4,5
$$

::: panel-tabset
### R

```{r}
a1 <- matrix(c(1, 0, 0, 0, 0, -1,  0,  0,  0,  0), ncol = 1)
a2 <- matrix(c(0, 1, 0, 0, 0,  0, -1,  0,  0,  0), ncol = 1)
a3 <- matrix(c(0, 0, 1, 0, 0,  0,  0, -1,  0,  0), ncol = 1)
a4 <- matrix(c(0, 0, 0, 1, 0,  0,  0,  0, -1,  0), ncol = 1)
a5 <- matrix(c(0, 0, 0, 0, 1,  0,  0,  0,  0, -1), ncol = 1)
```

A soma de quadrados dos contrastes é calculada da seguinte forma:

$$
SQ_c = (\boldsymbol{\lambda' \mu})' \left[\boldsymbol{\lambda'} (\mathbf{W'W})^{-1} \boldsymbol{\lambda}\right]^{-1} (\boldsymbol{\lambda' \mu})
$$

Cada contraste tem um graus de liberdade.

```{r}
SQa1 <- t(t(a1) %*% Mi) %*% solve(t(a1) %*% solve(t(W) %*% W) %*% a1) %*% (t(a1) %*% Mi)
SQa1

SQa2 <- t(t(a2) %*% Mi) %*% solve(t(a2) %*% solve(t(W) %*% W) %*% a2) %*% (t(a2) %*% Mi)
SQa2

SQa3 <- t(t(a3) %*% Mi) %*% solve(t(a3) %*% solve(t(W) %*% W) %*% a3) %*% (t(a3) %*% Mi)
SQa3

SQa4 <- t(t(a4) %*% Mi) %*% solve(t(a4) %*% solve(t(W) %*% W) %*% a4) %*% (t(a4) %*% Mi)
SQa4

SQa5 <- t(t(a5) %*% Mi) %*% solve(t(a5) %*% solve(t(W) %*% W) %*% a5) %*% (t(a5) %*% Mi)
SQa5

gl_a <- 1
```

A estatística F e o p-valor de cada contraste é dado por:

```{r}
Fcalc1 <- SQa1 / QMRes
p_valor1 <- 1 - pf(Fcalc1, 1, gl_res)
p_valor1

Fcalc2 <- SQa2 / QMRes
p_valor2 <- 1 - pf(Fcalc2, 1, gl_res)
p_valor2

Fcalc3 <- SQa3 / QMRes
p_valor3 <- 1 - pf(Fcalc3, 1, gl_res)
p_valor3

Fcalc4 <- SQa4 / QMRes
p_valor4 <- 1 - pf(Fcalc4, 1, gl_res)
p_valor4

Fcalc5 <- SQa5 / QMRes
p_valor5 <- 1 - pf(Fcalc5, 1, gl_res)
p_valor5
```

O quadro de ANOVA resume os resultados obtidos.

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "Hipóteses" = c("Girassol = 0: M = F", "Girassol = 25: M = F", "Girassol = 50: M = F", "Girassol = 75: M = F", "Girassol = 100: M = F"),
  "gl" = c(gl_a),
  "SQ" = c(SQa1, SQa2, SQa3, SQa4, SQa5) |> round(3),
  "Fcal" = c(Fcalc1, Fcalc2, Fcalc3, Fcalc4, Fcalc5) |> round(3),
  "p.valor" = c(p_valor1, p_valor2, p_valor3, p_valor4, p_valor5) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "ANOVA: Fatorial Sexo x Girassol (caso desbalanceado) - Sexo dentro de Girassol",
    col.names = c("Hipóteses", "gl", "SQ", "Fcal", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center")
```

Apenas para Girassol = 0% $H_0: \mu_{1j} = \mu_{2j}$ não foi rejeitada. No demais níveis (25%, 50%, 75% e 100%), rejeita-se a hipótese nula, ou seja, há diferença entre o ganho de peso entre machos e fêmeas.

Para verificar a diferença do percentual de farelo de girassol entre os sexo, devemos analisar as médias dos efeitos simples.

```{r}
#| echo: false

data.frame(
  Girassol = rep(c(0,25,50,75,100),2),
  Sexo = c(rep("F", 5), rep("M", 5)),
  medias = round(Mi, 2)
) |> 
  tidyr::pivot_wider(names_from = Girassol, values_from = medias) |> 
  kableExtra::kbl(align = "c") |> 
  kableExtra::kable_styling(position = "center")
```

Uma vez que para girassol = 0% não houve significância, as médias entre os sexos não diferem, estatisticamente. Para os níveis 25%, 50%, 75% e 100%, o ganho médio de peso dos machos foi superior ao das fêmeas, mas com 100% de Girassol, as fêmeas tiveram maior ganho médio de peso.

```{r}
#| echo: false

data.frame(
  Sexo = c("F", "M"),
  "0"  = c("81.53 a", "85.00 a"),
  "25" = c("71.83 b", "95.43 a"),	
  "50" = c("66.25 b", "98.75 a"),	
  "75" = c("71.60 b", "93.17 a"),	
  "100" = c("91.4 a", "80.5 b")
) |> 
  kableExtra::kbl(align = "c", col.names = c("Sexo","0","25","50","75","100")) |> 
  kableExtra::kable_styling(position = "center")
```

### SAS

```{r}
#| eval: false

a1 = {1 0 0 0 0 -1  0  0  0  0};
a2 = {0 1 0 0 0  0 -1  0  0  0};
a3 = {0 0 1 0 0  0  0 -1  0  0};
a4 = {0 0 0 1 0  0  0  0 -1  0};
a5 = {0 0 0 0 1  0  0  0  0 -1};
SQa1 = t(a1*Mi)*inv(a1*inv(t(W)*W)*t(a1))*(a1*Mi);
SQa2 = t(a2*Mi)*inv(a2*inv(t(W)*W)*t(a2))*(a2*Mi);
SQa3 = t(a3*Mi)*inv(a3*inv(t(W)*W)*t(a3))*(a3*Mi);
SQa4 = t(a4*Mi)*inv(a4*inv(t(W)*W)*t(a4))*(a4*Mi);
SQa5 = t(a5*Mi)*inv(a5*inv(t(W)*W)*t(a5))*(a5*Mi);
gl_a = 1;
Fa1 = SQa1/QMRes; Fa2 = SQa2/QMRes; Fa3 = SQa3/QMRes; Fa4 = SQa4/QMRes; Fa5 = SQa5/QMRes;
p_a1 = 1-cdf('F',Fa1,1,gl_res); p_a2 = 1-cdf('F',Fa2,1,gl_res);
p_a3 = 1-cdf('F',Fa3,1,gl_res); p_a4 = 1-cdf('F',Fa4,1,gl_res);
p_a5 = 1-cdf('F',Fa5,1,gl_res);

print '------------------------------------------------------------------------',
      ' Desdobramento (1): Compara as médias de Sexo em cada nível de Girassol ',
      '------------------------------------------------------------------------',,
'Girassol=  0: M=F' gl_a SQa1[format=12.4] Fa1[format=12.4] p_a1[format=12.4],,
'Girassol= 25: M=F' gl_a SQa2[format=12.4] Fa2[format=12.4] p_a2[format=12.4],,
'Girassol= 50: M=F' gl_a SQa3[format=12.4] Fa3[format=12.4] p_a3[format=12.4],,
'Girassol= 75: M=F' gl_a SQa4[format=12.4] Fa4[format=12.4] p_a4[format=12.4],,
'Girassol=100: M=F' gl_a SQa5[format=12.4] Fa5[format=12.4] p_a5[format=12.4];

quit;
```
:::

###### Girassol dentro de Sexo

Agora, compararemos as médias do fator Girassol dentro de cada nível de sexo. Como Girassol é um fator **quantitativo** e seus níveis são igualmente espaçados, vamos usar coeficientes de polinômios ortogonais para realizar os testes de tendência, separadamente, para cada Sexo.

$$
H_0: \mu_{i1} = \mu_{i2} = \mu_{i3} = \mu_{i4} = \mu_{i5}, \quad \text{para } i = 1,2
$$

::: panel-tabset
### R

Como são 5 níveis, utilizaremos os coeficientes até o 4º grau.

Para as fêmeas vamos usar:

```{r}
F1 <- c(-2, -1,  0,  1, 2, 0, 0, 0, 0, 0)  # Grau 1
F2 <- c( 2, -1, -2, -1, 2, 0, 0, 0, 0, 0)  # Grau 2
F3 <- c(-1,  2,  0, -2, 1, 0, 0, 0, 0, 0)  # Grau 3
F4 <- c( 1, -4,  6, -4, 1, 0, 0, 0, 0, 0)  # Grau 4
```

e para os machos:

```{r}
M1 <- c(0, 0, 0, 0, 0, -2, -1,  0,  1, 2)  # Grau 1
M2 <- c(0, 0, 0, 0, 0,  2, -1, -2, -1, 2)  # Grau 2
M3 <- c(0, 0, 0, 0, 0, -1,  2,  0, -2, 1)  # Grau 3
M4 <- c(0, 0, 0, 0, 0,  1, -4,  6, -4, 1)  # Grau 4
```

Com os contrastes ortogonais para cada sexo, realizaremos a ANOVA para cada um deles, a fim de verificar qual o grau de polinômio que melhor explica o ganho de peso de cada um dos sexos.

-   **Fêmeas**:

```{r}
SQf1 <- t(t(F1) %*% Mi) %*% solve(t(F1) %*% solve(t(W) %*% W) %*% F1) %*% (t(F1) %*% Mi)

SQf2 <- t(t(F2) %*% Mi) %*% solve(t(F2) %*% solve(t(W) %*% W) %*% F2) %*% (t(F2) %*% Mi)

SQf3 <- t(t(F3) %*% Mi) %*% solve(t(F3) %*% solve(t(W) %*% W) %*% F3) %*% (t(F3) %*% Mi)

SQf4 <- t(t(F4) %*% Mi) %*% solve(t(F4) %*% solve(t(W) %*% W) %*% F4) %*% (t(F4) %*% Mi)


gl_f <- 1


Fcalc1 <- SQf1 / QMRes
p_valor1 <- 1 - pf(Fcalc1, 1, gl_res)

Fcalc2 <- SQf2 / QMRes
p_valor2 <- 1 - pf(Fcalc2, 1, gl_res)

Fcalc3 <- SQf3 / QMRes
p_valor3 <- 1 - pf(Fcalc3, 1, gl_res)

Fcalc4 <- SQf4 / QMRes
p_valor4 <- 1 - pf(Fcalc4, 1, gl_res)
```

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "Grau" = c("1º", "2º", "3º", "4º"),
  "gl" = c(gl_f),
  "SQ" = c(SQf1, SQf2, SQf3, SQf4) |> round(3),
  "Fcal" = c(Fcalc1, Fcalc2, Fcalc3, Fcalc4) |> round(3),
  "p.valor" = c(p_valor1, p_valor2, p_valor3, p_valor4) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "ANOVA: Fatorial Sexo x Girassol (caso desbalanceado) - Girassol dentro de Sexo F",
    col.names = c("Grau", "gl", "SQ", "Fcal", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center") |> 
  kableExtra::row_spec(3, bold = TRUE, background = "#B9C6EB")
```

Analisando, decrescentemente, a ordem dos graus, o 3º grau é o primeiro a apresentar significância estatística, considerando p-valor = 5%. Assim, o comportamento do ganho de peso em função do aumento da porcentagem de substituição do farelo de soja por farelo de girassol pode ser bem explicado por um polinômio de 3º grau para as fêmeas.

-   **Machos:**

```{r}
SQm1 <- t(t(M1) %*% Mi) %*% solve(t(M1) %*% solve(t(W) %*% W) %*% M1) %*% (t(M1) %*% Mi)

SQm2 <- t(t(M2) %*% Mi) %*% solve(t(M2) %*% solve(t(W) %*% W) %*% M2) %*% (t(M2) %*% Mi)

SQm3 <- t(t(M3) %*% Mi) %*% solve(t(M3) %*% solve(t(W) %*% W) %*% M3) %*% (t(M3) %*% Mi)

SQm4 <- t(t(M4) %*% Mi) %*% solve(t(M4) %*% solve(t(W) %*% W) %*% M4) %*% (t(M4) %*% Mi)


gl_m <- 1


Fcalc1 <- SQm1 / QMRes
p_valor1 <- 1 - pf(Fcalc1, 1, gl_res)

Fcalc2 <- SQm2 / QMRes
p_valor2 <- 1 - pf(Fcalc2, 1, gl_res)

Fcalc3 <- SQm3 / QMRes
p_valor3 <- 1 - pf(Fcalc3, 1, gl_res)

Fcalc4 <- SQm4 / QMRes
p_valor4 <- 1 - pf(Fcalc4, 1, gl_res)
```

```{r}
#| echo: false

options(knitr.kable.NA = '')

data.frame(
  "Grau" = c("1º", "2º", "3º", "4º"),
  "gl" = c(gl_m),
  "SQ" = c(SQm1, SQm2, SQm3, SQm4) |> round(3),
  "Fcal" = c(Fcalc1, Fcalc2, Fcalc3, Fcalc4) |> round(3),
  "p.valor" = c(p_valor1, p_valor2, p_valor3, p_valor4) |> round(4)
) |> 
  kableExtra::kbl(
    align = "c",
    caption = "ANOVA: Fatorial Sexo x Girassol (caso desbalanceado) - Girassol dentro de Sexo M",
    col.names = c("Grau", "gl", "SQ", "Fcal", "p-valor (5%)")
  ) |> 
  kableExtra::kable_styling(position = "center") |> 
  kableExtra::row_spec(2, bold = TRUE, background = "#B9C6EB")
```

Enquanto isso, para os machos, o comportamento do ganho de peso em função do aumento da porcentagem de substituição do farelo de soja por farelo de girassol pode ser bem explicado por um polinômio de 2º grau.

A seguir, ajustaremos as curvas de regressão de ambos os modelos. Para isso, utilizaremos a função `fat2.dic()` do pacote `ExpDes.pt`, a fim de obter os coeficientes de regressão estimados.

```{r}
#| eval: false

install.packages("ExpDes.pt")
library(ExpDes.pt)
```

```{r}
#| message: false

dados <- data.frame(
  Sexo = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),
  Girassol = c(0, 0, 0, 25, 25, 25, 50, 50, 75, 75, 75, 100, 100, 100, 0, 0, 25, 25, 25, 50, 50, 75, 75, 75, 100, 100, 100),
  Trat = c(1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 9, 9, 9, 10, 10, 10),
  Rep = c(1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3),
  GP = c(77.9, 83.2, 83.5, 71.5, 73.5, 70.5, 67.5, 65.0, 71.5, 70.8, 72.5, 89.5, 91.8, 92.9, 86.0, 84.0, 94.5, 96.0, 95.8, 99.5, 98.0, 93.0, 96.0, 90.5, 83.0, 80.0, 78.5)
)

with(
  dados,
  fat2.dic(Sexo, Girassol, resp = GP, quali = c(TRUE, FALSE))
  )
```

A seguir, contruiremos os gráficos dos modelos ajustados.

```{r}
dados <- dados |> dplyr::mutate(Sexo = as.factor(Sexo))

## Fêmeas
dados_f <- dados |> dplyr::filter(Sexo == 1)

## Machos
dados_m <- dados |> dplyr::filter(Sexo == 2)

## Gráfico
library(ggplot2)

dados |> 
  ggplot() + 
  geom_point(aes(x = Girassol, y = GP, color = Sexo)) +
  stat_function(
    data = dados_f, 
    fun = function(Girassol) {81.577 - 0.425*Girassol - 0.0003*Girassol^2 + 0.00006 * Girassol^3},
    color = "#F77D7D", linewidth = 1, linetype = 2
  ) +
  stat_function(
    data = dados_m,
    fun = function(Girassol) {84.951 + 0.5847*Girassol - 0.0063*Girassol^2},
    color = "#52A7E8", linewidth = 1, linetype = 2
  ) +
  theme_minimal() +
  labs(x = NULL, y = "Ganho de peso", color = NULL) +
  scale_color_discrete(labels = c("Fêmea", "Macho")) +
  theme(legend.position = "bottom")
```

### SAS

```{r}
#| eval: false

proc glm;
class trat;
model GP = trat / ss3;

contrast 'Sexo' trat 1  1  1  1  1 -1 -1 -1 -1 -1;
contrast 'Girassol'  trat -2 -1  0  1  2 -2 -1  0  1  2,
      				 trat  2 -1 -2 -1  2  2 -1 -2 -1  2,
	  				 trat -1  2  0 -2  1 -1  2  0 -2  1,
	  				 trat  1 -4  6 -4  1  1 -4  6 -4  1;
contrast 'Interação' trat -4  1  1  1  1  4 -1 -1 -1 -1,
      				 trat  0 -3  1  1  1  0  3 -1 -1 -1,
 	  				 trat  0  0 -2  1  1  0  0  2 -1 -1,
 	  				 trat  0  0  0 -1  1  0  0  0  1 -1;

contrast 'Girassol=  0: F = M' trat 1 0 0 0 0 -1  0  0  0  0;
contrast 'Girassol= 25: F = M' trat 0 1 0 0 0  0 -1  0  0  0;
contrast 'Girassol= 50: F = M' trat 0 0 1 0 0  0  0 -1  0  0;
contrast 'Girassol= 75: F = M' trat 0 0 0 1 0  0  0  0 -1  0;
contrast 'Girassol=100: F = M' trat 0 0 0 0 1  0  0  0  0 -1;
contrast 'Machos: Girassol grau 1' trat -2 -1  0  1  2  0  0  0  0  0;
contrast 'Machos: Girassol grau 2' trat  2 -1 -2 -1  2  0  0  0  0  0;
contrast 'Machos: Girassol grau 3' trat -1  2  0 -2  1  0  0  0  0  0;
contrast 'Machos: Girassol grau 4' trat  1 -4  6 -4  1  0  0  0  0  0;
contrast 'Fêmeas: Girassol grau 1' trat  0  0  0  0  0 -2 -1  0  1  2;
contrast 'Fêmeas: Girassol grau 2' trat  0  0  0  0  0  2 -1 -2 -1  2;
contrast 'Fêmeas: Girassol grau 3' trat  0  0  0  0  0 -1  2  0 -2  1;
contrast 'Fêmeas: Girassol grau 4' trat  0  0  0  0  0  1 -4  6 -4  1;
run;

proc glm;
class Sexo Girassol;
model GP = Sexo Girassol  / ss1 ss2 ss3;
run;
```
:::
